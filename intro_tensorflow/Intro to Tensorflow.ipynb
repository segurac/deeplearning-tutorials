{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Deep Learning with Tensorflow\n",
    "<br><br><br>\n",
    "\n",
    "##  Carlos Segura \n",
    "<b>\n",
    "## Associate Researcher @ TID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This slides: \n",
    "[http://crepe.hi.inet:8888](http://crepe.hi.inet:8888)\n",
    "<br>\n",
    "password: deeplearning\n",
    "<br> \n",
    "Select a copy \"Intro to Tensorflow_copy_xx.ipynb\", rename it and open it\n",
    "<br>\n",
    "\n",
    "[https://github.com/segurac/deeplearning-tutorials/](https://github.com/segurac/deeplearning-tutorials)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Tensorflow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* TensorFlow is a deep learning library\n",
    "     * open-sourced by Google.\n",
    "* TensorFlow provides primitives for\n",
    "defining functions on tensors and\n",
    "automatically computing their derivatives.\n",
    "\n",
    "* Python library that can work with symbolic mathematical expressions\n",
    "    * *Symbolic differentation*: symbolic graphs for computing gradients\n",
    "* Optimized for multi-dimensional arrays, like numpy.ndarray\n",
    "    * Tensors\n",
    "* Same code can work in CPU or GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Installing Tensorflow\n",
    "* Requirements\n",
    "    * OS: Linux, Mac OS X, Windows\n",
    "    * Python: >= 2.7 || >= 3.5\n",
    "* pip install tensorflow [tensorflow-gpu]\n",
    "\n",
    "### For running this tutorial:\n",
    "#### In your environment:\n",
    "* pip3 install tensorflow notebook matplotlib RISE\n",
    "* jupyter-nbextension install rise --py --sys-prefix\n",
    "* jupyter-nbextension enable rise --py --sys-prefix\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic usage:\n",
    "\n",
    "* Represent computations as graphs\n",
    "* Execute graphs in the context of sessions\n",
    "* Represent data as tensors\n",
    "* Maintain state with Variables\n",
    "* Use feeds and fetches to get data in and out of graph executions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensorflow graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Graphs\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create a Constant op that produces a 1x2 matrix.  The op is\n",
    "# added as a node to the default graph.\n",
    "#\n",
    "# The value returned by the constructor represents the output\n",
    "# of the Constant op.\n",
    "matrix1 = tf.constant([[3., 3.]])\n",
    "\n",
    "# Create another Constant that produces a 2x1 matrix.\n",
    "matrix2 = tf.constant([[2.],[2.]])\n",
    "\n",
    "# Create a Matmul op that takes 'matrix1' and 'matrix2' as inputs.\n",
    "# The returned value, 'product', represents the result of the matrix\n",
    "# multiplication.\n",
    "product = tf.matmul(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Execute graphs in the context of sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.]]\n"
     ]
    }
   ],
   "source": [
    "#To actually multiply the matrices, and get the result of the multiplication, you must launch the graph in a session.\n",
    "# Launch the default graph.\n",
    "sess = tf.Session()\n",
    "\n",
    "# To run the matmul op we call the session 'run()' method, passing 'product'\n",
    "# which represents the output of the matmul op.  This indicates to the call\n",
    "# that we want to get the output of the matmul op back.\n",
    "#\n",
    "# All inputs needed by the op are run automatically by the session.  They\n",
    "# typically are run in parallel.\n",
    "#\n",
    "# The call 'run(product)' thus causes the execution of three ops in the\n",
    "# graph: the two constants and matmul.\n",
    "#\n",
    "# The output of the op is returned in 'result' as a numpy `ndarray` object.\n",
    "result = sess.run(product)\n",
    "print(result)\n",
    "# ==> [[ 12.]]\n",
    "\n",
    "# Close the Session when we're done.\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 12.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Sessions should be closed to release resources.\n",
    "# You can also enter a Session with a \"with\" block. \n",
    "# The Session closes automatically at the end of the with block.\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  result = sess.run([product])\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "with tf.Session() as sess:\n",
    "  with tf.device(\"/cpu:0\"): #/cpu:0 /gpu:0 /gpu:1 /gpu:2\n",
    "    matrix1 = tf.constant([[3., 3.]])\n",
    "    matrix2 = tf.constant([[2.],[2.]])\n",
    "    product = tf.matmul(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maintain state with Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Create a Variable, that will be initialized to the \n",
    "# scalar value 0.\n",
    "state = tf.Variable(0, name=\"counter\")\n",
    "\n",
    "# Create an Op to add one to `state`.\n",
    "\n",
    "one = tf.constant(1)\n",
    "new_value = tf.add(state, one)\n",
    "update = tf.assign(state, new_value)\n",
    "\n",
    "# Variables must be initialized by running an `init` \n",
    "# Op after having\n",
    "# launched the graph.  We first have to add the `init` \n",
    "# Op to the graph.\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph and run the ops.\n",
    "with tf.Session() as sess:\n",
    "  # Run the 'init' op\n",
    "  sess.run(init_op)\n",
    "  # Print the initial value of 'state'\n",
    "  print(sess.run(state))\n",
    "  # Run the op that updates 'state' and print 'state'.\n",
    "  for _ in range(3):\n",
    "    sess.run(update)\n",
    "    print(sess.run(state))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use feeds and fetches to get data in and out of graph executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 21.], dtype=float32), array([ 7.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#Fetches:\n",
    "input1 = tf.constant([3.0])\n",
    "input2 = tf.constant([2.0])\n",
    "input3 = tf.constant([5.0])\n",
    "intermed = tf.add(input2, input3)\n",
    "mul = input1 * intermed\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  result = sess.run([mul, intermed])\n",
    "  print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 14.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#Feeds\n",
    "input1 = tf.placeholder(tf.float32)\n",
    "input2 = tf.placeholder(tf.float32)\n",
    "output = input1 * input2\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  print(sess.run([output], feed_dict={input1:[7.], input2:[2.]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression example\n",
    "\n",
    "<img src=\"http://www.atmos.washington.edu/~robwood/teaching/451/labs/images/concepts12.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAHVCAYAAAAHJgkZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f63bdc822b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reset\n",
    "import numpy as np\n",
    "trainX = np.linspace(-1,1, 200 ) #200 evenly spaced samples\n",
    "trainY = (2 * trainX + 1.0 + np.random.randn(*trainX.shape)*0.8)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure( figsize=(8, 8))\n",
    "plt.plot(trainX, trainY, 'bo', markeredgecolor='none')\n",
    "plt.ylabel('Line with noise')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 1])\n",
    "Y = tf.placeholder(\"float\", [None, 1])\n",
    "\n",
    "# Set model weights\n",
    "w = tf.Variable(np.random.randn(), name=\"weight\")\n",
    "b = tf.Variable(np.random.randn(), name=\"bias\")\n",
    "\n",
    "# our model y=xw+b\n",
    "prediction = X*w + b\n",
    "loss = tf.reduce_mean(tf.pow(prediction-Y, 2))\n",
    "\n",
    "var_grads = tf.gradients(loss, [w,b])\n",
    "\n",
    "learning_rate = tf.constant(0.3)\n",
    "\n",
    "new_w =  w - var_grads[0] * learning_rate \n",
    "new_b =  b - var_grads[1] * learning_rate\n",
    "\n",
    "update_w = tf.assign(w, new_w)\n",
    "update_b = tf.assign(b, new_b)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlUAAAJCCAYAAADp1TKRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f631c243390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:  0.667029  w: [2.0713215]  b: [1.0017048]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlUAAAJCCAYAAADp1TKRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f631c243390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as plt\n",
    "from IPython import display\n",
    "\n",
    "fig = plt.figure( figsize=(10, 10))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for epoch in range(15):\n",
    "        [current_loss, _, _ ] = sess.run(\n",
    "                [loss, update_w, update_b ], \n",
    "                feed_dict={X:trainX.reshape([-1,1]),\n",
    "                           Y:trainY.reshape([-1,1])})\n",
    "        current_w = sess.run([w])\n",
    "        current_b = sess.run([b])\n",
    "        \n",
    "\n",
    "        plt.clf()\n",
    "        plt.plot(trainX, trainY,'o',color='b', markeredgecolor='none')\n",
    "        plt.plot(trainX, trainX*current_w + current_b, color='r',)\n",
    "        print('Cost: ', current_loss,' w:',current_w, ' b:',current_b)\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "print('Cost: ', current_loss,' w:',current_w, ' b:',current_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CNN example with MNIST\n",
    "\n",
    "Dataset of images of handwritten digits like these\n",
    "\n",
    "<div style=\"width: 400px\">\n",
    "![esto puede in en el style float:left;](https://www.tensorflow.org/images/MNIST.png)\n",
    "</div>\n",
    " Each image is 28 by 28 pixels. Can be interpreted as a matrix:\n",
    "<div style=\"width: 200px\">\n",
    "![MNIST Pixels](https://www.tensorflow.org/images/MNIST-Matrix.png)\n",
    "</div>\n",
    "\n",
    "<div style=\"float:left;width: 400px\">\n",
    "![train ](https://www.tensorflow.org/images/mnist-train-xs.png)\n",
    "</div>\n",
    "\n",
    "<div style=\"float:right;width: 500px\">\n",
    "![1hot labels ](https://www.tensorflow.org/images/mnist-train-ys.png)\n",
    "</div>\n",
    "<br><br><br><br><br><br><br><br><br>\n",
    "Images are = [55000, 28 ,28]    \n",
    "reshaped into [55000, 784]         \n",
    "\n",
    "Labels are [55000, 10]\n",
    "(One-hot encoding of classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![lenet](http://www.pyimagesearch.com/wp-content/uploads/2016/06/lenet_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "Extracting ./MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_DATA/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./MNIST_DATA\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class MNIST_CNN():\n",
    "    \"\"\"\n",
    "    A network for a classifier of digit images.\n",
    "    Input are 2D grayscale images reshaped into a vector, and the labels\n",
    "    in one-hot encoding\n",
    "    Uses 2 Conv Layers, 1 hidden layer and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_length, hidden_layer_size = 256, num_classes=10):\n",
    "        self.input_x = tf.placeholder(tf.float32, \n",
    "                            [None, img_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, \n",
    "                            [None, num_classes], name=\"input_y\")\n",
    "\n",
    "        # Input Layer\n",
    "        # Dimesnions are [batch_size, image_width, image_height, channels]\n",
    "        self.input_layer = tf.reshape(self.input_x, [-1, 28, 28, 1]) \n",
    "\n",
    "\n",
    "        # Convolutional Layer #1\n",
    "        with tf.name_scope(\"Conv_1\"):\n",
    "            nfilters1=8\n",
    "            self.W_conv1=tf.Variable(\n",
    "                tf.truncated_normal([5, 5, 1, nfilters1], stddev=0.1), \n",
    "                name=\"W\")\n",
    "            b = tf.Variable(\n",
    "                tf.constant(0.1, shape=[nfilters1]), \n",
    "                name=\"b\")\n",
    "            conv_out = tf.nn.conv2d(self.input_layer, self.W_conv1, \n",
    "                                 strides=[1, 1, 1, 1], padding='SAME') \n",
    "            self.conv1 = tf.nn.relu(conv_out + b)\n",
    "            self.pool1 = tf.nn.max_pool(self.conv1, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "            \n",
    "            #self.conv1 = tf.layers.conv2d(\n",
    "            #      inputs=self.input_layer,\n",
    "            #      filters=32,\n",
    "            #      kernel_size=[5, 5],\n",
    "            #      padding=\"same\",\n",
    "            #      activation=tf.nn.relu)\n",
    "            # Pooling Layer #1\n",
    "            #self.pool1 = tf.layers.max_pooling2d(inputs=self.conv1,\n",
    "            #pool_size=[2, 2], strides=2)\n",
    "\n",
    "        # Convolutional Layer #2 and Pooling Layer #2\n",
    "        with tf.name_scope(\"Conv_2\"):\n",
    "            nfilters2=16\n",
    "            self.W_conv2=tf.Variable(\n",
    "                tf.truncated_normal([3, 3, nfilters1, nfilters2], stddev=0.1), \n",
    "                name=\"W\")\n",
    "            b = tf.Variable(\n",
    "                tf.constant(0.1, shape=[nfilters2]), \n",
    "                name=\"b\")\n",
    "            \n",
    "            conv_out = tf.nn.conv2d(self.pool1, self.W_conv2, \n",
    "                                 strides=[1, 1, 1, 1], padding='SAME') \n",
    "            self.conv2 = tf.nn.relu(conv_out + b)\n",
    "            self.pool2 = tf.nn.max_pool(self.conv2, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "            #self.conv2 = tf.layers.conv2d(\n",
    "            #      inputs=self.pool1,\n",
    "            #      filters=64,\n",
    "            #      kernel_size=[5, 5],\n",
    "            #      padding=\"same\",\n",
    "            #      activation=tf.nn.relu)\n",
    "            #self.pool2 = tf.layers.max_pooling2d(inputs=self.conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "        #Flat feature maps\n",
    "        self.pool2_flat = tf.reshape(self.pool2, \n",
    "                                     [-1, 7 * 7 * nfilters2])\n",
    "\n",
    "        # Dense Layer\n",
    "        with tf.name_scope(\"hidden_layer_1\"):\n",
    "            W = tf.Variable(\n",
    "                tf.truncated_normal([7 * 7 * nfilters2, hidden_layer_size], stddev=0.1),\n",
    "                name=\"W\")\n",
    "            b = tf.Variable(\n",
    "                tf.constant(0.1, shape=[hidden_layer_size]), \n",
    "                name=\"b\")\n",
    "            self.hl1 = tf.nn.relu ( tf.matmul(self.pool2_flat, W) + b )\n",
    "            #dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"softmax_layer\"):\n",
    "            W = tf.Variable(\n",
    "                tf.truncated_normal([hidden_layer_size, num_classes], stddev=0.1), \n",
    "                name=\"W\")\n",
    "            b = tf.Variable(\n",
    "                tf.constant(0.1, shape=[num_classes]), \n",
    "                name=\"b\")\n",
    "            self.scores = tf.nn.xw_plus_b(self.hl1, W, b, \n",
    "                                          name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, \n",
    "                                         name=\"predictions\")\n",
    "\n",
    "\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, \n",
    "                                           tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(\n",
    "                tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#sess = tf.Session()\n",
    "sess = tf.InteractiveSession() #for running inside notebook\n",
    "\n",
    "mnist_dnn = MNIST_CNN(img_length=784, \n",
    "                      hidden_layer_size = 128, num_classes=10)\n",
    "\n",
    "    # Define Training procedure\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "train_op = tf.train.GradientDescentOptimizer(0.05).minimize(\n",
    "    mnist_dnn.loss, global_step=global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/csp/repo/deeplearning-tutorials/intro_tensorflow/runs/1490823626\n",
      "\n",
      "Conv_1/W:0\n",
      "Conv_1/b:0\n",
      "Conv_2/W:0\n",
      "Conv_2/b:0\n",
      "hidden_layer_1/W:0\n",
      "hidden_layer_1/b:0\n",
      "softmax_layer/W:0\n",
      "softmax_layer/b:0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import numpy as np\n",
    "\n",
    "timestamp = str(int(time.time()))\n",
    "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "# Summaries for loss and accuracy\n",
    "loss_summary = tf.summary.scalar(\"loss\", mnist_dnn.loss)\n",
    "acc_summary = tf.summary.scalar(\"accuracy\", mnist_dnn.accuracy)\n",
    "\n",
    "\n",
    "with tf.name_scope('var_summary'):\n",
    "    for var in tf.trainable_variables():\n",
    "        with tf.name_scope(var.name.split(':')[0]):\n",
    "            print(var.name)\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            tf.summary.histogram('histogram', var)\n",
    "\n",
    "with tf.variable_scope('visualization_conv1'):\n",
    "    # scale weights to [0 1], type is still float\n",
    "    kernel = mnist_dnn.W_conv1\n",
    "    x_min = tf.reduce_min(kernel)\n",
    "    x_max = tf.reduce_max(kernel)\n",
    "    kernel_0_to_1 = (kernel - x_min) / (x_max - x_min)\n",
    "\n",
    "    # to tf.image_summary format [batch_size, height, width, channels]\n",
    "    kernel_transposed = tf.transpose (kernel_0_to_1, [3, 0, 1, 2])\n",
    "\n",
    "    # this will display random 3 filters from the 32 in conv1\n",
    "    tf.summary.image('filters', kernel_transposed, max_outputs=16)\n",
    "\n",
    "with tf.variable_scope('visualization_conv2'):    \n",
    "    kernel = mnist_dnn.W_conv2\n",
    "    x_min = tf.reduce_min(kernel)\n",
    "    x_max = tf.reduce_max(kernel)\n",
    "    kernel_0_to_1 = (kernel - x_min) / (x_max - x_min)\n",
    "\n",
    "    # to tf.image_summary format [batch_size, height, width, channels]\n",
    "    kernel_transposed = tf.transpose (kernel_0_to_1, [3, 0, 1, 2])\n",
    "    kernel_transposed = tf.slice(kernel_transposed, [0, 0, 0,0], [-1, -1, -1, 1]) \n",
    "\n",
    "    # this will display random 3 filters from the 32 in conv1\n",
    "    tf.summary.image('conv2/filters', kernel_transposed, max_outputs=16)\n",
    "            \n",
    "image_features = tf.Variable(tf.truncated_normal([10000, 128], stddev=0.1), trainable=False, name='image_features')\n",
    "# Train Summaries\n",
    "#train_summary_op = tf.summary.merge([loss_summary, acc_summary ])\n",
    "train_summary_op = tf.summary.merge_all()\n",
    "train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "\n",
    "\n",
    "config = projector.ProjectorConfig()\n",
    "embed= config.embeddings.add()\n",
    "embed.tensor_name = image_features.name\n",
    "embed.metadata_path = os.path.join(train_summary_dir + '/metadata.tsv')\n",
    "embed.sprite.image_path = os.path.join(train_summary_dir + '/mnist_10k_sprite.png')\n",
    "from shutil import copyfile\n",
    "copyfile('./mnist_10k_sprite.png', os.path.join(train_summary_dir + '/mnist_10k_sprite.png'))\n",
    "\n",
    "# Specify the width and height of a single thumbnail.\n",
    "embed.sprite.single_image_dim.extend([28, 28])\n",
    "projector.visualize_embeddings(train_summary_writer, config)\n",
    "\n",
    "def save_metadata(file):\n",
    "    with open(file, 'w') as f:\n",
    "        for i in range(10000):\n",
    "            c = np.nonzero(mnist.test.labels[::1])[1:][0][i]\n",
    "            f.write('{}\\n'.format(c))\n",
    "\n",
    "save_metadata(embed.metadata_path)\n",
    "get_image_features_op = tf.assign(image_features, mnist_dnn.hl1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_prefix = os.path.join(train_summary_dir, \"model\")\n",
    "if not os.path.exists(checkpoint_prefix):\n",
    "    os.makedirs(checkpoint_prefix)\n",
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-03-29T23:40:39.115174: step 1, loss 2.30453, acc 0.164\n",
      "2017-03-29T23:40:39.283402: step 2, loss 2.31818, acc 0.136\n",
      "2017-03-29T23:40:39.460833: step 3, loss 2.28227, acc 0.188\n",
      "2017-03-29T23:40:39.642522: step 4, loss 2.26999, acc 0.172\n",
      "2017-03-29T23:40:39.810121: step 5, loss 2.27837, acc 0.136\n",
      "2017-03-29T23:40:39.960146: step 6, loss 2.26453, acc 0.21\n",
      "2017-03-29T23:40:40.118439: step 7, loss 2.25886, acc 0.206\n",
      "2017-03-29T23:40:40.277039: step 8, loss 2.2425, acc 0.26\n",
      "2017-03-29T23:40:40.429632: step 9, loss 2.24161, acc 0.276\n",
      "2017-03-29T23:40:40.573604: step 10, loss 2.21785, acc 0.358\n",
      "2017-03-29T23:40:40.732068: step 11, loss 2.21931, acc 0.306\n",
      "2017-03-29T23:40:40.886847: step 12, loss 2.21568, acc 0.336\n",
      "2017-03-29T23:40:41.058454: step 13, loss 2.2022, acc 0.366\n",
      "2017-03-29T23:40:41.221271: step 14, loss 2.21093, acc 0.322\n",
      "2017-03-29T23:40:41.374007: step 15, loss 2.18767, acc 0.412\n",
      "2017-03-29T23:40:41.513828: step 16, loss 2.18783, acc 0.348\n",
      "2017-03-29T23:40:41.678151: step 17, loss 2.18252, acc 0.39\n",
      "2017-03-29T23:40:41.825940: step 18, loss 2.16215, acc 0.408\n",
      "2017-03-29T23:40:41.977815: step 19, loss 2.14814, acc 0.416\n",
      "2017-03-29T23:40:42.138095: step 20, loss 2.14663, acc 0.4\n",
      "2017-03-29T23:40:42.293304: step 21, loss 2.12476, acc 0.46\n",
      "2017-03-29T23:40:42.466152: step 22, loss 2.11833, acc 0.446\n",
      "2017-03-29T23:40:42.631449: step 23, loss 2.09826, acc 0.452\n",
      "2017-03-29T23:40:42.797113: step 24, loss 2.08755, acc 0.512\n",
      "2017-03-29T23:40:42.953171: step 25, loss 2.05713, acc 0.55\n",
      "2017-03-29T23:40:43.121575: step 26, loss 2.04955, acc 0.454\n",
      "2017-03-29T23:40:43.288381: step 27, loss 2.02286, acc 0.466\n",
      "2017-03-29T23:40:43.460258: step 28, loss 1.98005, acc 0.608\n",
      "2017-03-29T23:40:43.626376: step 29, loss 1.95465, acc 0.608\n",
      "2017-03-29T23:40:43.764299: step 30, loss 1.8932, acc 0.644\n",
      "2017-03-29T23:40:43.912193: step 31, loss 1.89745, acc 0.574\n",
      "2017-03-29T23:40:44.060608: step 32, loss 1.85362, acc 0.632\n",
      "2017-03-29T23:40:44.228824: step 33, loss 1.78064, acc 0.642\n",
      "2017-03-29T23:40:44.401318: step 34, loss 1.70433, acc 0.708\n",
      "2017-03-29T23:40:44.538022: step 35, loss 1.68958, acc 0.624\n",
      "2017-03-29T23:40:44.713162: step 36, loss 1.64228, acc 0.61\n",
      "2017-03-29T23:40:44.874850: step 37, loss 1.53169, acc 0.656\n",
      "2017-03-29T23:40:45.040868: step 38, loss 1.5256, acc 0.682\n",
      "2017-03-29T23:40:45.178785: step 39, loss 1.4334, acc 0.66\n",
      "2017-03-29T23:40:45.332000: step 40, loss 1.38273, acc 0.706\n",
      "2017-03-29T23:40:45.482466: step 41, loss 1.2501, acc 0.72\n",
      "2017-03-29T23:40:45.637288: step 42, loss 1.20523, acc 0.702\n",
      "2017-03-29T23:40:45.786753: step 43, loss 1.18812, acc 0.636\n",
      "2017-03-29T23:40:45.950652: step 44, loss 1.25321, acc 0.638\n",
      "2017-03-29T23:40:46.116565: step 45, loss 1.27937, acc 0.564\n",
      "2017-03-29T23:40:46.265171: step 46, loss 1.44633, acc 0.57\n",
      "2017-03-29T23:40:46.423456: step 47, loss 1.25952, acc 0.622\n",
      "2017-03-29T23:40:46.574450: step 48, loss 1.40696, acc 0.544\n",
      "2017-03-29T23:40:46.733422: step 49, loss 1.2908, acc 0.64\n",
      "2017-03-29T23:40:46.875351: step 50, loss 1.11541, acc 0.664\n",
      "2017-03-29T23:40:47.035222: step 51, loss 0.98914, acc 0.742\n",
      "2017-03-29T23:40:47.187637: step 52, loss 0.953047, acc 0.722\n",
      "2017-03-29T23:40:47.350340: step 53, loss 0.950796, acc 0.748\n",
      "2017-03-29T23:40:47.523205: step 54, loss 0.878441, acc 0.74\n",
      "2017-03-29T23:40:47.670498: step 55, loss 1.00643, acc 0.672\n",
      "2017-03-29T23:40:47.821327: step 56, loss 1.16034, acc 0.658\n",
      "2017-03-29T23:40:47.989475: step 57, loss 1.2746, acc 0.672\n",
      "2017-03-29T23:40:48.158216: step 58, loss 0.811996, acc 0.748\n",
      "2017-03-29T23:40:48.300296: step 59, loss 0.750717, acc 0.78\n",
      "2017-03-29T23:40:48.444536: step 60, loss 0.79079, acc 0.774\n",
      "2017-03-29T23:40:48.580331: step 61, loss 0.750432, acc 0.758\n",
      "2017-03-29T23:40:48.743737: step 62, loss 0.805035, acc 0.722\n",
      "2017-03-29T23:40:48.912054: step 63, loss 0.762439, acc 0.72\n",
      "2017-03-29T23:40:49.069242: step 64, loss 0.712726, acc 0.73\n",
      "2017-03-29T23:40:49.235235: step 65, loss 1.10633, acc 0.652\n",
      "2017-03-29T23:40:49.401637: step 66, loss 0.898947, acc 0.696\n",
      "2017-03-29T23:40:49.570768: step 67, loss 0.732756, acc 0.778\n",
      "2017-03-29T23:40:49.723881: step 68, loss 0.604433, acc 0.826\n",
      "2017-03-29T23:40:49.876169: step 69, loss 0.591408, acc 0.85\n",
      "2017-03-29T23:40:50.040230: step 70, loss 0.620159, acc 0.824\n",
      "2017-03-29T23:40:50.196064: step 71, loss 0.497889, acc 0.87\n",
      "2017-03-29T23:40:50.359310: step 72, loss 0.468605, acc 0.874\n",
      "2017-03-29T23:40:50.527240: step 73, loss 0.610018, acc 0.828\n",
      "2017-03-29T23:40:50.692357: step 74, loss 0.550438, acc 0.84\n",
      "2017-03-29T23:40:50.863058: step 75, loss 0.604467, acc 0.8\n",
      "2017-03-29T23:40:51.004777: step 76, loss 0.567514, acc 0.83\n",
      "2017-03-29T23:40:51.166266: step 77, loss 0.506221, acc 0.834\n",
      "2017-03-29T23:40:51.317675: step 78, loss 0.532607, acc 0.836\n",
      "2017-03-29T23:40:51.480576: step 79, loss 0.68868, acc 0.754\n",
      "2017-03-29T23:40:51.626514: step 80, loss 0.655964, acc 0.74\n",
      "2017-03-29T23:40:51.788810: step 81, loss 0.759538, acc 0.742\n",
      "2017-03-29T23:40:51.956158: step 82, loss 0.544274, acc 0.834\n",
      "2017-03-29T23:40:52.103836: step 83, loss 0.641286, acc 0.814\n",
      "2017-03-29T23:40:52.250065: step 84, loss 0.468793, acc 0.848\n",
      "2017-03-29T23:40:52.416110: step 85, loss 0.472981, acc 0.852\n",
      "2017-03-29T23:40:52.568410: step 86, loss 0.475801, acc 0.852\n",
      "2017-03-29T23:40:52.718094: step 87, loss 0.407731, acc 0.888\n",
      "2017-03-29T23:40:52.864704: step 88, loss 0.457159, acc 0.862\n",
      "2017-03-29T23:40:53.007199: step 89, loss 0.516713, acc 0.838\n",
      "2017-03-29T23:40:53.161422: step 90, loss 0.539924, acc 0.854\n",
      "2017-03-29T23:40:53.319753: step 91, loss 0.507432, acc 0.862\n",
      "2017-03-29T23:40:53.466262: step 92, loss 0.392559, acc 0.902\n",
      "2017-03-29T23:40:53.616815: step 93, loss 0.437631, acc 0.876\n",
      "2017-03-29T23:40:53.758880: step 94, loss 0.445563, acc 0.87\n",
      "2017-03-29T23:40:53.916116: step 95, loss 0.517087, acc 0.82\n",
      "2017-03-29T23:40:54.086148: step 96, loss 0.482062, acc 0.876\n",
      "2017-03-29T23:40:54.235585: step 97, loss 0.341022, acc 0.908\n",
      "2017-03-29T23:40:54.397222: step 98, loss 0.404516, acc 0.89\n",
      "2017-03-29T23:40:54.568972: step 99, loss 0.394023, acc 0.87\n",
      "2017-03-29T23:40:54.726637: step 100, loss 0.484274, acc 0.844\n",
      "2017-03-29T23:40:54.886837: step 101, loss 0.417376, acc 0.848\n",
      "2017-03-29T23:40:55.052879: step 102, loss 0.37584, acc 0.872\n",
      "2017-03-29T23:40:55.228166: step 103, loss 0.401593, acc 0.862\n",
      "2017-03-29T23:40:55.401752: step 104, loss 0.335189, acc 0.888\n",
      "2017-03-29T23:40:55.555739: step 105, loss 0.366277, acc 0.882\n",
      "2017-03-29T23:40:55.723559: step 106, loss 0.402745, acc 0.862\n",
      "2017-03-29T23:40:55.880597: step 107, loss 0.250905, acc 0.908\n",
      "2017-03-29T23:40:56.018818: step 108, loss 0.229362, acc 0.94\n",
      "2017-03-29T23:40:56.169871: step 109, loss 0.270968, acc 0.934\n",
      "2017-03-29T23:40:56.311099: step 110, loss 0.298522, acc 0.942\n",
      "2017-03-29T23:40:56.558756: step 111, loss 0.401808, acc 0.888\n",
      "2017-03-29T23:40:56.695439: step 112, loss 0.349819, acc 0.9\n",
      "2017-03-29T23:40:56.846116: step 113, loss 0.288248, acc 0.924\n",
      "2017-03-29T23:40:57.006513: step 114, loss 0.364927, acc 0.888\n",
      "2017-03-29T23:40:57.167016: step 115, loss 0.3544, acc 0.904\n",
      "2017-03-29T23:40:57.316708: step 116, loss 0.309396, acc 0.912\n",
      "2017-03-29T23:40:57.457792: step 117, loss 0.35308, acc 0.902\n",
      "2017-03-29T23:40:57.621666: step 118, loss 0.316732, acc 0.894\n",
      "2017-03-29T23:40:57.781812: step 119, loss 0.380646, acc 0.892\n",
      "2017-03-29T23:40:57.949832: step 120, loss 0.470925, acc 0.852\n",
      "2017-03-29T23:40:58.097443: step 121, loss 0.557681, acc 0.804\n",
      "2017-03-29T23:40:58.250155: step 122, loss 0.365823, acc 0.87\n",
      "2017-03-29T23:40:58.415173: step 123, loss 0.387016, acc 0.876\n",
      "2017-03-29T23:40:58.564530: step 124, loss 0.394598, acc 0.878\n",
      "2017-03-29T23:40:58.713167: step 125, loss 0.354989, acc 0.906\n",
      "2017-03-29T23:40:58.866180: step 126, loss 0.316919, acc 0.902\n",
      "2017-03-29T23:40:59.014582: step 127, loss 0.32218, acc 0.918\n",
      "2017-03-29T23:40:59.182762: step 128, loss 0.301182, acc 0.922\n",
      "2017-03-29T23:40:59.331711: step 129, loss 0.371589, acc 0.89\n",
      "2017-03-29T23:40:59.490467: step 130, loss 0.306571, acc 0.912\n",
      "2017-03-29T23:40:59.652071: step 131, loss 0.347821, acc 0.892\n",
      "2017-03-29T23:40:59.813173: step 132, loss 0.290441, acc 0.902\n",
      "2017-03-29T23:40:59.980888: step 133, loss 0.268471, acc 0.936\n",
      "2017-03-29T23:41:00.130533: step 134, loss 0.29888, acc 0.918\n",
      "2017-03-29T23:41:00.297488: step 135, loss 0.331738, acc 0.902\n",
      "2017-03-29T23:41:00.445142: step 136, loss 0.246646, acc 0.938\n",
      "2017-03-29T23:41:00.612346: step 137, loss 0.320953, acc 0.912\n",
      "2017-03-29T23:41:00.780169: step 138, loss 0.295029, acc 0.914\n",
      "2017-03-29T23:41:00.945773: step 139, loss 0.256664, acc 0.924\n",
      "2017-03-29T23:41:01.106063: step 140, loss 0.373343, acc 0.896\n",
      "2017-03-29T23:41:01.274559: step 141, loss 0.312486, acc 0.914\n",
      "2017-03-29T23:41:01.425863: step 142, loss 0.236911, acc 0.932\n",
      "2017-03-29T23:41:01.593824: step 143, loss 0.309238, acc 0.906\n",
      "2017-03-29T23:41:01.766965: step 144, loss 0.281721, acc 0.918\n",
      "2017-03-29T23:41:01.914398: step 145, loss 0.348736, acc 0.91\n",
      "2017-03-29T23:41:02.078765: step 146, loss 0.321193, acc 0.886\n",
      "2017-03-29T23:41:02.223595: step 147, loss 0.299031, acc 0.914\n",
      "2017-03-29T23:41:02.391394: step 148, loss 0.31643, acc 0.902\n",
      "2017-03-29T23:41:02.555710: step 149, loss 0.240893, acc 0.93\n",
      "2017-03-29T23:41:02.706127: step 150, loss 0.266442, acc 0.916\n",
      "2017-03-29T23:41:02.853508: step 151, loss 0.294306, acc 0.906\n",
      "2017-03-29T23:41:03.025712: step 152, loss 0.309653, acc 0.908\n",
      "2017-03-29T23:41:03.164161: step 153, loss 0.310492, acc 0.898\n",
      "2017-03-29T23:41:03.310211: step 154, loss 0.338045, acc 0.908\n",
      "2017-03-29T23:41:03.461465: step 155, loss 0.253502, acc 0.926\n",
      "2017-03-29T23:41:03.627476: step 156, loss 0.280568, acc 0.912\n",
      "2017-03-29T23:41:03.776153: step 157, loss 0.265442, acc 0.928\n",
      "2017-03-29T23:41:03.936645: step 158, loss 0.274334, acc 0.916\n",
      "2017-03-29T23:41:04.119718: step 159, loss 0.32877, acc 0.91\n",
      "2017-03-29T23:41:04.267451: step 160, loss 0.307531, acc 0.91\n",
      "2017-03-29T23:41:04.413064: step 161, loss 0.273103, acc 0.914\n",
      "2017-03-29T23:41:04.557080: step 162, loss 0.256334, acc 0.918\n",
      "2017-03-29T23:41:04.720078: step 163, loss 0.270645, acc 0.932\n",
      "2017-03-29T23:41:04.864772: step 164, loss 0.282007, acc 0.922\n",
      "2017-03-29T23:41:05.018424: step 165, loss 0.253743, acc 0.922\n",
      "2017-03-29T23:41:05.176295: step 166, loss 0.284543, acc 0.91\n",
      "2017-03-29T23:41:05.345250: step 167, loss 0.286675, acc 0.932\n",
      "2017-03-29T23:41:05.509875: step 168, loss 0.254407, acc 0.926\n",
      "2017-03-29T23:41:05.671136: step 169, loss 0.288891, acc 0.912\n",
      "2017-03-29T23:41:05.835100: step 170, loss 0.313842, acc 0.918\n",
      "2017-03-29T23:41:05.984375: step 171, loss 0.290249, acc 0.91\n",
      "2017-03-29T23:41:06.149058: step 172, loss 0.235033, acc 0.922\n",
      "2017-03-29T23:41:06.295717: step 173, loss 0.293499, acc 0.912\n",
      "2017-03-29T23:41:06.456120: step 174, loss 0.287502, acc 0.902\n",
      "2017-03-29T23:41:06.605695: step 175, loss 0.269929, acc 0.924\n",
      "2017-03-29T23:41:06.772368: step 176, loss 0.256805, acc 0.926\n",
      "2017-03-29T23:41:06.936973: step 177, loss 0.304209, acc 0.902\n",
      "2017-03-29T23:41:07.088500: step 178, loss 0.267864, acc 0.932\n",
      "2017-03-29T23:41:07.246287: step 179, loss 0.247259, acc 0.912\n",
      "2017-03-29T23:41:07.412405: step 180, loss 0.238853, acc 0.922\n",
      "2017-03-29T23:41:07.558197: step 181, loss 0.258987, acc 0.92\n",
      "2017-03-29T23:41:07.713187: step 182, loss 0.251984, acc 0.924\n",
      "2017-03-29T23:41:07.873942: step 183, loss 0.222619, acc 0.938\n",
      "2017-03-29T23:41:08.044529: step 184, loss 0.253315, acc 0.914\n",
      "2017-03-29T23:41:08.212474: step 185, loss 0.243071, acc 0.942\n",
      "2017-03-29T23:41:08.360452: step 186, loss 0.23127, acc 0.938\n",
      "2017-03-29T23:41:08.511556: step 187, loss 0.210814, acc 0.942\n",
      "2017-03-29T23:41:08.662457: step 188, loss 0.199181, acc 0.944\n",
      "2017-03-29T23:41:08.815513: step 189, loss 0.238259, acc 0.936\n",
      "2017-03-29T23:41:08.975506: step 190, loss 0.243939, acc 0.93\n",
      "2017-03-29T23:41:09.145231: step 191, loss 0.226583, acc 0.93\n",
      "2017-03-29T23:41:09.295660: step 192, loss 0.267639, acc 0.924\n",
      "2017-03-29T23:41:09.446743: step 193, loss 0.252243, acc 0.926\n",
      "2017-03-29T23:41:09.590280: step 194, loss 0.311683, acc 0.906\n",
      "2017-03-29T23:41:09.752958: step 195, loss 0.289921, acc 0.914\n",
      "2017-03-29T23:41:09.898694: step 196, loss 0.251467, acc 0.922\n",
      "2017-03-29T23:41:10.078022: step 197, loss 0.274864, acc 0.928\n",
      "2017-03-29T23:41:10.241490: step 198, loss 0.275004, acc 0.924\n",
      "2017-03-29T23:41:10.379222: step 199, loss 0.25255, acc 0.932\n",
      "2017-03-29T23:41:10.551870: step 200, loss 0.23745, acc 0.924\n",
      "2017-03-29T23:41:10.723140: step 201, loss 0.273903, acc 0.92\n",
      "2017-03-29T23:41:10.892413: step 202, loss 0.262914, acc 0.934\n",
      "2017-03-29T23:41:11.042146: step 203, loss 0.225401, acc 0.932\n",
      "2017-03-29T23:41:11.192481: step 204, loss 0.224147, acc 0.944\n",
      "2017-03-29T23:41:11.336865: step 205, loss 0.242805, acc 0.934\n",
      "2017-03-29T23:41:11.478784: step 206, loss 0.237532, acc 0.932\n",
      "2017-03-29T23:41:11.643689: step 207, loss 0.233548, acc 0.93\n",
      "2017-03-29T23:41:11.809182: step 208, loss 0.190948, acc 0.94\n",
      "2017-03-29T23:41:11.957874: step 209, loss 0.210695, acc 0.944\n",
      "2017-03-29T23:41:12.110359: step 210, loss 0.263703, acc 0.924\n",
      "2017-03-29T23:41:12.286369: step 211, loss 0.232157, acc 0.938\n",
      "2017-03-29T23:41:12.433620: step 212, loss 0.210135, acc 0.938\n",
      "2017-03-29T23:41:12.589321: step 213, loss 0.219689, acc 0.94\n",
      "2017-03-29T23:41:12.757973: step 214, loss 0.238826, acc 0.932\n",
      "2017-03-29T23:41:12.924375: step 215, loss 0.268728, acc 0.932\n",
      "2017-03-29T23:41:13.069281: step 216, loss 0.1871, acc 0.946\n",
      "2017-03-29T23:41:13.227085: step 217, loss 0.195284, acc 0.946\n",
      "2017-03-29T23:41:13.395455: step 218, loss 0.213392, acc 0.922\n",
      "2017-03-29T23:41:13.563064: step 219, loss 0.224416, acc 0.926\n",
      "2017-03-29T23:41:13.713286: step 220, loss 0.226055, acc 0.926\n",
      "2017-03-29T23:41:13.956966: step 221, loss 0.246159, acc 0.934\n",
      "2017-03-29T23:41:14.119414: step 222, loss 0.251557, acc 0.93\n",
      "2017-03-29T23:41:14.283852: step 223, loss 0.218704, acc 0.938\n",
      "2017-03-29T23:41:14.428279: step 224, loss 0.273014, acc 0.932\n",
      "2017-03-29T23:41:14.595353: step 225, loss 0.223479, acc 0.922\n",
      "2017-03-29T23:41:14.730859: step 226, loss 0.233269, acc 0.922\n",
      "2017-03-29T23:41:14.883089: step 227, loss 0.207596, acc 0.942\n",
      "2017-03-29T23:41:15.032595: step 228, loss 0.216306, acc 0.928\n",
      "2017-03-29T23:41:15.171897: step 229, loss 0.212723, acc 0.942\n",
      "2017-03-29T23:41:15.334421: step 230, loss 0.267252, acc 0.934\n",
      "2017-03-29T23:41:15.489032: step 231, loss 0.180677, acc 0.944\n",
      "2017-03-29T23:41:15.628540: step 232, loss 0.203253, acc 0.948\n",
      "2017-03-29T23:41:15.774683: step 233, loss 0.195381, acc 0.94\n",
      "2017-03-29T23:41:15.923642: step 234, loss 0.255219, acc 0.924\n",
      "2017-03-29T23:41:16.095892: step 235, loss 0.25045, acc 0.912\n",
      "2017-03-29T23:41:16.262042: step 236, loss 0.273079, acc 0.9\n",
      "2017-03-29T23:41:16.432485: step 237, loss 0.27337, acc 0.916\n",
      "2017-03-29T23:41:16.585268: step 238, loss 0.246976, acc 0.918\n",
      "2017-03-29T23:41:16.745316: step 239, loss 0.274098, acc 0.92\n",
      "2017-03-29T23:41:16.906140: step 240, loss 0.196313, acc 0.938\n",
      "2017-03-29T23:41:17.053547: step 241, loss 0.186823, acc 0.946\n",
      "2017-03-29T23:41:17.205135: step 242, loss 0.169211, acc 0.96\n",
      "2017-03-29T23:41:17.356457: step 243, loss 0.227791, acc 0.944\n",
      "2017-03-29T23:41:17.526368: step 244, loss 0.202795, acc 0.926\n",
      "2017-03-29T23:41:17.696112: step 245, loss 0.195921, acc 0.946\n",
      "2017-03-29T23:41:17.862866: step 246, loss 0.248025, acc 0.932\n",
      "2017-03-29T23:41:18.012991: step 247, loss 0.214945, acc 0.94\n",
      "2017-03-29T23:41:18.153986: step 248, loss 0.134342, acc 0.97\n",
      "2017-03-29T23:41:18.330071: step 249, loss 0.241188, acc 0.918\n",
      "2017-03-29T23:41:18.480448: step 250, loss 0.225487, acc 0.934\n",
      "2017-03-29T23:41:18.621978: step 251, loss 0.197662, acc 0.934\n",
      "2017-03-29T23:41:18.779925: step 252, loss 0.174783, acc 0.938\n",
      "2017-03-29T23:41:18.949961: step 253, loss 0.229665, acc 0.936\n",
      "2017-03-29T23:41:19.102012: step 254, loss 0.200896, acc 0.938\n",
      "2017-03-29T23:41:19.260644: step 255, loss 0.170827, acc 0.96\n",
      "2017-03-29T23:41:19.421171: step 256, loss 0.22948, acc 0.93\n",
      "2017-03-29T23:41:19.568172: step 257, loss 0.152468, acc 0.954\n",
      "2017-03-29T23:41:19.724583: step 258, loss 0.207116, acc 0.936\n",
      "2017-03-29T23:41:19.882310: step 259, loss 0.172845, acc 0.958\n",
      "2017-03-29T23:41:20.047483: step 260, loss 0.217712, acc 0.938\n",
      "2017-03-29T23:41:20.219911: step 261, loss 0.164412, acc 0.96\n",
      "2017-03-29T23:41:20.373163: step 262, loss 0.184338, acc 0.936\n",
      "2017-03-29T23:41:20.514874: step 263, loss 0.204657, acc 0.938\n",
      "2017-03-29T23:41:20.655062: step 264, loss 0.185982, acc 0.94\n",
      "2017-03-29T23:41:20.816843: step 265, loss 0.189316, acc 0.946\n",
      "2017-03-29T23:41:20.980414: step 266, loss 0.216317, acc 0.936\n",
      "2017-03-29T23:41:21.133914: step 267, loss 0.198939, acc 0.938\n",
      "2017-03-29T23:41:21.275348: step 268, loss 0.226861, acc 0.93\n",
      "2017-03-29T23:41:21.424471: step 269, loss 0.189973, acc 0.942\n",
      "2017-03-29T23:41:21.580836: step 270, loss 0.199949, acc 0.938\n",
      "2017-03-29T23:41:21.732051: step 271, loss 0.211668, acc 0.946\n",
      "2017-03-29T23:41:21.865719: step 272, loss 0.205783, acc 0.94\n",
      "2017-03-29T23:41:22.040336: step 273, loss 0.188444, acc 0.942\n",
      "2017-03-29T23:41:22.207859: step 274, loss 0.190134, acc 0.942\n",
      "2017-03-29T23:41:22.373246: step 275, loss 0.20458, acc 0.932\n",
      "2017-03-29T23:41:22.533033: step 276, loss 0.230253, acc 0.934\n",
      "2017-03-29T23:41:22.695379: step 277, loss 0.182898, acc 0.948\n",
      "2017-03-29T23:41:22.857061: step 278, loss 0.189661, acc 0.944\n",
      "2017-03-29T23:41:23.021013: step 279, loss 0.19157, acc 0.944\n",
      "2017-03-29T23:41:23.177603: step 280, loss 0.224956, acc 0.942\n",
      "2017-03-29T23:41:23.339300: step 281, loss 0.151336, acc 0.954\n",
      "2017-03-29T23:41:23.503929: step 282, loss 0.195273, acc 0.938\n",
      "2017-03-29T23:41:23.657543: step 283, loss 0.26865, acc 0.924\n",
      "2017-03-29T23:41:23.828092: step 284, loss 0.158468, acc 0.95\n",
      "2017-03-29T23:41:23.997350: step 285, loss 0.197855, acc 0.932\n",
      "2017-03-29T23:41:24.160308: step 286, loss 0.171018, acc 0.936\n",
      "2017-03-29T23:41:24.300887: step 287, loss 0.180709, acc 0.948\n",
      "2017-03-29T23:41:24.474868: step 288, loss 0.207151, acc 0.932\n",
      "2017-03-29T23:41:24.624635: step 289, loss 0.184551, acc 0.95\n",
      "2017-03-29T23:41:24.783770: step 290, loss 0.196651, acc 0.948\n",
      "2017-03-29T23:41:24.954674: step 291, loss 0.215318, acc 0.932\n",
      "2017-03-29T23:41:25.123366: step 292, loss 0.165144, acc 0.964\n",
      "2017-03-29T23:41:25.274333: step 293, loss 0.19261, acc 0.942\n",
      "2017-03-29T23:41:25.444277: step 294, loss 0.149563, acc 0.954\n",
      "2017-03-29T23:41:25.594462: step 295, loss 0.213724, acc 0.928\n",
      "2017-03-29T23:41:25.762460: step 296, loss 0.15263, acc 0.964\n",
      "2017-03-29T23:41:25.910372: step 297, loss 0.189358, acc 0.948\n",
      "2017-03-29T23:41:26.068622: step 298, loss 0.100026, acc 0.974\n",
      "2017-03-29T23:41:26.215724: step 299, loss 0.228691, acc 0.926\n",
      "2017-03-29T23:41:26.360275: step 300, loss 0.147138, acc 0.96\n",
      "2017-03-29T23:41:26.503631: step 301, loss 0.155846, acc 0.962\n",
      "2017-03-29T23:41:26.681989: step 302, loss 0.1662, acc 0.954\n",
      "2017-03-29T23:41:26.831666: step 303, loss 0.159871, acc 0.954\n",
      "2017-03-29T23:41:26.980779: step 304, loss 0.210453, acc 0.936\n",
      "2017-03-29T23:41:27.146858: step 305, loss 0.168956, acc 0.95\n",
      "2017-03-29T23:41:27.305827: step 306, loss 0.216464, acc 0.944\n",
      "2017-03-29T23:41:27.473215: step 307, loss 0.269604, acc 0.926\n",
      "2017-03-29T23:41:27.634162: step 308, loss 0.136298, acc 0.948\n",
      "2017-03-29T23:41:27.784707: step 309, loss 0.200516, acc 0.948\n",
      "2017-03-29T23:41:27.945179: step 310, loss 0.195128, acc 0.928\n",
      "2017-03-29T23:41:28.123057: step 311, loss 0.213848, acc 0.938\n",
      "2017-03-29T23:41:28.285659: step 312, loss 0.17852, acc 0.944\n",
      "2017-03-29T23:41:28.454463: step 313, loss 0.192733, acc 0.946\n",
      "2017-03-29T23:41:28.619758: step 314, loss 0.191696, acc 0.942\n",
      "2017-03-29T23:41:28.779972: step 315, loss 0.161161, acc 0.95\n",
      "2017-03-29T23:41:28.944213: step 316, loss 0.193196, acc 0.938\n",
      "2017-03-29T23:41:29.087616: step 317, loss 0.131581, acc 0.96\n",
      "2017-03-29T23:41:29.253916: step 318, loss 0.114074, acc 0.972\n",
      "2017-03-29T23:41:29.402498: step 319, loss 0.198986, acc 0.926\n",
      "2017-03-29T23:41:29.556238: step 320, loss 0.134758, acc 0.954\n",
      "2017-03-29T23:41:29.723284: step 321, loss 0.150284, acc 0.96\n",
      "2017-03-29T23:41:29.891062: step 322, loss 0.168364, acc 0.954\n",
      "2017-03-29T23:41:30.046413: step 323, loss 0.138103, acc 0.97\n",
      "2017-03-29T23:41:30.195593: step 324, loss 0.20015, acc 0.948\n",
      "2017-03-29T23:41:30.343681: step 325, loss 0.176956, acc 0.948\n",
      "2017-03-29T23:41:30.486670: step 326, loss 0.166994, acc 0.94\n",
      "2017-03-29T23:41:30.636129: step 327, loss 0.146184, acc 0.954\n",
      "2017-03-29T23:41:30.794017: step 328, loss 0.192296, acc 0.95\n",
      "2017-03-29T23:41:30.957983: step 329, loss 0.174603, acc 0.948\n",
      "2017-03-29T23:41:31.113463: step 330, loss 0.19918, acc 0.942\n",
      "2017-03-29T23:41:31.343404: step 331, loss 0.166474, acc 0.946\n",
      "2017-03-29T23:41:31.489302: step 332, loss 0.175872, acc 0.942\n",
      "2017-03-29T23:41:31.635098: step 333, loss 0.130775, acc 0.962\n",
      "2017-03-29T23:41:31.803823: step 334, loss 0.152149, acc 0.952\n",
      "2017-03-29T23:41:31.953998: step 335, loss 0.180642, acc 0.948\n",
      "2017-03-29T23:41:32.125874: step 336, loss 0.165802, acc 0.944\n",
      "2017-03-29T23:41:32.282968: step 337, loss 0.207982, acc 0.94\n",
      "2017-03-29T23:41:32.432372: step 338, loss 0.144653, acc 0.958\n",
      "2017-03-29T23:41:32.576189: step 339, loss 0.160695, acc 0.962\n",
      "2017-03-29T23:41:32.731520: step 340, loss 0.193407, acc 0.946\n",
      "2017-03-29T23:41:32.894972: step 341, loss 0.206241, acc 0.934\n",
      "2017-03-29T23:41:33.045207: step 342, loss 0.216245, acc 0.922\n",
      "2017-03-29T23:41:33.213781: step 343, loss 0.18101, acc 0.946\n",
      "2017-03-29T23:41:33.382414: step 344, loss 0.135767, acc 0.958\n",
      "2017-03-29T23:41:33.533672: step 345, loss 0.147374, acc 0.956\n",
      "2017-03-29T23:41:33.672221: step 346, loss 0.144958, acc 0.966\n",
      "2017-03-29T23:41:33.823153: step 347, loss 0.159816, acc 0.964\n",
      "2017-03-29T23:41:33.990677: step 348, loss 0.211559, acc 0.938\n",
      "2017-03-29T23:41:34.136826: step 349, loss 0.164279, acc 0.948\n",
      "2017-03-29T23:41:34.281906: step 350, loss 0.212659, acc 0.942\n",
      "2017-03-29T23:41:34.448496: step 351, loss 0.150219, acc 0.968\n",
      "2017-03-29T23:41:34.611089: step 352, loss 0.188185, acc 0.93\n",
      "2017-03-29T23:41:34.775368: step 353, loss 0.166584, acc 0.948\n",
      "2017-03-29T23:41:34.915696: step 354, loss 0.18906, acc 0.948\n",
      "2017-03-29T23:41:35.064190: step 355, loss 0.190351, acc 0.946\n",
      "2017-03-29T23:41:35.207376: step 356, loss 0.185889, acc 0.942\n",
      "2017-03-29T23:41:35.378158: step 357, loss 0.161854, acc 0.946\n",
      "2017-03-29T23:41:35.530876: step 358, loss 0.13119, acc 0.966\n",
      "2017-03-29T23:41:35.683426: step 359, loss 0.148534, acc 0.956\n",
      "2017-03-29T23:41:35.821929: step 360, loss 0.126121, acc 0.96\n",
      "2017-03-29T23:41:35.984472: step 361, loss 0.152789, acc 0.96\n",
      "2017-03-29T23:41:36.151704: step 362, loss 0.167208, acc 0.948\n",
      "2017-03-29T23:41:36.313362: step 363, loss 0.176114, acc 0.96\n",
      "2017-03-29T23:41:36.459794: step 364, loss 0.165118, acc 0.942\n",
      "2017-03-29T23:41:36.607676: step 365, loss 0.160845, acc 0.95\n",
      "2017-03-29T23:41:36.772551: step 366, loss 0.182349, acc 0.942\n",
      "2017-03-29T23:41:36.937612: step 367, loss 0.166151, acc 0.946\n",
      "2017-03-29T23:41:37.089346: step 368, loss 0.122751, acc 0.956\n",
      "2017-03-29T23:41:37.225829: step 369, loss 0.173574, acc 0.962\n",
      "2017-03-29T23:41:37.394589: step 370, loss 0.153285, acc 0.952\n",
      "2017-03-29T23:41:37.544668: step 371, loss 0.171254, acc 0.952\n",
      "2017-03-29T23:41:37.687313: step 372, loss 0.117514, acc 0.968\n",
      "2017-03-29T23:41:37.834597: step 373, loss 0.130941, acc 0.966\n",
      "2017-03-29T23:41:38.004833: step 374, loss 0.146519, acc 0.942\n",
      "2017-03-29T23:41:38.180203: step 375, loss 0.127427, acc 0.97\n",
      "2017-03-29T23:41:38.347424: step 376, loss 0.171956, acc 0.946\n",
      "2017-03-29T23:41:38.508210: step 377, loss 0.147614, acc 0.958\n",
      "2017-03-29T23:41:38.654839: step 378, loss 0.145585, acc 0.956\n",
      "2017-03-29T23:41:38.822373: step 379, loss 0.163039, acc 0.948\n",
      "2017-03-29T23:41:38.975848: step 380, loss 0.179547, acc 0.942\n",
      "2017-03-29T23:41:39.137454: step 381, loss 0.186875, acc 0.942\n",
      "2017-03-29T23:41:39.285655: step 382, loss 0.127477, acc 0.96\n",
      "2017-03-29T23:41:39.454173: step 383, loss 0.175259, acc 0.944\n",
      "2017-03-29T23:41:39.600217: step 384, loss 0.155956, acc 0.95\n",
      "2017-03-29T23:41:39.748344: step 385, loss 0.145518, acc 0.96\n",
      "2017-03-29T23:41:39.917048: step 386, loss 0.132628, acc 0.964\n",
      "2017-03-29T23:41:40.081276: step 387, loss 0.150684, acc 0.95\n",
      "2017-03-29T23:41:40.227548: step 388, loss 0.140942, acc 0.95\n",
      "2017-03-29T23:41:40.382592: step 389, loss 0.139112, acc 0.956\n",
      "2017-03-29T23:41:40.535315: step 390, loss 0.157372, acc 0.952\n",
      "2017-03-29T23:41:40.686961: step 391, loss 0.202994, acc 0.936\n",
      "2017-03-29T23:41:40.855017: step 392, loss 0.153078, acc 0.96\n",
      "2017-03-29T23:41:41.005459: step 393, loss 0.117561, acc 0.96\n",
      "2017-03-29T23:41:41.169583: step 394, loss 0.136787, acc 0.964\n",
      "2017-03-29T23:41:41.349600: step 395, loss 0.203919, acc 0.928\n",
      "2017-03-29T23:41:41.498873: step 396, loss 0.145071, acc 0.958\n",
      "2017-03-29T23:41:41.661798: step 397, loss 0.154442, acc 0.956\n",
      "2017-03-29T23:41:41.831904: step 398, loss 0.129895, acc 0.97\n",
      "2017-03-29T23:41:41.978855: step 399, loss 0.194052, acc 0.938\n",
      "2017-03-29T23:41:42.147389: step 400, loss 0.144011, acc 0.966\n",
      "2017-03-29T23:41:42.316987: step 401, loss 0.1534, acc 0.954\n",
      "2017-03-29T23:41:42.466257: step 402, loss 0.192656, acc 0.948\n",
      "2017-03-29T23:41:42.618538: step 403, loss 0.148019, acc 0.958\n",
      "2017-03-29T23:41:42.767282: step 404, loss 0.143905, acc 0.956\n",
      "2017-03-29T23:41:42.919567: step 405, loss 0.165576, acc 0.956\n",
      "2017-03-29T23:41:43.063539: step 406, loss 0.124067, acc 0.96\n",
      "2017-03-29T23:41:43.236559: step 407, loss 0.168708, acc 0.956\n",
      "2017-03-29T23:41:43.399602: step 408, loss 0.161537, acc 0.954\n",
      "2017-03-29T23:41:43.550678: step 409, loss 0.127387, acc 0.956\n",
      "2017-03-29T23:41:43.720056: step 410, loss 0.0875973, acc 0.976\n",
      "2017-03-29T23:41:43.874637: step 411, loss 0.168787, acc 0.946\n",
      "2017-03-29T23:41:44.035661: step 412, loss 0.195156, acc 0.932\n",
      "2017-03-29T23:41:44.213205: step 413, loss 0.179888, acc 0.938\n",
      "2017-03-29T23:41:44.363722: step 414, loss 0.187429, acc 0.954\n",
      "2017-03-29T23:41:44.505028: step 415, loss 0.149563, acc 0.944\n",
      "2017-03-29T23:41:44.666444: step 416, loss 0.133928, acc 0.968\n",
      "2017-03-29T23:41:44.827843: step 417, loss 0.122818, acc 0.95\n",
      "2017-03-29T23:41:44.962394: step 418, loss 0.175107, acc 0.95\n",
      "2017-03-29T23:41:45.108103: step 419, loss 0.124338, acc 0.972\n",
      "2017-03-29T23:41:45.249395: step 420, loss 0.158724, acc 0.946\n",
      "2017-03-29T23:41:45.417324: step 421, loss 0.146805, acc 0.96\n",
      "2017-03-29T23:41:45.555130: step 422, loss 0.161824, acc 0.966\n",
      "2017-03-29T23:41:45.721443: step 423, loss 0.0916236, acc 0.972\n",
      "2017-03-29T23:41:45.868310: step 424, loss 0.169488, acc 0.946\n",
      "2017-03-29T23:41:46.024570: step 425, loss 0.143563, acc 0.958\n",
      "2017-03-29T23:41:46.174853: step 426, loss 0.165835, acc 0.948\n",
      "2017-03-29T23:41:46.330466: step 427, loss 0.150574, acc 0.95\n",
      "2017-03-29T23:41:46.477817: step 428, loss 0.126265, acc 0.958\n",
      "2017-03-29T23:41:46.646461: step 429, loss 0.146398, acc 0.958\n",
      "2017-03-29T23:41:46.795825: step 430, loss 0.158197, acc 0.95\n",
      "2017-03-29T23:41:46.941194: step 431, loss 0.150159, acc 0.968\n",
      "2017-03-29T23:41:47.094613: step 432, loss 0.104303, acc 0.97\n",
      "2017-03-29T23:41:47.261755: step 433, loss 0.188846, acc 0.96\n",
      "2017-03-29T23:41:47.425051: step 434, loss 0.14537, acc 0.95\n",
      "2017-03-29T23:41:47.586725: step 435, loss 0.134874, acc 0.964\n",
      "2017-03-29T23:41:47.730886: step 436, loss 0.151139, acc 0.958\n",
      "2017-03-29T23:41:47.896112: step 437, loss 0.16024, acc 0.956\n",
      "2017-03-29T23:41:48.040134: step 438, loss 0.130419, acc 0.96\n",
      "2017-03-29T23:41:48.208631: step 439, loss 0.109341, acc 0.966\n",
      "2017-03-29T23:41:48.371298: step 440, loss 0.140789, acc 0.96\n",
      "2017-03-29T23:41:48.618541: step 441, loss 0.108637, acc 0.958\n",
      "2017-03-29T23:41:48.778693: step 442, loss 0.128564, acc 0.964\n",
      "2017-03-29T23:41:48.929274: step 443, loss 0.143467, acc 0.952\n",
      "2017-03-29T23:41:49.096417: step 444, loss 0.169633, acc 0.952\n",
      "2017-03-29T23:41:49.254633: step 445, loss 0.129985, acc 0.964\n",
      "2017-03-29T23:41:49.406297: step 446, loss 0.127385, acc 0.954\n",
      "2017-03-29T23:41:49.573513: step 447, loss 0.132351, acc 0.962\n",
      "2017-03-29T23:41:49.724700: step 448, loss 0.164708, acc 0.942\n",
      "2017-03-29T23:41:49.868789: step 449, loss 0.147674, acc 0.942\n",
      "2017-03-29T23:41:50.019246: step 450, loss 0.159903, acc 0.946\n",
      "2017-03-29T23:41:50.155855: step 451, loss 0.128036, acc 0.956\n",
      "2017-03-29T23:41:50.292243: step 452, loss 0.155753, acc 0.962\n",
      "2017-03-29T23:41:50.436004: step 453, loss 0.141079, acc 0.96\n",
      "2017-03-29T23:41:50.602233: step 454, loss 0.133046, acc 0.96\n",
      "2017-03-29T23:41:50.775127: step 455, loss 0.141825, acc 0.956\n",
      "2017-03-29T23:41:50.942104: step 456, loss 0.13109, acc 0.964\n",
      "2017-03-29T23:41:51.079197: step 457, loss 0.133907, acc 0.96\n",
      "2017-03-29T23:41:51.243090: step 458, loss 0.126426, acc 0.964\n",
      "2017-03-29T23:41:51.411580: step 459, loss 0.183912, acc 0.944\n",
      "2017-03-29T23:41:51.582897: step 460, loss 0.161103, acc 0.944\n",
      "2017-03-29T23:41:51.748742: step 461, loss 0.168583, acc 0.956\n",
      "2017-03-29T23:41:51.898477: step 462, loss 0.161566, acc 0.952\n",
      "2017-03-29T23:41:52.063550: step 463, loss 0.184037, acc 0.942\n",
      "2017-03-29T23:41:52.211621: step 464, loss 0.148177, acc 0.96\n",
      "2017-03-29T23:41:52.375692: step 465, loss 0.175435, acc 0.95\n",
      "2017-03-29T23:41:52.519664: step 466, loss 0.133856, acc 0.964\n",
      "2017-03-29T23:41:52.657787: step 467, loss 0.131283, acc 0.958\n",
      "2017-03-29T23:41:52.811416: step 468, loss 0.155279, acc 0.954\n",
      "2017-03-29T23:41:52.962021: step 469, loss 0.110044, acc 0.968\n",
      "2017-03-29T23:41:53.109723: step 470, loss 0.116381, acc 0.96\n",
      "2017-03-29T23:41:53.276210: step 471, loss 0.145892, acc 0.962\n",
      "2017-03-29T23:41:53.435480: step 472, loss 0.138794, acc 0.966\n",
      "2017-03-29T23:41:53.591572: step 473, loss 0.14616, acc 0.96\n",
      "2017-03-29T23:41:53.742165: step 474, loss 0.147677, acc 0.956\n",
      "2017-03-29T23:41:53.907164: step 475, loss 0.105378, acc 0.982\n",
      "2017-03-29T23:41:54.076452: step 476, loss 0.0986799, acc 0.96\n",
      "2017-03-29T23:41:54.234171: step 477, loss 0.112465, acc 0.98\n",
      "2017-03-29T23:41:54.391834: step 478, loss 0.104477, acc 0.968\n",
      "2017-03-29T23:41:54.541799: step 479, loss 0.145509, acc 0.964\n",
      "2017-03-29T23:41:54.708527: step 480, loss 0.136242, acc 0.952\n",
      "2017-03-29T23:41:54.859238: step 481, loss 0.146045, acc 0.968\n",
      "2017-03-29T23:41:55.004443: step 482, loss 0.112455, acc 0.964\n",
      "2017-03-29T23:41:55.161404: step 483, loss 0.121388, acc 0.964\n",
      "2017-03-29T23:41:55.321740: step 484, loss 0.0993679, acc 0.978\n",
      "2017-03-29T23:41:55.490865: step 485, loss 0.113375, acc 0.97\n",
      "2017-03-29T23:41:55.657585: step 486, loss 0.0891669, acc 0.974\n",
      "2017-03-29T23:41:55.804848: step 487, loss 0.152672, acc 0.956\n",
      "2017-03-29T23:41:55.958212: step 488, loss 0.15245, acc 0.952\n",
      "2017-03-29T23:41:56.103799: step 489, loss 0.143352, acc 0.968\n",
      "2017-03-29T23:41:56.242809: step 490, loss 0.121169, acc 0.964\n",
      "2017-03-29T23:41:56.397394: step 491, loss 0.110525, acc 0.966\n",
      "2017-03-29T23:41:56.537867: step 492, loss 0.146676, acc 0.952\n",
      "2017-03-29T23:41:56.688059: step 493, loss 0.154175, acc 0.958\n",
      "2017-03-29T23:41:56.838211: step 494, loss 0.0815528, acc 0.982\n",
      "2017-03-29T23:41:56.988545: step 495, loss 0.129236, acc 0.966\n",
      "2017-03-29T23:41:57.130870: step 496, loss 0.149658, acc 0.952\n",
      "2017-03-29T23:41:57.279189: step 497, loss 0.136784, acc 0.96\n",
      "2017-03-29T23:41:57.439383: step 498, loss 0.138631, acc 0.958\n",
      "2017-03-29T23:41:57.588186: step 499, loss 0.128634, acc 0.96\n",
      "2017-03-29T23:41:57.751679: step 500, loss 0.158093, acc 0.956\n",
      "2017-03-29T23:41:57.901532: step 501, loss 0.139693, acc 0.956\n",
      "2017-03-29T23:41:58.052437: step 502, loss 0.138048, acc 0.95\n",
      "2017-03-29T23:41:58.194714: step 503, loss 0.118063, acc 0.974\n",
      "2017-03-29T23:41:58.348718: step 504, loss 0.100604, acc 0.966\n",
      "2017-03-29T23:41:58.495166: step 505, loss 0.0918925, acc 0.972\n",
      "2017-03-29T23:41:58.649308: step 506, loss 0.117916, acc 0.968\n",
      "2017-03-29T23:41:58.821273: step 507, loss 0.134933, acc 0.96\n",
      "2017-03-29T23:41:58.992107: step 508, loss 0.14014, acc 0.956\n",
      "2017-03-29T23:41:59.159007: step 509, loss 0.123023, acc 0.97\n",
      "2017-03-29T23:41:59.306393: step 510, loss 0.15113, acc 0.954\n",
      "2017-03-29T23:41:59.467905: step 511, loss 0.130228, acc 0.968\n",
      "2017-03-29T23:41:59.634184: step 512, loss 0.117258, acc 0.972\n",
      "2017-03-29T23:41:59.779814: step 513, loss 0.107264, acc 0.974\n",
      "2017-03-29T23:41:59.931415: step 514, loss 0.108688, acc 0.962\n",
      "2017-03-29T23:42:00.082141: step 515, loss 0.137934, acc 0.956\n",
      "2017-03-29T23:42:00.245723: step 516, loss 0.0903109, acc 0.972\n",
      "2017-03-29T23:42:00.400168: step 517, loss 0.142166, acc 0.952\n",
      "2017-03-29T23:42:00.560879: step 518, loss 0.118219, acc 0.966\n",
      "2017-03-29T23:42:00.708201: step 519, loss 0.142519, acc 0.954\n",
      "2017-03-29T23:42:00.869455: step 520, loss 0.102885, acc 0.97\n",
      "2017-03-29T23:42:01.019934: step 521, loss 0.136147, acc 0.952\n",
      "2017-03-29T23:42:01.174518: step 522, loss 0.119273, acc 0.96\n",
      "2017-03-29T23:42:01.341056: step 523, loss 0.0913327, acc 0.97\n",
      "2017-03-29T23:42:01.484208: step 524, loss 0.117629, acc 0.97\n",
      "2017-03-29T23:42:01.627798: step 525, loss 0.110957, acc 0.966\n",
      "2017-03-29T23:42:01.762739: step 526, loss 0.127525, acc 0.952\n",
      "2017-03-29T23:42:01.923859: step 527, loss 0.114658, acc 0.966\n",
      "2017-03-29T23:42:02.073995: step 528, loss 0.118803, acc 0.966\n",
      "2017-03-29T23:42:02.241606: step 529, loss 0.120008, acc 0.978\n",
      "2017-03-29T23:42:02.395222: step 530, loss 0.191042, acc 0.948\n",
      "2017-03-29T23:42:02.570376: step 531, loss 0.186457, acc 0.95\n",
      "2017-03-29T23:42:02.733254: step 532, loss 0.17842, acc 0.948\n",
      "2017-03-29T23:42:02.897611: step 533, loss 0.127243, acc 0.964\n",
      "2017-03-29T23:42:03.056895: step 534, loss 0.0826989, acc 0.978\n",
      "2017-03-29T23:42:03.216229: step 535, loss 0.186989, acc 0.944\n",
      "2017-03-29T23:42:03.364574: step 536, loss 0.13504, acc 0.966\n",
      "2017-03-29T23:42:03.530524: step 537, loss 0.102261, acc 0.968\n",
      "2017-03-29T23:42:03.667627: step 538, loss 0.117061, acc 0.966\n",
      "2017-03-29T23:42:03.811094: step 539, loss 0.143232, acc 0.962\n",
      "2017-03-29T23:42:03.972321: step 540, loss 0.163156, acc 0.942\n",
      "2017-03-29T23:42:04.118643: step 541, loss 0.0892393, acc 0.976\n",
      "2017-03-29T23:42:04.295760: step 542, loss 0.134086, acc 0.956\n",
      "2017-03-29T23:42:04.444595: step 543, loss 0.110094, acc 0.964\n",
      "2017-03-29T23:42:04.587900: step 544, loss 0.0945228, acc 0.972\n",
      "2017-03-29T23:42:04.733813: step 545, loss 0.123664, acc 0.964\n",
      "2017-03-29T23:42:04.883431: step 546, loss 0.113986, acc 0.972\n",
      "2017-03-29T23:42:05.043214: step 547, loss 0.109104, acc 0.966\n",
      "2017-03-29T23:42:05.183075: step 548, loss 0.121948, acc 0.966\n",
      "2017-03-29T23:42:05.326747: step 549, loss 0.118386, acc 0.96\n",
      "2017-03-29T23:42:05.493366: step 550, loss 0.11884, acc 0.958\n",
      "2017-03-29T23:42:05.722318: step 551, loss 0.100623, acc 0.972\n",
      "2017-03-29T23:42:05.881721: step 552, loss 0.110211, acc 0.968\n",
      "2017-03-29T23:42:06.045974: step 553, loss 0.101245, acc 0.974\n",
      "2017-03-29T23:42:06.193553: step 554, loss 0.0941898, acc 0.978\n",
      "2017-03-29T23:42:06.332607: step 555, loss 0.122001, acc 0.96\n",
      "2017-03-29T23:42:06.496238: step 556, loss 0.0844163, acc 0.972\n",
      "2017-03-29T23:42:06.644837: step 557, loss 0.112368, acc 0.96\n",
      "2017-03-29T23:42:06.801730: step 558, loss 0.143014, acc 0.956\n",
      "2017-03-29T23:42:06.959022: step 559, loss 0.10522, acc 0.952\n",
      "2017-03-29T23:42:07.128281: step 560, loss 0.112981, acc 0.968\n",
      "2017-03-29T23:42:07.300472: step 561, loss 0.119391, acc 0.968\n",
      "2017-03-29T23:42:07.463534: step 562, loss 0.128702, acc 0.966\n",
      "2017-03-29T23:42:07.641510: step 563, loss 0.0756321, acc 0.98\n",
      "2017-03-29T23:42:07.814096: step 564, loss 0.161048, acc 0.958\n",
      "2017-03-29T23:42:07.975689: step 565, loss 0.141715, acc 0.956\n",
      "2017-03-29T23:42:08.124849: step 566, loss 0.109576, acc 0.966\n",
      "2017-03-29T23:42:08.263795: step 567, loss 0.118572, acc 0.964\n",
      "2017-03-29T23:42:08.426124: step 568, loss 0.130711, acc 0.96\n",
      "2017-03-29T23:42:08.574174: step 569, loss 0.110112, acc 0.968\n",
      "2017-03-29T23:42:08.724297: step 570, loss 0.111892, acc 0.974\n",
      "2017-03-29T23:42:08.886346: step 571, loss 0.136212, acc 0.966\n",
      "2017-03-29T23:42:09.061009: step 572, loss 0.143943, acc 0.956\n",
      "2017-03-29T23:42:09.237167: step 573, loss 0.104465, acc 0.97\n",
      "2017-03-29T23:42:09.382534: step 574, loss 0.108051, acc 0.972\n",
      "2017-03-29T23:42:09.551468: step 575, loss 0.108291, acc 0.958\n",
      "2017-03-29T23:42:09.701182: step 576, loss 0.117636, acc 0.96\n",
      "2017-03-29T23:42:09.843188: step 577, loss 0.117903, acc 0.954\n",
      "2017-03-29T23:42:10.004514: step 578, loss 0.132198, acc 0.97\n",
      "2017-03-29T23:42:10.164997: step 579, loss 0.125106, acc 0.968\n",
      "2017-03-29T23:42:10.327987: step 580, loss 0.12912, acc 0.96\n",
      "2017-03-29T23:42:10.465457: step 581, loss 0.117344, acc 0.96\n",
      "2017-03-29T23:42:10.633776: step 582, loss 0.120697, acc 0.954\n",
      "2017-03-29T23:42:10.780768: step 583, loss 0.148987, acc 0.968\n",
      "2017-03-29T23:42:10.926084: step 584, loss 0.104736, acc 0.97\n",
      "2017-03-29T23:42:11.077404: step 585, loss 0.110438, acc 0.968\n",
      "2017-03-29T23:42:11.237307: step 586, loss 0.166882, acc 0.952\n",
      "2017-03-29T23:42:11.392599: step 587, loss 0.108568, acc 0.97\n",
      "2017-03-29T23:42:11.543940: step 588, loss 0.11559, acc 0.976\n",
      "2017-03-29T23:42:11.684564: step 589, loss 0.0828012, acc 0.978\n",
      "2017-03-29T23:42:11.849444: step 590, loss 0.0858593, acc 0.972\n",
      "2017-03-29T23:42:11.996525: step 591, loss 0.130412, acc 0.96\n",
      "2017-03-29T23:42:12.160353: step 592, loss 0.113059, acc 0.968\n",
      "2017-03-29T23:42:12.319184: step 593, loss 0.111193, acc 0.966\n",
      "2017-03-29T23:42:12.468548: step 594, loss 0.0985755, acc 0.968\n",
      "2017-03-29T23:42:12.614641: step 595, loss 0.123826, acc 0.964\n",
      "2017-03-29T23:42:12.775848: step 596, loss 0.118928, acc 0.958\n",
      "2017-03-29T23:42:12.945627: step 597, loss 0.128858, acc 0.964\n",
      "2017-03-29T23:42:13.105511: step 598, loss 0.0702118, acc 0.984\n",
      "2017-03-29T23:42:13.242994: step 599, loss 0.099609, acc 0.972\n",
      "2017-03-29T23:42:13.408933: step 600, loss 0.100573, acc 0.974\n",
      "2017-03-29T23:42:13.548540: step 601, loss 0.131783, acc 0.958\n",
      "2017-03-29T23:42:13.690374: step 602, loss 0.100689, acc 0.974\n",
      "2017-03-29T23:42:13.846443: step 603, loss 0.12582, acc 0.97\n",
      "2017-03-29T23:42:14.001552: step 604, loss 0.136131, acc 0.966\n",
      "2017-03-29T23:42:14.173633: step 605, loss 0.0740062, acc 0.984\n",
      "2017-03-29T23:42:14.316194: step 606, loss 0.147963, acc 0.964\n",
      "2017-03-29T23:42:14.467715: step 607, loss 0.155907, acc 0.96\n",
      "2017-03-29T23:42:14.620165: step 608, loss 0.102923, acc 0.964\n",
      "2017-03-29T23:42:14.768407: step 609, loss 0.161303, acc 0.95\n",
      "2017-03-29T23:42:14.917224: step 610, loss 0.0911412, acc 0.966\n",
      "2017-03-29T23:42:15.064459: step 611, loss 0.115296, acc 0.966\n",
      "2017-03-29T23:42:15.226400: step 612, loss 0.120452, acc 0.962\n",
      "2017-03-29T23:42:15.374172: step 613, loss 0.101802, acc 0.966\n",
      "2017-03-29T23:42:15.539539: step 614, loss 0.119249, acc 0.968\n",
      "2017-03-29T23:42:15.686621: step 615, loss 0.107499, acc 0.952\n",
      "2017-03-29T23:42:15.835256: step 616, loss 0.106067, acc 0.958\n",
      "2017-03-29T23:42:15.982878: step 617, loss 0.137756, acc 0.954\n",
      "2017-03-29T23:42:16.152733: step 618, loss 0.10393, acc 0.976\n",
      "2017-03-29T23:42:16.321252: step 619, loss 0.0987181, acc 0.972\n",
      "2017-03-29T23:42:16.482966: step 620, loss 0.108905, acc 0.96\n",
      "2017-03-29T23:42:16.658538: step 621, loss 0.102622, acc 0.972\n",
      "2017-03-29T23:42:16.809456: step 622, loss 0.116678, acc 0.972\n",
      "2017-03-29T23:42:16.961745: step 623, loss 0.129326, acc 0.96\n",
      "2017-03-29T23:42:17.122893: step 624, loss 0.10384, acc 0.964\n",
      "2017-03-29T23:42:17.300803: step 625, loss 0.0907049, acc 0.972\n",
      "2017-03-29T23:42:17.452841: step 626, loss 0.068272, acc 0.974\n",
      "2017-03-29T23:42:17.623530: step 627, loss 0.130788, acc 0.96\n",
      "2017-03-29T23:42:17.788084: step 628, loss 0.141655, acc 0.95\n",
      "2017-03-29T23:42:17.941157: step 629, loss 0.0940641, acc 0.972\n",
      "2017-03-29T23:42:18.108065: step 630, loss 0.123514, acc 0.96\n",
      "2017-03-29T23:42:18.259401: step 631, loss 0.145131, acc 0.958\n",
      "2017-03-29T23:42:18.423034: step 632, loss 0.108775, acc 0.964\n",
      "2017-03-29T23:42:18.576511: step 633, loss 0.113129, acc 0.97\n",
      "2017-03-29T23:42:18.738136: step 634, loss 0.163726, acc 0.952\n",
      "2017-03-29T23:42:18.877564: step 635, loss 0.0902122, acc 0.972\n",
      "2017-03-29T23:42:19.024152: step 636, loss 0.121938, acc 0.968\n",
      "2017-03-29T23:42:19.166918: step 637, loss 0.16999, acc 0.94\n",
      "2017-03-29T23:42:19.328372: step 638, loss 0.148236, acc 0.95\n",
      "2017-03-29T23:42:19.491049: step 639, loss 0.125283, acc 0.962\n",
      "2017-03-29T23:42:19.648768: step 640, loss 0.115901, acc 0.968\n",
      "2017-03-29T23:42:19.820842: step 641, loss 0.111702, acc 0.974\n",
      "2017-03-29T23:42:20.003479: step 642, loss 0.169826, acc 0.948\n",
      "2017-03-29T23:42:20.168436: step 643, loss 0.112769, acc 0.964\n",
      "2017-03-29T23:42:20.322693: step 644, loss 0.126221, acc 0.962\n",
      "2017-03-29T23:42:20.497668: step 645, loss 0.117112, acc 0.972\n",
      "2017-03-29T23:42:20.685761: step 646, loss 0.15862, acc 0.96\n",
      "2017-03-29T23:42:20.853827: step 647, loss 0.117891, acc 0.964\n",
      "2017-03-29T23:42:21.003789: step 648, loss 0.11168, acc 0.97\n",
      "2017-03-29T23:42:21.165441: step 649, loss 0.0890054, acc 0.974\n",
      "2017-03-29T23:42:21.332677: step 650, loss 0.11548, acc 0.974\n",
      "2017-03-29T23:42:21.483896: step 651, loss 0.0831459, acc 0.978\n",
      "2017-03-29T23:42:21.627648: step 652, loss 0.115251, acc 0.97\n",
      "2017-03-29T23:42:21.790760: step 653, loss 0.115687, acc 0.97\n",
      "2017-03-29T23:42:21.957678: step 654, loss 0.137223, acc 0.968\n",
      "2017-03-29T23:42:22.126185: step 655, loss 0.0930874, acc 0.974\n",
      "2017-03-29T23:42:22.275955: step 656, loss 0.0870175, acc 0.968\n",
      "2017-03-29T23:42:22.429399: step 657, loss 0.122057, acc 0.96\n",
      "2017-03-29T23:42:22.582697: step 658, loss 0.12978, acc 0.964\n",
      "2017-03-29T23:42:22.747491: step 659, loss 0.0952144, acc 0.972\n",
      "2017-03-29T23:42:22.917536: step 660, loss 0.0820623, acc 0.978\n",
      "2017-03-29T23:42:23.149614: step 661, loss 0.104964, acc 0.966\n",
      "2017-03-29T23:42:23.301533: step 662, loss 0.11876, acc 0.962\n",
      "2017-03-29T23:42:23.466312: step 663, loss 0.103133, acc 0.964\n",
      "2017-03-29T23:42:23.622591: step 664, loss 0.106662, acc 0.962\n",
      "2017-03-29T23:42:23.771273: step 665, loss 0.103394, acc 0.974\n",
      "2017-03-29T23:42:23.916066: step 666, loss 0.117951, acc 0.962\n",
      "2017-03-29T23:42:24.082246: step 667, loss 0.134709, acc 0.96\n",
      "2017-03-29T23:42:24.253092: step 668, loss 0.0938423, acc 0.98\n",
      "2017-03-29T23:42:24.425079: step 669, loss 0.0801258, acc 0.974\n",
      "2017-03-29T23:42:24.574310: step 670, loss 0.134288, acc 0.962\n",
      "2017-03-29T23:42:24.722432: step 671, loss 0.120258, acc 0.964\n",
      "2017-03-29T23:42:24.870339: step 672, loss 0.0882128, acc 0.976\n",
      "2017-03-29T23:42:25.037778: step 673, loss 0.151708, acc 0.95\n",
      "2017-03-29T23:42:25.199302: step 674, loss 0.116508, acc 0.966\n",
      "2017-03-29T23:42:25.367948: step 675, loss 0.119264, acc 0.96\n",
      "2017-03-29T23:42:25.537629: step 676, loss 0.102691, acc 0.966\n",
      "2017-03-29T23:42:25.682121: step 677, loss 0.080476, acc 0.982\n",
      "2017-03-29T23:42:25.835735: step 678, loss 0.109838, acc 0.962\n",
      "2017-03-29T23:42:25.983637: step 679, loss 0.0863992, acc 0.972\n",
      "2017-03-29T23:42:26.146856: step 680, loss 0.106989, acc 0.964\n",
      "2017-03-29T23:42:26.316237: step 681, loss 0.124342, acc 0.96\n",
      "2017-03-29T23:42:26.481873: step 682, loss 0.0947998, acc 0.982\n",
      "2017-03-29T23:42:26.623110: step 683, loss 0.0955102, acc 0.97\n",
      "2017-03-29T23:42:26.790949: step 684, loss 0.131352, acc 0.972\n",
      "2017-03-29T23:42:26.957795: step 685, loss 0.131445, acc 0.954\n",
      "2017-03-29T23:42:27.113029: step 686, loss 0.11037, acc 0.968\n",
      "2017-03-29T23:42:27.268891: step 687, loss 0.101394, acc 0.968\n",
      "2017-03-29T23:42:27.406933: step 688, loss 0.104766, acc 0.968\n",
      "2017-03-29T23:42:27.570964: step 689, loss 0.107387, acc 0.972\n",
      "2017-03-29T23:42:27.732486: step 690, loss 0.12142, acc 0.97\n",
      "2017-03-29T23:42:27.880963: step 691, loss 0.144016, acc 0.958\n",
      "2017-03-29T23:42:28.047972: step 692, loss 0.104571, acc 0.97\n",
      "2017-03-29T23:42:28.196994: step 693, loss 0.0793588, acc 0.974\n",
      "2017-03-29T23:42:28.382204: step 694, loss 0.101496, acc 0.97\n",
      "2017-03-29T23:42:28.532826: step 695, loss 0.0780842, acc 0.982\n",
      "2017-03-29T23:42:28.676544: step 696, loss 0.111724, acc 0.962\n",
      "2017-03-29T23:42:28.830581: step 697, loss 0.0656125, acc 0.982\n",
      "2017-03-29T23:42:28.992985: step 698, loss 0.113847, acc 0.966\n",
      "2017-03-29T23:42:29.158953: step 699, loss 0.098489, acc 0.968\n",
      "2017-03-29T23:42:29.323231: step 700, loss 0.0638569, acc 0.98\n",
      "2017-03-29T23:42:29.471577: step 701, loss 0.107165, acc 0.964\n",
      "2017-03-29T23:42:29.616610: step 702, loss 0.115014, acc 0.966\n",
      "2017-03-29T23:42:29.784047: step 703, loss 0.100574, acc 0.974\n",
      "2017-03-29T23:42:29.952727: step 704, loss 0.122847, acc 0.962\n",
      "2017-03-29T23:42:30.096793: step 705, loss 0.125605, acc 0.96\n",
      "2017-03-29T23:42:30.256797: step 706, loss 0.0987007, acc 0.974\n",
      "2017-03-29T23:42:30.409292: step 707, loss 0.115332, acc 0.96\n",
      "2017-03-29T23:42:30.554626: step 708, loss 0.127129, acc 0.964\n",
      "2017-03-29T23:42:30.704294: step 709, loss 0.0985399, acc 0.974\n",
      "2017-03-29T23:42:30.853212: step 710, loss 0.110251, acc 0.97\n",
      "2017-03-29T23:42:31.017657: step 711, loss 0.104993, acc 0.966\n",
      "2017-03-29T23:42:31.162905: step 712, loss 0.102082, acc 0.976\n",
      "2017-03-29T23:42:31.319778: step 713, loss 0.0857194, acc 0.976\n",
      "2017-03-29T23:42:31.486321: step 714, loss 0.078261, acc 0.982\n",
      "2017-03-29T23:42:31.634657: step 715, loss 0.125308, acc 0.97\n",
      "2017-03-29T23:42:31.786185: step 716, loss 0.0958346, acc 0.968\n",
      "2017-03-29T23:42:31.960918: step 717, loss 0.137762, acc 0.958\n",
      "2017-03-29T23:42:32.130169: step 718, loss 0.124821, acc 0.966\n",
      "2017-03-29T23:42:32.286704: step 719, loss 0.11688, acc 0.962\n",
      "2017-03-29T23:42:32.450050: step 720, loss 0.0885232, acc 0.978\n",
      "2017-03-29T23:42:32.609211: step 721, loss 0.0814078, acc 0.968\n",
      "2017-03-29T23:42:32.752889: step 722, loss 0.109943, acc 0.966\n",
      "2017-03-29T23:42:32.896706: step 723, loss 0.0588057, acc 0.978\n",
      "2017-03-29T23:42:33.059130: step 724, loss 0.111532, acc 0.962\n",
      "2017-03-29T23:42:33.208821: step 725, loss 0.0642402, acc 0.982\n",
      "2017-03-29T23:42:33.348076: step 726, loss 0.116599, acc 0.968\n",
      "2017-03-29T23:42:33.501865: step 727, loss 0.177613, acc 0.95\n",
      "2017-03-29T23:42:33.675725: step 728, loss 0.127362, acc 0.968\n",
      "2017-03-29T23:42:33.845225: step 729, loss 0.0802764, acc 0.974\n",
      "2017-03-29T23:42:34.002393: step 730, loss 0.102377, acc 0.976\n",
      "2017-03-29T23:42:34.162121: step 731, loss 0.0717571, acc 0.976\n",
      "2017-03-29T23:42:34.327620: step 732, loss 0.113308, acc 0.962\n",
      "2017-03-29T23:42:34.488208: step 733, loss 0.0865491, acc 0.972\n",
      "2017-03-29T23:42:34.653081: step 734, loss 0.0767767, acc 0.98\n",
      "2017-03-29T23:42:34.802067: step 735, loss 0.0786607, acc 0.97\n",
      "2017-03-29T23:42:34.976590: step 736, loss 0.0932167, acc 0.97\n",
      "2017-03-29T23:42:35.143266: step 737, loss 0.0704148, acc 0.984\n",
      "2017-03-29T23:42:35.282763: step 738, loss 0.095959, acc 0.966\n",
      "2017-03-29T23:42:35.432394: step 739, loss 0.104021, acc 0.968\n",
      "2017-03-29T23:42:35.597754: step 740, loss 0.116501, acc 0.956\n",
      "2017-03-29T23:42:35.744233: step 741, loss 0.100816, acc 0.968\n",
      "2017-03-29T23:42:35.888443: step 742, loss 0.0883319, acc 0.972\n",
      "2017-03-29T23:42:36.036351: step 743, loss 0.0782666, acc 0.978\n",
      "2017-03-29T23:42:36.169769: step 744, loss 0.0972188, acc 0.97\n",
      "2017-03-29T23:42:36.335405: step 745, loss 0.130055, acc 0.96\n",
      "2017-03-29T23:42:36.491680: step 746, loss 0.122952, acc 0.964\n",
      "2017-03-29T23:42:36.652354: step 747, loss 0.116544, acc 0.962\n",
      "2017-03-29T23:42:36.818561: step 748, loss 0.120713, acc 0.962\n",
      "2017-03-29T23:42:36.967274: step 749, loss 0.0991212, acc 0.972\n",
      "2017-03-29T23:42:37.132742: step 750, loss 0.0817448, acc 0.98\n",
      "2017-03-29T23:42:37.288302: step 751, loss 0.135498, acc 0.97\n",
      "2017-03-29T23:42:37.453336: step 752, loss 0.143266, acc 0.956\n",
      "2017-03-29T23:42:37.606846: step 753, loss 0.117707, acc 0.956\n",
      "2017-03-29T23:42:37.778057: step 754, loss 0.0998406, acc 0.97\n",
      "2017-03-29T23:42:37.926289: step 755, loss 0.122, acc 0.974\n",
      "2017-03-29T23:42:38.076888: step 756, loss 0.0719166, acc 0.98\n",
      "2017-03-29T23:42:38.225601: step 757, loss 0.0894548, acc 0.974\n",
      "2017-03-29T23:42:38.383696: step 758, loss 0.0972492, acc 0.97\n",
      "2017-03-29T23:42:38.549665: step 759, loss 0.0613858, acc 0.98\n",
      "2017-03-29T23:42:38.716816: step 760, loss 0.113939, acc 0.968\n",
      "2017-03-29T23:42:38.885953: step 761, loss 0.0931773, acc 0.968\n",
      "2017-03-29T23:42:39.034983: step 762, loss 0.109949, acc 0.968\n",
      "2017-03-29T23:42:39.186580: step 763, loss 0.104758, acc 0.968\n",
      "2017-03-29T23:42:39.335600: step 764, loss 0.101077, acc 0.976\n",
      "2017-03-29T23:42:39.495919: step 765, loss 0.0955144, acc 0.974\n",
      "2017-03-29T23:42:39.634313: step 766, loss 0.10134, acc 0.968\n",
      "2017-03-29T23:42:39.783564: step 767, loss 0.110943, acc 0.966\n",
      "2017-03-29T23:42:39.932276: step 768, loss 0.0949041, acc 0.974\n",
      "2017-03-29T23:42:40.076126: step 769, loss 0.0757803, acc 0.974\n",
      "2017-03-29T23:42:40.240994: step 770, loss 0.0755307, acc 0.976\n",
      "2017-03-29T23:42:40.481681: step 771, loss 0.11938, acc 0.956\n",
      "2017-03-29T23:42:40.619749: step 772, loss 0.0783451, acc 0.972\n",
      "2017-03-29T23:42:40.779613: step 773, loss 0.106673, acc 0.972\n",
      "2017-03-29T23:42:40.931684: step 774, loss 0.0895394, acc 0.976\n",
      "2017-03-29T23:42:41.098640: step 775, loss 0.110392, acc 0.97\n",
      "2017-03-29T23:42:41.243916: step 776, loss 0.120262, acc 0.964\n",
      "2017-03-29T23:42:41.380997: step 777, loss 0.0932254, acc 0.964\n",
      "2017-03-29T23:42:41.536274: step 778, loss 0.124328, acc 0.968\n",
      "2017-03-29T23:42:41.677894: step 779, loss 0.125811, acc 0.968\n",
      "2017-03-29T23:42:41.814522: step 780, loss 0.125282, acc 0.968\n",
      "2017-03-29T23:42:41.970833: step 781, loss 0.102564, acc 0.958\n",
      "2017-03-29T23:42:42.136583: step 782, loss 0.0765259, acc 0.97\n",
      "2017-03-29T23:42:42.298747: step 783, loss 0.09315, acc 0.972\n",
      "2017-03-29T23:42:42.447819: step 784, loss 0.0861018, acc 0.972\n",
      "2017-03-29T23:42:42.598670: step 785, loss 0.0640517, acc 0.984\n",
      "2017-03-29T23:42:42.759485: step 786, loss 0.0822134, acc 0.976\n",
      "2017-03-29T23:42:42.933022: step 787, loss 0.084756, acc 0.978\n",
      "2017-03-29T23:42:43.115411: step 788, loss 0.125672, acc 0.974\n",
      "2017-03-29T23:42:43.259131: step 789, loss 0.117505, acc 0.974\n",
      "2017-03-29T23:42:43.407564: step 790, loss 0.0717091, acc 0.984\n",
      "2017-03-29T23:42:43.558572: step 791, loss 0.113242, acc 0.966\n",
      "2017-03-29T23:42:43.709118: step 792, loss 0.116256, acc 0.98\n",
      "2017-03-29T23:42:43.870017: step 793, loss 0.0995622, acc 0.97\n",
      "2017-03-29T23:42:44.041018: step 794, loss 0.0795342, acc 0.976\n",
      "2017-03-29T23:42:44.205111: step 795, loss 0.119268, acc 0.96\n",
      "2017-03-29T23:42:44.368144: step 796, loss 0.0805675, acc 0.966\n",
      "2017-03-29T23:42:44.515636: step 797, loss 0.0975334, acc 0.972\n",
      "2017-03-29T23:42:44.668521: step 798, loss 0.101664, acc 0.968\n",
      "2017-03-29T23:42:44.813800: step 799, loss 0.093154, acc 0.966\n",
      "2017-03-29T23:42:44.983312: step 800, loss 0.0733239, acc 0.97\n",
      "2017-03-29T23:42:45.151891: step 801, loss 0.0875433, acc 0.976\n",
      "2017-03-29T23:42:45.325331: step 802, loss 0.129633, acc 0.962\n",
      "2017-03-29T23:42:45.489140: step 803, loss 0.111277, acc 0.962\n",
      "2017-03-29T23:42:45.665355: step 804, loss 0.0835365, acc 0.974\n",
      "2017-03-29T23:42:45.815362: step 805, loss 0.116571, acc 0.966\n",
      "2017-03-29T23:42:45.960991: step 806, loss 0.0870327, acc 0.97\n",
      "2017-03-29T23:42:46.113430: step 807, loss 0.0709763, acc 0.978\n",
      "2017-03-29T23:42:46.256485: step 808, loss 0.065707, acc 0.986\n",
      "2017-03-29T23:42:46.399097: step 809, loss 0.092175, acc 0.97\n",
      "2017-03-29T23:42:46.558667: step 810, loss 0.10598, acc 0.97\n",
      "2017-03-29T23:42:46.707279: step 811, loss 0.0718694, acc 0.974\n",
      "2017-03-29T23:42:46.860204: step 812, loss 0.0865333, acc 0.976\n",
      "2017-03-29T23:42:47.019264: step 813, loss 0.116211, acc 0.962\n",
      "2017-03-29T23:42:47.176977: step 814, loss 0.0925814, acc 0.978\n",
      "2017-03-29T23:42:47.348093: step 815, loss 0.0795955, acc 0.976\n",
      "2017-03-29T23:42:47.513754: step 816, loss 0.0803951, acc 0.976\n",
      "2017-03-29T23:42:47.670187: step 817, loss 0.0934755, acc 0.976\n",
      "2017-03-29T23:42:47.819754: step 818, loss 0.11817, acc 0.96\n",
      "2017-03-29T23:42:47.988427: step 819, loss 0.0830891, acc 0.976\n",
      "2017-03-29T23:42:48.138782: step 820, loss 0.116016, acc 0.968\n",
      "2017-03-29T23:42:48.296554: step 821, loss 0.0634158, acc 0.982\n",
      "2017-03-29T23:42:48.462976: step 822, loss 0.144535, acc 0.96\n",
      "2017-03-29T23:42:48.622011: step 823, loss 0.123913, acc 0.964\n",
      "2017-03-29T23:42:48.768298: step 824, loss 0.099596, acc 0.966\n",
      "2017-03-29T23:42:48.921003: step 825, loss 0.0962792, acc 0.97\n",
      "2017-03-29T23:42:49.075590: step 826, loss 0.117416, acc 0.976\n",
      "2017-03-29T23:42:49.232505: step 827, loss 0.0755245, acc 0.978\n",
      "2017-03-29T23:42:49.389111: step 828, loss 0.0563219, acc 0.984\n",
      "2017-03-29T23:42:49.553715: step 829, loss 0.0853215, acc 0.97\n",
      "2017-03-29T23:42:49.713364: step 830, loss 0.0637425, acc 0.982\n",
      "2017-03-29T23:42:49.883222: step 831, loss 0.0840711, acc 0.976\n",
      "2017-03-29T23:42:50.038388: step 832, loss 0.130388, acc 0.958\n",
      "2017-03-29T23:42:50.206557: step 833, loss 0.0874425, acc 0.97\n",
      "2017-03-29T23:42:50.355172: step 834, loss 0.106608, acc 0.964\n",
      "2017-03-29T23:42:50.504344: step 835, loss 0.0899122, acc 0.97\n",
      "2017-03-29T23:42:50.670240: step 836, loss 0.124149, acc 0.964\n",
      "2017-03-29T23:42:50.818711: step 837, loss 0.0898652, acc 0.968\n",
      "2017-03-29T23:42:50.977661: step 838, loss 0.0915123, acc 0.974\n",
      "2017-03-29T23:42:51.121830: step 839, loss 0.0748106, acc 0.986\n",
      "2017-03-29T23:42:51.292692: step 840, loss 0.0981741, acc 0.97\n",
      "2017-03-29T23:42:51.440963: step 841, loss 0.062915, acc 0.984\n",
      "2017-03-29T23:42:51.592147: step 842, loss 0.0791019, acc 0.976\n",
      "2017-03-29T23:42:51.751082: step 843, loss 0.0907653, acc 0.962\n",
      "2017-03-29T23:42:51.901515: step 844, loss 0.0676028, acc 0.982\n",
      "2017-03-29T23:42:52.043572: step 845, loss 0.0728373, acc 0.98\n",
      "2017-03-29T23:42:52.190694: step 846, loss 0.124688, acc 0.968\n",
      "2017-03-29T23:42:52.366343: step 847, loss 0.117325, acc 0.962\n",
      "2017-03-29T23:42:52.529790: step 848, loss 0.139163, acc 0.964\n",
      "2017-03-29T23:42:52.679858: step 849, loss 0.0663341, acc 0.974\n",
      "2017-03-29T23:42:52.839774: step 850, loss 0.0758206, acc 0.978\n",
      "2017-03-29T23:42:53.000103: step 851, loss 0.09644, acc 0.964\n",
      "2017-03-29T23:42:53.149149: step 852, loss 0.119689, acc 0.966\n",
      "2017-03-29T23:42:53.294483: step 853, loss 0.109085, acc 0.968\n",
      "2017-03-29T23:42:53.463788: step 854, loss 0.0959922, acc 0.974\n",
      "2017-03-29T23:42:53.616757: step 855, loss 0.100798, acc 0.966\n",
      "2017-03-29T23:42:53.756587: step 856, loss 0.0779617, acc 0.978\n",
      "2017-03-29T23:42:53.896675: step 857, loss 0.0672228, acc 0.982\n",
      "2017-03-29T23:42:54.036498: step 858, loss 0.0883934, acc 0.972\n",
      "2017-03-29T23:42:54.204107: step 859, loss 0.0689117, acc 0.98\n",
      "2017-03-29T23:42:54.370455: step 860, loss 0.0953845, acc 0.966\n",
      "2017-03-29T23:42:54.537189: step 861, loss 0.109173, acc 0.966\n",
      "2017-03-29T23:42:54.684297: step 862, loss 0.12878, acc 0.972\n",
      "2017-03-29T23:42:54.843765: step 863, loss 0.0997648, acc 0.972\n",
      "2017-03-29T23:42:55.009587: step 864, loss 0.0809356, acc 0.976\n",
      "2017-03-29T23:42:55.189101: step 865, loss 0.0790025, acc 0.976\n",
      "2017-03-29T23:42:55.345755: step 866, loss 0.0747261, acc 0.98\n",
      "2017-03-29T23:42:55.493020: step 867, loss 0.117769, acc 0.966\n",
      "2017-03-29T23:42:55.657023: step 868, loss 0.0646926, acc 0.974\n",
      "2017-03-29T23:42:55.805130: step 869, loss 0.0447906, acc 0.986\n",
      "2017-03-29T23:42:55.973273: step 870, loss 0.0817299, acc 0.972\n",
      "2017-03-29T23:42:56.138009: step 871, loss 0.0650233, acc 0.982\n",
      "2017-03-29T23:42:56.310472: step 872, loss 0.0858975, acc 0.976\n",
      "2017-03-29T23:42:56.456877: step 873, loss 0.0984016, acc 0.97\n",
      "2017-03-29T23:42:56.628949: step 874, loss 0.125415, acc 0.974\n",
      "2017-03-29T23:42:56.781226: step 875, loss 0.068652, acc 0.982\n",
      "2017-03-29T23:42:56.924596: step 876, loss 0.0557964, acc 0.984\n",
      "2017-03-29T23:42:57.086734: step 877, loss 0.1314, acc 0.958\n",
      "2017-03-29T23:42:57.236099: step 878, loss 0.133721, acc 0.956\n",
      "2017-03-29T23:42:57.391640: step 879, loss 0.074161, acc 0.974\n",
      "2017-03-29T23:42:57.562324: step 880, loss 0.10246, acc 0.972\n",
      "2017-03-29T23:42:57.791845: step 881, loss 0.115139, acc 0.962\n",
      "2017-03-29T23:42:57.935513: step 882, loss 0.035956, acc 0.99\n",
      "2017-03-29T23:42:58.101138: step 883, loss 0.151677, acc 0.964\n",
      "2017-03-29T23:42:58.253403: step 884, loss 0.0735806, acc 0.982\n",
      "2017-03-29T23:42:58.399440: step 885, loss 0.0754459, acc 0.978\n",
      "2017-03-29T23:42:58.581557: step 886, loss 0.0899941, acc 0.97\n",
      "2017-03-29T23:42:58.727287: step 887, loss 0.0814408, acc 0.966\n",
      "2017-03-29T23:42:58.891520: step 888, loss 0.063167, acc 0.978\n",
      "2017-03-29T23:42:59.045182: step 889, loss 0.137704, acc 0.962\n",
      "2017-03-29T23:42:59.183534: step 890, loss 0.0849305, acc 0.97\n",
      "2017-03-29T23:42:59.324663: step 891, loss 0.0866121, acc 0.976\n",
      "2017-03-29T23:42:59.469659: step 892, loss 0.0782872, acc 0.98\n",
      "2017-03-29T23:42:59.645850: step 893, loss 0.0506851, acc 0.986\n",
      "2017-03-29T23:42:59.820748: step 894, loss 0.0908139, acc 0.974\n",
      "2017-03-29T23:42:59.962521: step 895, loss 0.0775852, acc 0.982\n",
      "2017-03-29T23:43:00.128128: step 896, loss 0.0897727, acc 0.972\n",
      "2017-03-29T23:43:00.279283: step 897, loss 0.102912, acc 0.964\n",
      "2017-03-29T23:43:00.443661: step 898, loss 0.0544266, acc 0.98\n",
      "2017-03-29T23:43:00.590949: step 899, loss 0.0805798, acc 0.972\n",
      "2017-03-29T23:43:00.736229: step 900, loss 0.0771152, acc 0.978\n",
      "2017-03-29T23:43:00.884591: step 901, loss 0.0865912, acc 0.974\n",
      "2017-03-29T23:43:01.028306: step 902, loss 0.0742931, acc 0.978\n",
      "2017-03-29T23:43:01.190308: step 903, loss 0.0899028, acc 0.972\n",
      "2017-03-29T23:43:01.339495: step 904, loss 0.0794725, acc 0.986\n",
      "2017-03-29T23:43:01.487906: step 905, loss 0.0844144, acc 0.98\n",
      "2017-03-29T23:43:01.637114: step 906, loss 0.133632, acc 0.974\n",
      "2017-03-29T23:43:01.779176: step 907, loss 0.111794, acc 0.972\n",
      "2017-03-29T23:43:01.917020: step 908, loss 0.0883151, acc 0.976\n",
      "2017-03-29T23:43:02.077769: step 909, loss 0.0769316, acc 0.978\n",
      "2017-03-29T23:43:02.233391: step 910, loss 0.087716, acc 0.974\n",
      "2017-03-29T23:43:02.381182: step 911, loss 0.0703961, acc 0.978\n",
      "2017-03-29T23:43:02.550353: step 912, loss 0.0861417, acc 0.978\n",
      "2017-03-29T23:43:02.712834: step 913, loss 0.0680653, acc 0.98\n",
      "2017-03-29T23:43:02.879539: step 914, loss 0.106627, acc 0.964\n",
      "2017-03-29T23:43:03.046723: step 915, loss 0.0870137, acc 0.974\n",
      "2017-03-29T23:43:03.196822: step 916, loss 0.103574, acc 0.962\n",
      "2017-03-29T23:43:03.362900: step 917, loss 0.123408, acc 0.968\n",
      "2017-03-29T23:43:03.517335: step 918, loss 0.0880255, acc 0.97\n",
      "2017-03-29T23:43:03.656255: step 919, loss 0.0949685, acc 0.972\n",
      "2017-03-29T23:43:03.792571: step 920, loss 0.0869218, acc 0.97\n",
      "2017-03-29T23:43:03.944776: step 921, loss 0.0739801, acc 0.97\n",
      "2017-03-29T23:43:04.093065: step 922, loss 0.0866464, acc 0.974\n",
      "2017-03-29T23:43:04.232379: step 923, loss 0.067799, acc 0.984\n",
      "2017-03-29T23:43:04.391316: step 924, loss 0.0788769, acc 0.978\n",
      "2017-03-29T23:43:04.560733: step 925, loss 0.0937864, acc 0.966\n",
      "2017-03-29T23:43:04.701113: step 926, loss 0.101075, acc 0.96\n",
      "2017-03-29T23:43:04.842625: step 927, loss 0.111272, acc 0.97\n",
      "2017-03-29T23:43:05.013998: step 928, loss 0.141435, acc 0.958\n",
      "2017-03-29T23:43:05.166297: step 929, loss 0.0906725, acc 0.964\n",
      "2017-03-29T23:43:05.321173: step 930, loss 0.073468, acc 0.976\n",
      "2017-03-29T23:43:05.490397: step 931, loss 0.11202, acc 0.976\n",
      "2017-03-29T23:43:05.634770: step 932, loss 0.074922, acc 0.982\n",
      "2017-03-29T23:43:05.784804: step 933, loss 0.0584635, acc 0.986\n",
      "2017-03-29T23:43:05.934627: step 934, loss 0.0573838, acc 0.984\n",
      "2017-03-29T23:43:06.102900: step 935, loss 0.0991934, acc 0.966\n",
      "2017-03-29T23:43:06.250006: step 936, loss 0.118343, acc 0.964\n",
      "2017-03-29T23:43:06.412370: step 937, loss 0.0823428, acc 0.98\n",
      "2017-03-29T23:43:06.574980: step 938, loss 0.108678, acc 0.964\n",
      "2017-03-29T23:43:06.745146: step 939, loss 0.101523, acc 0.97\n",
      "2017-03-29T23:43:06.905130: step 940, loss 0.108446, acc 0.968\n",
      "2017-03-29T23:43:07.045387: step 941, loss 0.0767475, acc 0.978\n",
      "2017-03-29T23:43:07.186014: step 942, loss 0.0934324, acc 0.972\n",
      "2017-03-29T23:43:07.353044: step 943, loss 0.0829561, acc 0.974\n",
      "2017-03-29T23:43:07.520149: step 944, loss 0.0517299, acc 0.988\n",
      "2017-03-29T23:43:07.670532: step 945, loss 0.0901188, acc 0.978\n",
      "2017-03-29T23:43:07.820468: step 946, loss 0.0705166, acc 0.978\n",
      "2017-03-29T23:43:07.982852: step 947, loss 0.0833286, acc 0.97\n",
      "2017-03-29T23:43:08.132598: step 948, loss 0.069796, acc 0.966\n",
      "2017-03-29T23:43:08.275017: step 949, loss 0.0630881, acc 0.978\n",
      "2017-03-29T23:43:08.429267: step 950, loss 0.0612513, acc 0.98\n",
      "2017-03-29T23:43:08.599933: step 951, loss 0.0831211, acc 0.976\n",
      "2017-03-29T23:43:08.749398: step 952, loss 0.0881415, acc 0.974\n",
      "2017-03-29T23:43:08.888737: step 953, loss 0.107969, acc 0.966\n",
      "2017-03-29T23:43:09.045811: step 954, loss 0.0783485, acc 0.974\n",
      "2017-03-29T23:43:09.210922: step 955, loss 0.0683051, acc 0.978\n",
      "2017-03-29T23:43:09.377441: step 956, loss 0.0641031, acc 0.982\n",
      "2017-03-29T23:43:09.542279: step 957, loss 0.120649, acc 0.966\n",
      "2017-03-29T23:43:09.685285: step 958, loss 0.088288, acc 0.982\n",
      "2017-03-29T23:43:09.836642: step 959, loss 0.0575345, acc 0.98\n",
      "2017-03-29T23:43:09.985004: step 960, loss 0.0629322, acc 0.984\n",
      "2017-03-29T23:43:10.133644: step 961, loss 0.0771074, acc 0.974\n",
      "2017-03-29T23:43:10.292738: step 962, loss 0.106571, acc 0.972\n",
      "2017-03-29T23:43:10.440355: step 963, loss 0.07342, acc 0.978\n",
      "2017-03-29T23:43:10.601060: step 964, loss 0.100277, acc 0.97\n",
      "2017-03-29T23:43:10.749367: step 965, loss 0.0587298, acc 0.982\n",
      "2017-03-29T23:43:10.902816: step 966, loss 0.0855284, acc 0.97\n",
      "2017-03-29T23:43:11.055345: step 967, loss 0.0631166, acc 0.98\n",
      "2017-03-29T23:43:11.191177: step 968, loss 0.0991312, acc 0.972\n",
      "2017-03-29T23:43:11.340934: step 969, loss 0.0698567, acc 0.982\n",
      "2017-03-29T23:43:11.489626: step 970, loss 0.0734243, acc 0.976\n",
      "2017-03-29T23:43:11.650310: step 971, loss 0.0906559, acc 0.972\n",
      "2017-03-29T23:43:11.816113: step 972, loss 0.042472, acc 0.986\n",
      "2017-03-29T23:43:11.978841: step 973, loss 0.11107, acc 0.966\n",
      "2017-03-29T23:43:12.129545: step 974, loss 0.0910038, acc 0.97\n",
      "2017-03-29T23:43:12.308601: step 975, loss 0.08704, acc 0.968\n",
      "2017-03-29T23:43:12.473313: step 976, loss 0.0976365, acc 0.974\n",
      "2017-03-29T23:43:12.624027: step 977, loss 0.137442, acc 0.976\n",
      "2017-03-29T23:43:12.765510: step 978, loss 0.0793558, acc 0.978\n",
      "2017-03-29T23:43:12.927363: step 979, loss 0.0942938, acc 0.974\n",
      "2017-03-29T23:43:13.100580: step 980, loss 0.086375, acc 0.974\n",
      "2017-03-29T23:43:13.269006: step 981, loss 0.094914, acc 0.972\n",
      "2017-03-29T23:43:13.433171: step 982, loss 0.0853885, acc 0.97\n",
      "2017-03-29T23:43:13.601988: step 983, loss 0.105179, acc 0.964\n",
      "2017-03-29T23:43:13.764495: step 984, loss 0.0680394, acc 0.984\n",
      "2017-03-29T23:43:13.921296: step 985, loss 0.085204, acc 0.968\n",
      "2017-03-29T23:43:14.068158: step 986, loss 0.0935554, acc 0.972\n",
      "2017-03-29T23:43:14.215859: step 987, loss 0.105228, acc 0.97\n",
      "2017-03-29T23:43:14.367964: step 988, loss 0.0930076, acc 0.974\n",
      "2017-03-29T23:43:14.535428: step 989, loss 0.0722016, acc 0.982\n",
      "2017-03-29T23:43:14.683934: step 990, loss 0.108599, acc 0.964\n",
      "2017-03-29T23:43:14.926047: step 991, loss 0.12806, acc 0.98\n",
      "2017-03-29T23:43:15.078285: step 992, loss 0.0855379, acc 0.974\n",
      "2017-03-29T23:43:15.247350: step 993, loss 0.0773439, acc 0.98\n",
      "2017-03-29T23:43:15.397136: step 994, loss 0.0644406, acc 0.98\n",
      "2017-03-29T23:43:15.571471: step 995, loss 0.098838, acc 0.974\n",
      "2017-03-29T23:43:15.735224: step 996, loss 0.104823, acc 0.966\n",
      "2017-03-29T23:43:15.875997: step 997, loss 0.0728888, acc 0.98\n",
      "2017-03-29T23:43:16.023788: step 998, loss 0.0963238, acc 0.972\n",
      "2017-03-29T23:43:16.169767: step 999, loss 0.0885746, acc 0.972\n",
      "2017-03-29T23:43:16.332568: step 1000, loss 0.0770378, acc 0.974\n",
      "Test accuracy\n",
      "0.9763\n",
      "Saved model checkpoint to /home/csp/repo/deeplearning-tutorials/intro_tensorflow/runs/1490823626/summaries/train/model-1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# Train\n",
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(500)\n",
    "    feed_dict={mnist_dnn.input_x: batch_xs, mnist_dnn.input_y: batch_ys}\n",
    "    _, step, summaries, loss, accuracy = sess.run(\n",
    "        [train_op, global_step, train_summary_op, \n",
    "         mnist_dnn.loss, mnist_dnn.accuracy],  feed_dict)\n",
    "\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "    train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "    \n",
    "    \n",
    "# Test trained model\n",
    "print(\"Test accuracy\")\n",
    "[accuracy, _ ] =sess.run(\n",
    "    [mnist_dnn.accuracy, get_image_features_op], \n",
    "    feed_dict={mnist_dnn.input_x: mnist.test.images,\n",
    "               mnist_dnn.input_y: mnist.test.labels})\n",
    "print(accuracy)\n",
    "\n",
    "path = saver.save(sess, checkpoint_prefix, global_step=step)\n",
    "print(\"Saved model checkpoint to {}\\n\".format(path))    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "livereveal": {
   "height": 768,
   "scroll": true,
   "start_slideshow_at": "selected",
   "width": 1024
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
