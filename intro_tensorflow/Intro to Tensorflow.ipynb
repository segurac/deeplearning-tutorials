{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Deep Learning with Tensorflow\n",
    "<br><br><br>\n",
    "\n",
    "##  Carlos Segura \n",
    "<b>\n",
    "## Associate Researcher @ TID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Tensorflow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* TensorFlow is a deep learning library\n",
    "     * open-sourced by Google.\n",
    "* TensorFlow provides primitives for\n",
    "defining functions on tensors and\n",
    "automatically computing their derivatives.\n",
    "\n",
    "* Python library that can work with symbolic mathematical expressions\n",
    "    * *Symbolic differentation*: symbolic graphs for computing gradients\n",
    "* Optimized for multi-dimensional arrays, like numpy.ndarray\n",
    "    * Tensors\n",
    "* Same code can work in CPU or GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Installing Tensorflow\n",
    "* Requirements\n",
    "    * OS: Linux, Mac OS X, Windows\n",
    "    * Python: >= 2.7 || >= 3.5\n",
    "* pip install tensorflow [tensorflow-gpu]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic usage:\n",
    "\n",
    "* Represent computations as graphs\n",
    "* Execute graphs in the context of sessions\n",
    "* Represent data as tensors\n",
    "* Maintain state with Variables\n",
    "* Use feeds and fetches to get data in and out of graph executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Graphs\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create a Constant op that produces a 1x2 matrix.  The op is\n",
    "# added as a node to the default graph.\n",
    "#\n",
    "# The value returned by the constructor represents the output\n",
    "# of the Constant op.\n",
    "matrix1 = tf.constant([[3., 3.]])\n",
    "\n",
    "# Create another Constant that produces a 2x1 matrix.\n",
    "matrix2 = tf.constant([[2.],[2.]])\n",
    "\n",
    "# Create a Matmul op that takes 'matrix1' and 'matrix2' as inputs.\n",
    "# The returned value, 'product', represents the result of the matrix\n",
    "# multiplication.\n",
    "product = tf.matmul(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.]]\n"
     ]
    }
   ],
   "source": [
    "#To actually multiply the matrices, and get the result of the multiplication, you must launch the graph in a session.\n",
    "# Launch the default graph.\n",
    "sess = tf.Session()\n",
    "\n",
    "# To run the matmul op we call the session 'run()' method, passing 'product'\n",
    "# which represents the output of the matmul op.  This indicates to the call\n",
    "# that we want to get the output of the matmul op back.\n",
    "#\n",
    "# All inputs needed by the op are run automatically by the session.  They\n",
    "# typically are run in parallel.\n",
    "#\n",
    "# The call 'run(product)' thus causes the execution of three ops in the\n",
    "# graph: the two constants and matmul.\n",
    "#\n",
    "# The output of the op is returned in 'result' as a numpy `ndarray` object.\n",
    "result = sess.run(product)\n",
    "print(result)\n",
    "# ==> [[ 12.]]\n",
    "\n",
    "# Close the Session when we're done.\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 12.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Sessions should be closed to release resources.\n",
    "# You can also enter a Session with a \"with\" block. \n",
    "# The Session closes automatically at the end of the with block.\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  result = sess.run([product])\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "with tf.Session() as sess:\n",
    "  with tf.device(\"/gpu:0\"): #/cpu:0 /gpu:0 /gpu:1 /gpu:2\n",
    "    matrix1 = tf.constant([[3., 3.]])\n",
    "    matrix2 = tf.constant([[2.],[2.]])\n",
    "    product = tf.matmul(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Create a Variable, that will be initialized to the scalar value 0.\n",
    "state = tf.Variable(0, name=\"counter\")\n",
    "\n",
    "# Create an Op to add one to `state`.\n",
    "\n",
    "one = tf.constant(1)\n",
    "new_value = tf.add(state, one)\n",
    "update = tf.assign(state, new_value)\n",
    "\n",
    "# Variables must be initialized by running an `init` Op after having\n",
    "# launched the graph.  We first have to add the `init` Op to the graph.\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph and run the ops.\n",
    "with tf.Session() as sess:\n",
    "  # Run the 'init' op\n",
    "  sess.run(init_op)\n",
    "  # Print the initial value of 'state'\n",
    "  print(sess.run(state))\n",
    "  # Run the op that updates 'state' and print 'state'.\n",
    "  for _ in range(3):\n",
    "    sess.run(update)\n",
    "    print(sess.run(state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 21.], dtype=float32), array([ 7.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#Fetches:\n",
    "input1 = tf.constant([3.0])\n",
    "input2 = tf.constant([2.0])\n",
    "input3 = tf.constant([5.0])\n",
    "intermed = tf.add(input2, input3)\n",
    "mul = input1 * intermed\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  result = sess.run([mul, intermed])\n",
    "  print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 14.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#Feeds\n",
    "input1 = tf.placeholder(tf.float32)\n",
    "input2 = tf.placeholder(tf.float32)\n",
    "output = input1 * input2\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  print(sess.run([output], feed_dict={input1:[7.], input2:[2.]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression example\n",
    "\n",
    "<img src=\"http://www.atmos.washington.edu/~robwood/teaching/451/labs/images/concepts12.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAHVCAYAAAAHJgkZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f39b68a3ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reset\n",
    "import numpy as np\n",
    "trainX = np.linspace(-1,1, 200 )\n",
    "trainY = (2 * trainX + 1.0 + np.random.randn(*trainX.shape)*0.8)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure( figsize=(8, 8))\n",
    "plt.plot(trainX, trainY, 'bo', markeredgecolor='none')\n",
    "plt.ylabel('Line with noise')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "X = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")\n",
    "\n",
    "# Set model weights\n",
    "w = tf.Variable(np.random.randn(), name=\"weight\")\n",
    "b = tf.Variable(np.random.randn(), name=\"bias\")\n",
    "\n",
    "prediction = X*w + b\n",
    "loss = tf.reduce_mean(tf.pow(prediction-Y, 2))\n",
    "\n",
    "var_grads = tf.gradients(loss, [w,b])\n",
    "\n",
    "learning_rate = tf.constant(0.3)\n",
    "\n",
    "new_w =  w - var_grads[0] * learning_rate \n",
    "new_b =  b - var_grads[1] * learning_rate\n",
    "\n",
    "update_w = tf.assign(w, new_w)\n",
    "update_b = tf.assign(b, new_b)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlUAAAJCCAYAAADp1TKRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBpoqAIAGV0ZCK7kOw37zAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f394b7efda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:  0.573742  w: [2.1729417]  b: [1.0545049]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlUAAAJCCAYAAADp1TKRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBpoqAIAGV0ZCK7kOw37zAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f394b7efda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as plt\n",
    "from IPython import display\n",
    "\n",
    "fig = plt.figure( figsize=(10, 10))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for epoch in range(15):\n",
    "        [current_loss, _, _ ] = sess.run(\n",
    "                [loss, update_w, update_b ], \n",
    "                feed_dict={X:trainX, Y:trainY})\n",
    "        current_w = sess.run([w])\n",
    "        current_b = sess.run([b])\n",
    "        \n",
    "\n",
    "        plt.clf()\n",
    "        plt.plot(trainX, trainY,'o',color='b', markeredgecolor='none')\n",
    "        plt.plot(trainX, trainX*current_w + current_b, color='r',)\n",
    "        print('Cost: ', current_loss,' w:',current_w, ' b:',current_b)\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "print('Cost: ', current_loss,' w:',current_w, ' b:',current_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CNN example with MNIST\n",
    "\n",
    "Dataset of images of handwritten digits like these\n",
    "\n",
    "<div style=\"width: 400px\">\n",
    "![esto puede in en el style float:left;](https://www.tensorflow.org/images/MNIST.png)\n",
    "</div>\n",
    " Each image is 28 pixels by 28 pixels. Can be interpreted as a matrix:\n",
    "<div style=\"width: 200px\">\n",
    "![MNIST Pixels](https://www.tensorflow.org/images/MNIST-Matrix.png)\n",
    "</div>\n",
    "\n",
    "<div style=\"float:left;width: 400px\">\n",
    "![train ](https://www.tensorflow.org/images/mnist-train-xs.png)\n",
    "</div>\n",
    "\n",
    "<div style=\"float:right;width: 500px\">\n",
    "![1hot labels ](https://www.tensorflow.org/images/mnist-train-ys.png)\n",
    "</div>\n",
    "<br><br><br><br><br><br><br><br><br>\n",
    "Images are = [55000, 28 ,28]    \n",
    "reshaped into [5500, 784]         \n",
    "\n",
    "Labels are [55000, 10]\n",
    "(One-hot encoding of classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./MNIST_DATA/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./MNIST_DATA\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class MNIST_CNN():\n",
    "    \"\"\"\n",
    "    A network for a classifier of digit images.\n",
    "    Input are 2D grayscale images reshaped into a vector, and the labels in one-hot encoding\n",
    "    Uses 2 Conv Layers, 1 hidden layer and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_length, hidden_layer_size = 256, num_classes=10):\n",
    "        self.input_x = tf.placeholder(tf.float32, \n",
    "                            [None, img_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, \n",
    "                            [None, num_classes], name=\"input_y\")\n",
    "\n",
    "        # Input Layer\n",
    "        self.input_layer = tf.reshape(self.input_x, [-1, 28, 28, 1])\n",
    "\n",
    "\n",
    "        # Convolutional Layer #1\n",
    "        with tf.name_scope(\"Conv_1\"):\n",
    "            self.conv1 = tf.layers.conv2d(\n",
    "                  inputs=self.input_layer,\n",
    "                  filters=32,\n",
    "                  kernel_size=[5, 5],\n",
    "                  padding=\"same\",\n",
    "                  activation=tf.nn.relu)\n",
    "            # Pooling Layer #1\n",
    "            self.pool1 = tf.layers.max_pooling2d(inputs=self.conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "        # Convolutional Layer #2 and Pooling Layer #2\n",
    "        with tf.name_scope(\"Conv_2\"):\n",
    "            self.conv2 = tf.layers.conv2d(\n",
    "                  inputs=self.pool1,\n",
    "                  filters=64,\n",
    "                  kernel_size=[5, 5],\n",
    "                  padding=\"same\",\n",
    "                  activation=tf.nn.relu)\n",
    "            self.pool2 = tf.layers.max_pooling2d(inputs=self.conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "        #Flat feature maps\n",
    "        self.pool2_flat = tf.reshape(self.pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "        # Dense Layer\n",
    "        with tf.name_scope(\"hidden_layer_1\"):\n",
    "            W = tf.Variable(tf.truncated_normal([7 * 7 * 64, hidden_layer_size], stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[hidden_layer_size]), name=\"b\")\n",
    "            hl1 = tf.nn.relu ( tf.matmul(self.pool2_flat, W) + b )\n",
    "            #dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"softmax_layer\"):\n",
    "            W = tf.Variable(tf.truncated_normal([hidden_layer_size, num_classes], stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            self.scores = tf.nn.xw_plus_b(hl1, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#sess = tf.Session()\n",
    "sess = tf.InteractiveSession() #for running inside notebook\n",
    "\n",
    "mnist_dnn = MNIST_CNN(img_length=784, hidden_layer_size = 512, num_classes=10)\n",
    "\n",
    "# Define Training procedure\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "train_op = tf.train.GradientDescentOptimizer(0.5).minimize(mnist_dnn.loss, global_step=global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/csegura/repo/deeplearning-tutorials/intro_tensorflow/runs/1490781232\n",
      "\n",
      "counter:0\n",
      "weight:0\n",
      "bias:0\n",
      "conv2d/kernel:0\n",
      "conv2d/bias:0\n",
      "conv2d_1/kernel:0\n",
      "conv2d_1/bias:0\n",
      "hidden_layer_1/W:0\n",
      "hidden_layer_1/b:0\n",
      "softmax_layer/W:0\n",
      "softmax_layer/b:0\n"
     ]
    }
   ],
   "source": [
    "timestamp = str(int(time.time()))\n",
    "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "# Summaries for loss and accuracy\n",
    "loss_summary = tf.summary.scalar(\"loss\", mnist_dnn.loss)\n",
    "acc_summary = tf.summary.scalar(\"accuracy\", mnist_dnn.accuracy)\n",
    "\n",
    "\n",
    "with tf.name_scope('var_summary'):\n",
    "    for var in tf.trainable_variables():\n",
    "        with tf.name_scope(var.name.split(':')[0]):\n",
    "            print(var.name)\n",
    "            if var.name.startswith(\"counter\"):\n",
    "                continue\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            tf.summary.histogram('histogram', var)\n",
    "\n",
    "# Train Summaries\n",
    "#train_summary_op = tf.summary.merge([loss_summary, acc_summary ])\n",
    "train_summary_op = tf.summary.merge_all()\n",
    "train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "checkpoint_prefix = os.path.join(train_summary_dir, \"model\")\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-03-29T11:57:54.925830: step 1, loss 2.38586, acc 0.04\n",
      "2017-03-29T11:57:55.065492: step 2, loss 5.33027, acc 0.09\n",
      "2017-03-29T11:57:55.206778: step 3, loss 5.46975, acc 0.08\n",
      "2017-03-29T11:57:55.351634: step 4, loss 2.70691, acc 0.11\n",
      "2017-03-29T11:57:55.495668: step 5, loss 2.50241, acc 0.1\n",
      "2017-03-29T11:57:55.640728: step 6, loss 2.49662, acc 0.14\n",
      "2017-03-29T11:57:55.785079: step 7, loss 2.38134, acc 0.07\n",
      "2017-03-29T11:57:55.930808: step 8, loss 2.3413, acc 0.06\n",
      "2017-03-29T11:57:56.074831: step 9, loss 2.31554, acc 0.09\n",
      "2017-03-29T11:57:56.220382: step 10, loss 2.28967, acc 0.1\n",
      "2017-03-29T11:57:56.364561: step 11, loss 2.30343, acc 0.08\n",
      "2017-03-29T11:57:56.508328: step 12, loss 2.30797, acc 0.08\n",
      "2017-03-29T11:57:56.653058: step 13, loss 2.30418, acc 0.11\n",
      "2017-03-29T11:57:56.798589: step 14, loss 2.31698, acc 0.08\n",
      "2017-03-29T11:57:56.942921: step 15, loss 2.30441, acc 0.11\n",
      "2017-03-29T11:57:57.087937: step 16, loss 2.30298, acc 0.14\n",
      "2017-03-29T11:57:57.232398: step 17, loss 2.32826, acc 0.1\n",
      "2017-03-29T11:57:57.377488: step 18, loss 2.29949, acc 0.12\n",
      "2017-03-29T11:57:57.522910: step 19, loss 2.31114, acc 0.09\n",
      "2017-03-29T11:57:57.667759: step 20, loss 2.30374, acc 0.08\n",
      "2017-03-29T11:57:57.811856: step 21, loss 2.30213, acc 0.12\n",
      "2017-03-29T11:57:57.956961: step 22, loss 2.3031, acc 0.12\n",
      "2017-03-29T11:57:58.100902: step 23, loss 2.31984, acc 0.08\n",
      "2017-03-29T11:57:58.246439: step 24, loss 2.32026, acc 0.05\n",
      "2017-03-29T11:57:58.390344: step 25, loss 2.29623, acc 0.1\n",
      "2017-03-29T11:57:58.535499: step 26, loss 2.31656, acc 0.13\n",
      "2017-03-29T11:57:58.679952: step 27, loss 2.30674, acc 0.04\n",
      "2017-03-29T11:57:58.824474: step 28, loss 2.29758, acc 0.11\n",
      "2017-03-29T11:57:58.969632: step 29, loss 2.3128, acc 0.1\n",
      "2017-03-29T11:57:59.114683: step 30, loss 2.31916, acc 0.1\n",
      "2017-03-29T11:57:59.259255: step 31, loss 2.30566, acc 0.11\n",
      "2017-03-29T11:57:59.404277: step 32, loss 2.28882, acc 0.13\n",
      "2017-03-29T11:57:59.549809: step 33, loss 2.31675, acc 0.09\n",
      "2017-03-29T11:57:59.695372: step 34, loss 2.31814, acc 0.12\n",
      "2017-03-29T11:57:59.840031: step 35, loss 2.33897, acc 0.04\n",
      "2017-03-29T11:57:59.984198: step 36, loss 2.2845, acc 0.15\n",
      "2017-03-29T11:58:00.128567: step 37, loss 2.30745, acc 0.12\n",
      "2017-03-29T11:58:00.273587: step 38, loss 2.32455, acc 0.08\n",
      "2017-03-29T11:58:00.418903: step 39, loss 2.30586, acc 0.14\n",
      "2017-03-29T11:58:00.563338: step 40, loss 2.30401, acc 0.09\n",
      "2017-03-29T11:58:00.708738: step 41, loss 2.3237, acc 0.07\n",
      "2017-03-29T11:58:00.853474: step 42, loss 2.31821, acc 0.09\n",
      "2017-03-29T11:58:00.998285: step 43, loss 2.31249, acc 0.03\n",
      "2017-03-29T11:58:01.143576: step 44, loss 2.33872, acc 0.07\n",
      "2017-03-29T11:58:01.288386: step 45, loss 2.32259, acc 0.06\n",
      "2017-03-29T11:58:01.432293: step 46, loss 2.31827, acc 0.07\n",
      "2017-03-29T11:58:01.577640: step 47, loss 2.31155, acc 0.1\n",
      "2017-03-29T11:58:01.722734: step 48, loss 2.32622, acc 0.09\n",
      "2017-03-29T11:58:01.866739: step 49, loss 2.30496, acc 0.08\n",
      "2017-03-29T11:58:02.011685: step 50, loss 2.30357, acc 0.08\n",
      "2017-03-29T11:58:02.156736: step 51, loss 2.29436, acc 0.1\n",
      "2017-03-29T11:58:02.301175: step 52, loss 2.31502, acc 0.1\n",
      "2017-03-29T11:58:02.445911: step 53, loss 2.31171, acc 0.08\n",
      "2017-03-29T11:58:02.590051: step 54, loss 2.3062, acc 0.12\n",
      "2017-03-29T11:58:02.734277: step 55, loss 2.31865, acc 0.08\n",
      "2017-03-29T11:58:02.879331: step 56, loss 2.30884, acc 0.06\n",
      "2017-03-29T11:58:03.023911: step 57, loss 2.30722, acc 0.14\n",
      "2017-03-29T11:58:03.168702: step 58, loss 2.33346, acc 0.08\n",
      "2017-03-29T11:58:03.313704: step 59, loss 2.29963, acc 0.12\n",
      "2017-03-29T11:58:03.458479: step 60, loss 2.30963, acc 0.09\n",
      "2017-03-29T11:58:03.602949: step 61, loss 2.31333, acc 0.1\n",
      "2017-03-29T11:58:03.748316: step 62, loss 2.32661, acc 0.08\n",
      "2017-03-29T11:58:03.892714: step 63, loss 2.29442, acc 0.06\n",
      "2017-03-29T11:58:04.038270: step 64, loss 2.31851, acc 0.13\n",
      "2017-03-29T11:58:04.183567: step 65, loss 2.31545, acc 0.09\n",
      "2017-03-29T11:58:04.327744: step 66, loss 2.31503, acc 0.12\n",
      "2017-03-29T11:58:04.472175: step 67, loss 2.29959, acc 0.1\n",
      "2017-03-29T11:58:04.617797: step 68, loss 2.32035, acc 0.08\n",
      "2017-03-29T11:58:04.762763: step 69, loss 2.31712, acc 0.11\n",
      "2017-03-29T11:58:04.908578: step 70, loss 2.31129, acc 0.07\n",
      "2017-03-29T11:58:05.053157: step 71, loss 2.30666, acc 0.1\n",
      "2017-03-29T11:58:05.197676: step 72, loss 2.29805, acc 0.11\n",
      "2017-03-29T11:58:05.341907: step 73, loss 2.31515, acc 0.1\n",
      "2017-03-29T11:58:05.486909: step 74, loss 2.29422, acc 0.18\n",
      "2017-03-29T11:58:05.632200: step 75, loss 2.33686, acc 0.09\n",
      "2017-03-29T11:58:05.777305: step 76, loss 2.31972, acc 0.1\n",
      "2017-03-29T11:58:05.922500: step 77, loss 2.31357, acc 0.09\n",
      "2017-03-29T11:58:06.066958: step 78, loss 2.3016, acc 0.12\n",
      "2017-03-29T11:58:06.211181: step 79, loss 2.31263, acc 0.11\n",
      "2017-03-29T11:58:06.356253: step 80, loss 2.31061, acc 0.09\n",
      "2017-03-29T11:58:06.501127: step 81, loss 2.33432, acc 0.04\n",
      "2017-03-29T11:58:06.645849: step 82, loss 2.3068, acc 0.1\n",
      "2017-03-29T11:58:06.790125: step 83, loss 2.32513, acc 0.05\n",
      "2017-03-29T11:58:06.935111: step 84, loss 2.32492, acc 0.08\n",
      "2017-03-29T11:58:07.079076: step 85, loss 2.32017, acc 0.08\n",
      "2017-03-29T11:58:07.223592: step 86, loss 2.31795, acc 0.09\n",
      "2017-03-29T11:58:07.367147: step 87, loss 2.28764, acc 0.15\n",
      "2017-03-29T11:58:07.512453: step 88, loss 2.31715, acc 0.11\n",
      "2017-03-29T11:58:07.656498: step 89, loss 2.32412, acc 0.08\n",
      "2017-03-29T11:58:07.801610: step 90, loss 2.30917, acc 0.09\n",
      "2017-03-29T11:58:07.946628: step 91, loss 2.30487, acc 0.1\n",
      "2017-03-29T11:58:08.090895: step 92, loss 2.32181, acc 0.09\n",
      "2017-03-29T11:58:08.236124: step 93, loss 2.31784, acc 0.07\n",
      "2017-03-29T11:58:08.380449: step 94, loss 2.30591, acc 0.11\n",
      "2017-03-29T11:58:08.525315: step 95, loss 2.31488, acc 0.09\n",
      "2017-03-29T11:58:08.669816: step 96, loss 2.31417, acc 0.01\n",
      "2017-03-29T11:58:08.814592: step 97, loss 2.33037, acc 0.08\n",
      "2017-03-29T11:58:08.960181: step 98, loss 2.30311, acc 0.1\n",
      "2017-03-29T11:58:09.105389: step 99, loss 2.31869, acc 0.04\n",
      "2017-03-29T11:58:09.250090: step 100, loss 2.29469, acc 0.17\n",
      "2017-03-29T11:58:09.394556: step 101, loss 2.32771, acc 0.13\n",
      "2017-03-29T11:58:09.539417: step 102, loss 2.30686, acc 0.11\n",
      "2017-03-29T11:58:09.684338: step 103, loss 2.31684, acc 0.13\n",
      "2017-03-29T11:58:09.829021: step 104, loss 2.30695, acc 0.13\n",
      "2017-03-29T11:58:09.973416: step 105, loss 2.31711, acc 0.09\n",
      "2017-03-29T11:58:10.118616: step 106, loss 2.32358, acc 0.06\n",
      "2017-03-29T11:58:10.264252: step 107, loss 2.31085, acc 0.1\n",
      "2017-03-29T11:58:10.408661: step 108, loss 2.30196, acc 0.13\n",
      "2017-03-29T11:58:10.553394: step 109, loss 2.30877, acc 0.1\n",
      "2017-03-29T11:58:10.697381: step 110, loss 2.29852, acc 0.12\n",
      "2017-03-29T11:58:10.839998: step 111, loss 2.32142, acc 0.1\n",
      "2017-03-29T11:58:10.983048: step 112, loss 2.2955, acc 0.08\n",
      "2017-03-29T11:58:11.123800: step 113, loss 2.29279, acc 0.1\n",
      "2017-03-29T11:58:11.267094: step 114, loss 2.30032, acc 0.15\n",
      "2017-03-29T11:58:11.409559: step 115, loss 2.32265, acc 0.09\n",
      "2017-03-29T11:58:11.553243: step 116, loss 2.3192, acc 0.09\n",
      "2017-03-29T11:58:11.693809: step 117, loss 2.30344, acc 0.08\n",
      "2017-03-29T11:58:11.837583: step 118, loss 2.30538, acc 0.11\n",
      "2017-03-29T11:58:11.980269: step 119, loss 2.30115, acc 0.12\n",
      "2017-03-29T11:58:12.124427: step 120, loss 2.30918, acc 0.1\n",
      "2017-03-29T11:58:12.269883: step 121, loss 2.31684, acc 0.13\n",
      "2017-03-29T11:58:12.415347: step 122, loss 2.29202, acc 0.1\n",
      "2017-03-29T11:58:12.559906: step 123, loss 2.31191, acc 0.09\n",
      "2017-03-29T11:58:12.705082: step 124, loss 2.29905, acc 0.15\n",
      "2017-03-29T11:58:12.850391: step 125, loss 2.30674, acc 0.12\n",
      "2017-03-29T11:58:12.995557: step 126, loss 2.33423, acc 0.09\n",
      "2017-03-29T11:58:13.140292: step 127, loss 2.3111, acc 0.1\n",
      "2017-03-29T11:58:13.284850: step 128, loss 2.29437, acc 0.1\n",
      "2017-03-29T11:58:13.425435: step 129, loss 2.30698, acc 0.06\n",
      "2017-03-29T11:58:13.569271: step 130, loss 2.2837, acc 0.12\n",
      "2017-03-29T11:58:13.710468: step 131, loss 2.29649, acc 0.11\n",
      "2017-03-29T11:58:13.853760: step 132, loss 2.31903, acc 0.09\n",
      "2017-03-29T11:58:13.994752: step 133, loss 2.32451, acc 0.09\n",
      "2017-03-29T11:58:14.138715: step 134, loss 2.31115, acc 0.12\n",
      "2017-03-29T11:58:14.280569: step 135, loss 2.30183, acc 0.16\n",
      "2017-03-29T11:58:14.425007: step 136, loss 2.32386, acc 0.09\n",
      "2017-03-29T11:58:14.570027: step 137, loss 2.30857, acc 0.1\n",
      "2017-03-29T11:58:14.715357: step 138, loss 2.30518, acc 0.09\n",
      "2017-03-29T11:58:14.860441: step 139, loss 2.30359, acc 0.1\n",
      "2017-03-29T11:58:15.005309: step 140, loss 2.30895, acc 0.07\n",
      "2017-03-29T11:58:15.149623: step 141, loss 2.31008, acc 0.06\n",
      "2017-03-29T11:58:15.294354: step 142, loss 2.30324, acc 0.12\n",
      "2017-03-29T11:58:15.439774: step 143, loss 2.31643, acc 0.09\n",
      "2017-03-29T11:58:15.585182: step 144, loss 2.3106, acc 0.13\n",
      "2017-03-29T11:58:15.730070: step 145, loss 2.31523, acc 0.08\n",
      "2017-03-29T11:58:15.875456: step 146, loss 2.30909, acc 0.1\n",
      "2017-03-29T11:58:16.020219: step 147, loss 2.30023, acc 0.09\n",
      "2017-03-29T11:58:16.164920: step 148, loss 2.29452, acc 0.11\n",
      "2017-03-29T11:58:16.309995: step 149, loss 2.31422, acc 0.12\n",
      "2017-03-29T11:58:16.455168: step 150, loss 2.30389, acc 0.12\n",
      "2017-03-29T11:58:16.598933: step 151, loss 2.3163, acc 0.07\n",
      "2017-03-29T11:58:16.743500: step 152, loss 2.31109, acc 0.11\n",
      "2017-03-29T11:58:16.888533: step 153, loss 2.32368, acc 0.08\n",
      "2017-03-29T11:58:17.033189: step 154, loss 2.30303, acc 0.12\n",
      "2017-03-29T11:58:17.178547: step 155, loss 2.30835, acc 0.1\n",
      "2017-03-29T11:58:17.322501: step 156, loss 2.31652, acc 0.12\n",
      "2017-03-29T11:58:17.466640: step 157, loss 2.30302, acc 0.06\n",
      "2017-03-29T11:58:17.610503: step 158, loss 2.31782, acc 0.1\n",
      "2017-03-29T11:58:17.754689: step 159, loss 2.31249, acc 0.12\n",
      "2017-03-29T11:58:17.898799: step 160, loss 2.30264, acc 0.09\n",
      "2017-03-29T11:58:18.042971: step 161, loss 2.30706, acc 0.14\n",
      "2017-03-29T11:58:18.186639: step 162, loss 2.2986, acc 0.1\n",
      "2017-03-29T11:58:18.331214: step 163, loss 2.31437, acc 0.1\n",
      "2017-03-29T11:58:18.474821: step 164, loss 2.30219, acc 0.12\n",
      "2017-03-29T11:58:18.620215: step 165, loss 2.30707, acc 0.12\n",
      "2017-03-29T11:58:18.765728: step 166, loss 2.32899, acc 0.08\n",
      "2017-03-29T11:58:18.908129: step 167, loss 2.28521, acc 0.14\n",
      "2017-03-29T11:58:19.053002: step 168, loss 2.31552, acc 0.06\n",
      "2017-03-29T11:58:19.195548: step 169, loss 2.32292, acc 0.09\n",
      "2017-03-29T11:58:19.339705: step 170, loss 2.30039, acc 0.14\n",
      "2017-03-29T11:58:19.480547: step 171, loss 2.31822, acc 0.1\n",
      "2017-03-29T11:58:19.624117: step 172, loss 2.30957, acc 0.05\n",
      "2017-03-29T11:58:19.764344: step 173, loss 2.30587, acc 0.13\n",
      "2017-03-29T11:58:19.908249: step 174, loss 2.31162, acc 0.13\n",
      "2017-03-29T11:58:20.052268: step 175, loss 2.31666, acc 0.08\n",
      "2017-03-29T11:58:20.196315: step 176, loss 2.31483, acc 0.08\n",
      "2017-03-29T11:58:20.340634: step 177, loss 2.30764, acc 0.14\n",
      "2017-03-29T11:58:20.487154: step 178, loss 2.30585, acc 0.09\n",
      "2017-03-29T11:58:20.631263: step 179, loss 2.31242, acc 0.06\n",
      "2017-03-29T11:58:20.776100: step 180, loss 2.30737, acc 0.09\n",
      "2017-03-29T11:58:20.917809: step 181, loss 2.32557, acc 0.07\n",
      "2017-03-29T11:58:21.061306: step 182, loss 2.30844, acc 0.09\n",
      "2017-03-29T11:58:21.203651: step 183, loss 2.31547, acc 0.05\n",
      "2017-03-29T11:58:21.347833: step 184, loss 2.30272, acc 0.14\n",
      "2017-03-29T11:58:21.488253: step 185, loss 2.3032, acc 0.11\n",
      "2017-03-29T11:58:21.633061: step 186, loss 2.30853, acc 0.11\n",
      "2017-03-29T11:58:21.773898: step 187, loss 2.29912, acc 0.08\n",
      "2017-03-29T11:58:21.919596: step 188, loss 2.32456, acc 0.04\n",
      "2017-03-29T11:58:22.063841: step 189, loss 2.31919, acc 0.05\n",
      "2017-03-29T11:58:22.209232: step 190, loss 2.30251, acc 0.12\n",
      "2017-03-29T11:58:22.352097: step 191, loss 2.30862, acc 0.1\n",
      "2017-03-29T11:58:22.496115: step 192, loss 2.30629, acc 0.1\n",
      "2017-03-29T11:58:22.636704: step 193, loss 2.30686, acc 0.11\n",
      "2017-03-29T11:58:22.781175: step 194, loss 2.30675, acc 0.13\n",
      "2017-03-29T11:58:22.921700: step 195, loss 2.3086, acc 0.11\n",
      "2017-03-29T11:58:23.066707: step 196, loss 2.30417, acc 0.08\n",
      "2017-03-29T11:58:23.211450: step 197, loss 2.32081, acc 0.1\n",
      "2017-03-29T11:58:23.356703: step 198, loss 2.30068, acc 0.1\n",
      "2017-03-29T11:58:23.499426: step 199, loss 2.30299, acc 0.08\n",
      "2017-03-29T11:58:23.643521: step 200, loss 2.31485, acc 0.15\n",
      "2017-03-29T11:58:23.788046: step 201, loss 2.3101, acc 0.13\n",
      "2017-03-29T11:58:23.931254: step 202, loss 2.30012, acc 0.1\n",
      "2017-03-29T11:58:24.074051: step 203, loss 2.32372, acc 0.08\n",
      "2017-03-29T11:58:24.218455: step 204, loss 2.31467, acc 0.12\n",
      "2017-03-29T11:58:24.361133: step 205, loss 2.29223, acc 0.16\n",
      "2017-03-29T11:58:24.504928: step 206, loss 2.31074, acc 0.1\n",
      "2017-03-29T11:58:24.645337: step 207, loss 2.32151, acc 0.08\n",
      "2017-03-29T11:58:24.788813: step 208, loss 2.31146, acc 0.09\n",
      "2017-03-29T11:58:24.929079: step 209, loss 2.28987, acc 0.15\n",
      "2017-03-29T11:58:25.072492: step 210, loss 2.32661, acc 0.12\n",
      "2017-03-29T11:58:25.212213: step 211, loss 2.30642, acc 0.1\n",
      "2017-03-29T11:58:25.357242: step 212, loss 2.31369, acc 0.06\n",
      "2017-03-29T11:58:25.497955: step 213, loss 2.31935, acc 0.1\n",
      "2017-03-29T11:58:25.641454: step 214, loss 2.3302, acc 0.07\n",
      "2017-03-29T11:58:25.781903: step 215, loss 2.28245, acc 0.15\n",
      "2017-03-29T11:58:25.926184: step 216, loss 2.31805, acc 0.09\n",
      "2017-03-29T11:58:26.066777: step 217, loss 2.31035, acc 0.08\n",
      "2017-03-29T11:58:26.210153: step 218, loss 2.30669, acc 0.13\n",
      "2017-03-29T11:58:26.351293: step 219, loss 2.33005, acc 0.07\n",
      "2017-03-29T11:58:26.494919: step 220, loss 2.31014, acc 0.09\n",
      "2017-03-29T11:58:26.634423: step 221, loss 2.31205, acc 0.07\n",
      "2017-03-29T11:58:26.779142: step 222, loss 2.31317, acc 0.09\n",
      "2017-03-29T11:58:26.919824: step 223, loss 2.30209, acc 0.13\n",
      "2017-03-29T11:58:27.063931: step 224, loss 2.30943, acc 0.1\n",
      "2017-03-29T11:58:27.204101: step 225, loss 2.30126, acc 0.11\n",
      "2017-03-29T11:58:27.348893: step 226, loss 2.31669, acc 0.07\n",
      "2017-03-29T11:58:27.492262: step 227, loss 2.29883, acc 0.13\n",
      "2017-03-29T11:58:27.637869: step 228, loss 2.2971, acc 0.12\n",
      "2017-03-29T11:58:27.782111: step 229, loss 2.28691, acc 0.14\n",
      "2017-03-29T11:58:27.927179: step 230, loss 2.34763, acc 0.05\n",
      "2017-03-29T11:58:28.071680: step 231, loss 2.30034, acc 0.12\n",
      "2017-03-29T11:58:28.216357: step 232, loss 2.30524, acc 0.12\n",
      "2017-03-29T11:58:28.361274: step 233, loss 2.29422, acc 0.11\n",
      "2017-03-29T11:58:28.505998: step 234, loss 2.31915, acc 0.09\n",
      "2017-03-29T11:58:28.651261: step 235, loss 2.31786, acc 0.08\n",
      "2017-03-29T11:58:28.795578: step 236, loss 2.30951, acc 0.08\n",
      "2017-03-29T11:58:28.940205: step 237, loss 2.2999, acc 0.11\n",
      "2017-03-29T11:58:29.084937: step 238, loss 2.30912, acc 0.09\n",
      "2017-03-29T11:58:29.229350: step 239, loss 2.29374, acc 0.14\n",
      "2017-03-29T11:58:29.374005: step 240, loss 2.30634, acc 0.1\n",
      "2017-03-29T11:58:29.519516: step 241, loss 2.31842, acc 0.14\n",
      "2017-03-29T11:58:29.663521: step 242, loss 2.32196, acc 0.05\n",
      "2017-03-29T11:58:29.808581: step 243, loss 2.31608, acc 0.09\n",
      "2017-03-29T11:58:29.953374: step 244, loss 2.30949, acc 0.08\n",
      "2017-03-29T11:58:30.098211: step 245, loss 2.30149, acc 0.11\n",
      "2017-03-29T11:58:30.241952: step 246, loss 2.29588, acc 0.13\n",
      "2017-03-29T11:58:30.387255: step 247, loss 2.33494, acc 0.08\n",
      "2017-03-29T11:58:30.531020: step 248, loss 2.31633, acc 0.04\n",
      "2017-03-29T11:58:30.675234: step 249, loss 2.31721, acc 0.11\n",
      "2017-03-29T11:58:30.819845: step 250, loss 2.29927, acc 0.09\n",
      "2017-03-29T11:58:30.964317: step 251, loss 2.32552, acc 0.09\n",
      "2017-03-29T11:58:31.108591: step 252, loss 2.2963, acc 0.07\n",
      "2017-03-29T11:58:31.252760: step 253, loss 2.29897, acc 0.07\n",
      "2017-03-29T11:58:31.397485: step 254, loss 2.32034, acc 0.11\n",
      "2017-03-29T11:58:31.542449: step 255, loss 2.30502, acc 0.11\n",
      "2017-03-29T11:58:31.687280: step 256, loss 2.30756, acc 0.11\n",
      "2017-03-29T11:58:31.827463: step 257, loss 2.30107, acc 0.08\n",
      "2017-03-29T11:58:31.970885: step 258, loss 2.29789, acc 0.12\n",
      "2017-03-29T11:58:32.111173: step 259, loss 2.3052, acc 0.04\n",
      "2017-03-29T11:58:32.255628: step 260, loss 2.31616, acc 0.08\n",
      "2017-03-29T11:58:32.395919: step 261, loss 2.31918, acc 0.09\n",
      "2017-03-29T11:58:32.540855: step 262, loss 2.30372, acc 0.11\n",
      "2017-03-29T11:58:32.681685: step 263, loss 2.30197, acc 0.13\n",
      "2017-03-29T11:58:32.824419: step 264, loss 2.28917, acc 0.13\n",
      "2017-03-29T11:58:32.965469: step 265, loss 2.33525, acc 0.07\n",
      "2017-03-29T11:58:33.110251: step 266, loss 2.31376, acc 0.1\n",
      "2017-03-29T11:58:33.254343: step 267, loss 2.32103, acc 0.06\n",
      "2017-03-29T11:58:33.397717: step 268, loss 2.3116, acc 0.08\n",
      "2017-03-29T11:58:33.543148: step 269, loss 2.29903, acc 0.14\n",
      "2017-03-29T11:58:33.686886: step 270, loss 2.28838, acc 0.15\n",
      "2017-03-29T11:58:33.831517: step 271, loss 2.31326, acc 0.13\n",
      "2017-03-29T11:58:33.974837: step 272, loss 2.31125, acc 0.09\n",
      "2017-03-29T11:58:34.118977: step 273, loss 2.32076, acc 0.08\n",
      "2017-03-29T11:58:34.262656: step 274, loss 2.29766, acc 0.09\n",
      "2017-03-29T11:58:34.406011: step 275, loss 2.30967, acc 0.1\n",
      "2017-03-29T11:58:34.550507: step 276, loss 2.32256, acc 0.12\n",
      "2017-03-29T11:58:34.689851: step 277, loss 2.31455, acc 0.06\n",
      "2017-03-29T11:58:34.834591: step 278, loss 2.31191, acc 0.12\n",
      "2017-03-29T11:58:34.977814: step 279, loss 2.31558, acc 0.07\n",
      "2017-03-29T11:58:35.121594: step 280, loss 2.31166, acc 0.07\n",
      "2017-03-29T11:58:35.266537: step 281, loss 2.29101, acc 0.14\n",
      "2017-03-29T11:58:35.410352: step 282, loss 2.31909, acc 0.08\n",
      "2017-03-29T11:58:35.552967: step 283, loss 2.31582, acc 0.09\n",
      "2017-03-29T11:58:35.697129: step 284, loss 2.29778, acc 0.17\n",
      "2017-03-29T11:58:35.836749: step 285, loss 2.298, acc 0.17\n",
      "2017-03-29T11:58:35.980828: step 286, loss 2.31212, acc 0.11\n",
      "2017-03-29T11:58:36.125273: step 287, loss 2.31089, acc 0.14\n",
      "2017-03-29T11:58:36.270085: step 288, loss 2.31757, acc 0.1\n",
      "2017-03-29T11:58:36.412158: step 289, loss 2.31215, acc 0.1\n",
      "2017-03-29T11:58:36.557015: step 290, loss 2.29789, acc 0.12\n",
      "2017-03-29T11:58:36.699550: step 291, loss 2.30236, acc 0.13\n",
      "2017-03-29T11:58:36.843620: step 292, loss 2.32043, acc 0.11\n",
      "2017-03-29T11:58:36.984022: step 293, loss 2.30004, acc 0.11\n",
      "2017-03-29T11:58:37.128108: step 294, loss 2.30894, acc 0.07\n",
      "2017-03-29T11:58:37.271584: step 295, loss 2.32601, acc 0.04\n",
      "2017-03-29T11:58:37.415372: step 296, loss 2.30832, acc 0.1\n",
      "2017-03-29T11:58:37.560709: step 297, loss 2.30741, acc 0.14\n",
      "2017-03-29T11:58:37.706550: step 298, loss 2.3035, acc 0.06\n",
      "2017-03-29T11:58:37.850869: step 299, loss 2.32658, acc 0.07\n",
      "2017-03-29T11:58:37.994214: step 300, loss 2.30208, acc 0.06\n",
      "2017-03-29T11:58:38.137001: step 301, loss 2.30395, acc 0.03\n",
      "2017-03-29T11:58:38.281069: step 302, loss 2.31687, acc 0.06\n",
      "2017-03-29T11:58:38.423246: step 303, loss 2.31443, acc 0.08\n",
      "2017-03-29T11:58:38.568596: step 304, loss 2.29922, acc 0.1\n",
      "2017-03-29T11:58:38.712640: step 305, loss 2.31501, acc 0.09\n",
      "2017-03-29T11:58:38.856453: step 306, loss 2.29628, acc 0.07\n",
      "2017-03-29T11:58:38.999457: step 307, loss 2.32218, acc 0.11\n",
      "2017-03-29T11:58:39.143638: step 308, loss 2.32714, acc 0.06\n",
      "2017-03-29T11:58:39.285658: step 309, loss 2.29752, acc 0.08\n",
      "2017-03-29T11:58:39.430760: step 310, loss 2.31269, acc 0.1\n",
      "2017-03-29T11:58:39.574193: step 311, loss 2.31607, acc 0.12\n",
      "2017-03-29T11:58:39.718914: step 312, loss 2.30131, acc 0.1\n",
      "2017-03-29T11:58:39.862504: step 313, loss 2.32352, acc 0.09\n",
      "2017-03-29T11:58:40.007105: step 314, loss 2.30158, acc 0.1\n",
      "2017-03-29T11:58:40.147156: step 315, loss 2.3043, acc 0.1\n",
      "2017-03-29T11:58:40.291782: step 316, loss 2.29778, acc 0.09\n",
      "2017-03-29T11:58:40.432203: step 317, loss 2.30992, acc 0.1\n",
      "2017-03-29T11:58:40.575576: step 318, loss 2.30776, acc 0.09\n",
      "2017-03-29T11:58:40.716003: step 319, loss 2.30929, acc 0.11\n",
      "2017-03-29T11:58:40.860520: step 320, loss 2.31616, acc 0.07\n",
      "2017-03-29T11:58:41.000488: step 321, loss 2.30284, acc 0.08\n",
      "2017-03-29T11:58:41.145150: step 322, loss 2.30468, acc 0.09\n",
      "2017-03-29T11:58:41.285052: step 323, loss 2.30524, acc 0.11\n",
      "2017-03-29T11:58:41.429386: step 324, loss 2.28726, acc 0.13\n",
      "2017-03-29T11:58:41.568586: step 325, loss 2.32388, acc 0.08\n",
      "2017-03-29T11:58:41.713263: step 326, loss 2.3243, acc 0.05\n",
      "2017-03-29T11:58:41.856757: step 327, loss 2.30086, acc 0.15\n",
      "2017-03-29T11:58:42.002310: step 328, loss 2.30237, acc 0.08\n",
      "2017-03-29T11:58:42.147295: step 329, loss 2.33144, acc 0.1\n",
      "2017-03-29T11:58:42.290346: step 330, loss 2.30096, acc 0.12\n",
      "2017-03-29T11:58:42.434578: step 331, loss 2.30301, acc 0.08\n",
      "2017-03-29T11:58:42.580080: step 332, loss 2.3044, acc 0.12\n",
      "2017-03-29T11:58:42.722469: step 333, loss 2.32882, acc 0.08\n",
      "2017-03-29T11:58:42.866719: step 334, loss 2.31587, acc 0.09\n",
      "2017-03-29T11:58:43.007031: step 335, loss 2.29989, acc 0.14\n",
      "2017-03-29T11:58:43.151536: step 336, loss 2.29413, acc 0.09\n",
      "2017-03-29T11:58:43.292149: step 337, loss 2.32163, acc 0.08\n",
      "2017-03-29T11:58:43.436950: step 338, loss 2.30185, acc 0.15\n",
      "2017-03-29T11:58:43.580934: step 339, loss 2.30421, acc 0.12\n",
      "2017-03-29T11:58:43.725559: step 340, loss 2.31224, acc 0.12\n",
      "2017-03-29T11:58:43.869554: step 341, loss 2.29124, acc 0.15\n",
      "2017-03-29T11:58:44.015736: step 342, loss 2.32099, acc 0.12\n",
      "2017-03-29T11:58:44.159751: step 343, loss 2.30715, acc 0.1\n",
      "2017-03-29T11:58:44.302899: step 344, loss 2.29259, acc 0.08\n",
      "2017-03-29T11:58:44.445381: step 345, loss 2.32734, acc 0.09\n",
      "2017-03-29T11:58:44.590364: step 346, loss 2.30221, acc 0.08\n",
      "2017-03-29T11:58:44.732153: step 347, loss 2.30989, acc 0.09\n",
      "2017-03-29T11:58:44.876518: step 348, loss 2.30685, acc 0.1\n",
      "2017-03-29T11:58:45.017386: step 349, loss 2.29184, acc 0.09\n",
      "2017-03-29T11:58:45.161837: step 350, loss 2.32218, acc 0.09\n",
      "2017-03-29T11:58:45.302300: step 351, loss 2.30086, acc 0.14\n",
      "2017-03-29T11:58:45.447624: step 352, loss 2.29984, acc 0.11\n",
      "2017-03-29T11:58:45.587869: step 353, loss 2.28884, acc 0.1\n",
      "2017-03-29T11:58:45.731444: step 354, loss 2.30903, acc 0.12\n",
      "2017-03-29T11:58:45.872153: step 355, loss 2.30104, acc 0.14\n",
      "2017-03-29T11:58:46.016314: step 356, loss 2.33079, acc 0.06\n",
      "2017-03-29T11:58:46.160691: step 357, loss 2.3138, acc 0.08\n",
      "2017-03-29T11:58:46.305499: step 358, loss 2.30704, acc 0.11\n",
      "2017-03-29T11:58:46.449976: step 359, loss 2.30937, acc 0.08\n",
      "2017-03-29T11:58:46.595256: step 360, loss 2.30417, acc 0.15\n",
      "2017-03-29T11:58:46.740086: step 361, loss 2.3282, acc 0.08\n",
      "2017-03-29T11:58:46.885172: step 362, loss 2.31448, acc 0.08\n",
      "2017-03-29T11:58:47.029604: step 363, loss 2.30488, acc 0.11\n",
      "2017-03-29T11:58:47.174887: step 364, loss 2.29904, acc 0.06\n",
      "2017-03-29T11:58:47.320667: step 365, loss 2.2999, acc 0.11\n",
      "2017-03-29T11:58:47.466310: step 366, loss 2.31164, acc 0.07\n",
      "2017-03-29T11:58:47.610893: step 367, loss 2.29636, acc 0.11\n",
      "2017-03-29T11:58:47.755922: step 368, loss 2.30991, acc 0.07\n",
      "2017-03-29T11:58:47.900876: step 369, loss 2.28308, acc 0.13\n",
      "2017-03-29T11:58:48.045876: step 370, loss 2.31856, acc 0.12\n",
      "2017-03-29T11:58:48.191165: step 371, loss 2.30927, acc 0.12\n",
      "2017-03-29T11:58:48.336092: step 372, loss 2.28749, acc 0.14\n",
      "2017-03-29T11:58:48.480932: step 373, loss 2.31024, acc 0.09\n",
      "2017-03-29T11:58:48.625710: step 374, loss 2.33261, acc 0.1\n",
      "2017-03-29T11:58:48.771297: step 375, loss 2.2976, acc 0.14\n",
      "2017-03-29T11:58:48.915075: step 376, loss 2.2984, acc 0.12\n",
      "2017-03-29T11:58:49.059410: step 377, loss 2.3084, acc 0.11\n",
      "2017-03-29T11:58:49.204528: step 378, loss 2.30438, acc 0.11\n",
      "2017-03-29T11:58:49.349549: step 379, loss 2.28171, acc 0.09\n",
      "2017-03-29T11:58:49.494139: step 380, loss 2.3517, acc 0.09\n",
      "2017-03-29T11:58:49.638871: step 381, loss 2.30954, acc 0.12\n",
      "2017-03-29T11:58:49.782148: step 382, loss 2.31086, acc 0.07\n",
      "2017-03-29T11:58:49.927225: step 383, loss 2.29913, acc 0.13\n",
      "2017-03-29T11:58:50.070037: step 384, loss 2.31141, acc 0.07\n",
      "2017-03-29T11:58:50.214420: step 385, loss 2.29873, acc 0.13\n",
      "2017-03-29T11:58:50.357971: step 386, loss 2.31868, acc 0.08\n",
      "2017-03-29T11:58:50.499995: step 387, loss 2.3204, acc 0.1\n",
      "2017-03-29T11:58:50.644463: step 388, loss 2.3076, acc 0.09\n",
      "2017-03-29T11:58:50.793143: step 389, loss 2.30476, acc 0.1\n",
      "2017-03-29T11:58:50.938302: step 390, loss 2.31209, acc 0.07\n",
      "2017-03-29T11:58:51.083060: step 391, loss 2.30364, acc 0.09\n",
      "2017-03-29T11:58:51.227050: step 392, loss 2.3032, acc 0.08\n",
      "2017-03-29T11:58:51.371378: step 393, loss 2.30166, acc 0.09\n",
      "2017-03-29T11:58:51.515586: step 394, loss 2.30819, acc 0.07\n",
      "2017-03-29T11:58:51.660230: step 395, loss 2.30235, acc 0.1\n",
      "2017-03-29T11:58:51.803473: step 396, loss 2.30843, acc 0.04\n",
      "2017-03-29T11:58:51.947958: step 397, loss 2.33363, acc 0.1\n",
      "2017-03-29T11:58:52.092661: step 398, loss 2.3151, acc 0.11\n",
      "2017-03-29T11:58:52.236788: step 399, loss 2.29244, acc 0.13\n",
      "2017-03-29T11:58:52.383567: step 400, loss 2.29185, acc 0.07\n",
      "2017-03-29T11:58:52.534883: step 401, loss 2.3118, acc 0.12\n",
      "2017-03-29T11:58:52.679194: step 402, loss 2.30221, acc 0.11\n",
      "2017-03-29T11:58:52.823080: step 403, loss 2.32461, acc 0.06\n",
      "2017-03-29T11:58:52.974293: step 404, loss 2.32763, acc 0.06\n",
      "2017-03-29T11:58:53.120303: step 405, loss 2.3043, acc 0.1\n",
      "2017-03-29T11:58:53.265329: step 406, loss 2.31008, acc 0.08\n",
      "2017-03-29T11:58:53.411338: step 407, loss 2.3031, acc 0.06\n",
      "2017-03-29T11:58:53.558934: step 408, loss 2.31177, acc 0.08\n",
      "2017-03-29T11:58:53.709113: step 409, loss 2.31171, acc 0.11\n",
      "2017-03-29T11:58:53.852672: step 410, loss 2.29968, acc 0.12\n",
      "2017-03-29T11:58:54.007066: step 411, loss 2.3014, acc 0.11\n",
      "2017-03-29T11:58:54.156983: step 412, loss 2.29297, acc 0.11\n",
      "2017-03-29T11:58:54.307996: step 413, loss 2.26957, acc 0.14\n",
      "2017-03-29T11:58:54.458573: step 414, loss 2.33501, acc 0.1\n",
      "2017-03-29T11:58:54.607942: step 415, loss 2.3087, acc 0.14\n",
      "2017-03-29T11:58:54.759690: step 416, loss 2.30748, acc 0.11\n",
      "2017-03-29T11:58:54.909964: step 417, loss 2.31602, acc 0.1\n",
      "2017-03-29T11:58:55.055224: step 418, loss 2.32408, acc 0.07\n",
      "2017-03-29T11:58:55.199618: step 419, loss 2.31155, acc 0.09\n",
      "2017-03-29T11:58:55.342791: step 420, loss 2.30761, acc 0.07\n",
      "2017-03-29T11:58:55.489048: step 421, loss 2.3171, acc 0.05\n",
      "2017-03-29T11:58:55.633910: step 422, loss 2.30355, acc 0.12\n",
      "2017-03-29T11:58:55.778759: step 423, loss 2.30246, acc 0.08\n",
      "2017-03-29T11:58:55.921704: step 424, loss 2.29573, acc 0.13\n",
      "2017-03-29T11:58:56.064617: step 425, loss 2.32705, acc 0.1\n",
      "2017-03-29T11:58:56.209409: step 426, loss 2.30237, acc 0.07\n",
      "2017-03-29T11:58:56.348894: step 427, loss 2.31252, acc 0.08\n",
      "2017-03-29T11:58:56.493675: step 428, loss 2.29444, acc 0.11\n",
      "2017-03-29T11:58:56.633564: step 429, loss 2.33069, acc 0.08\n",
      "2017-03-29T11:58:56.778687: step 430, loss 2.28845, acc 0.17\n",
      "2017-03-29T11:58:56.918910: step 431, loss 2.31756, acc 0.11\n",
      "2017-03-29T11:58:57.061921: step 432, loss 2.30695, acc 0.1\n",
      "2017-03-29T11:58:57.204816: step 433, loss 2.31352, acc 0.1\n",
      "2017-03-29T11:58:57.348614: step 434, loss 2.32682, acc 0.1\n",
      "2017-03-29T11:58:57.491372: step 435, loss 2.29626, acc 0.1\n",
      "2017-03-29T11:58:57.637778: step 436, loss 2.30546, acc 0.12\n",
      "2017-03-29T11:58:57.781784: step 437, loss 2.30462, acc 0.08\n",
      "2017-03-29T11:58:57.926968: step 438, loss 2.31247, acc 0.1\n",
      "2017-03-29T11:58:58.066183: step 439, loss 2.31217, acc 0.14\n",
      "2017-03-29T11:58:58.211378: step 440, loss 2.29797, acc 0.15\n",
      "2017-03-29T11:58:58.351448: step 441, loss 2.3105, acc 0.09\n",
      "2017-03-29T11:58:58.495127: step 442, loss 2.28774, acc 0.12\n",
      "2017-03-29T11:58:58.634926: step 443, loss 2.32009, acc 0.13\n",
      "2017-03-29T11:58:58.779534: step 444, loss 2.29763, acc 0.11\n",
      "2017-03-29T11:58:58.920115: step 445, loss 2.33073, acc 0.07\n",
      "2017-03-29T11:58:59.064238: step 446, loss 2.30415, acc 0.08\n",
      "2017-03-29T11:58:59.204599: step 447, loss 2.31072, acc 0.08\n",
      "2017-03-29T11:58:59.348251: step 448, loss 2.31065, acc 0.08\n",
      "2017-03-29T11:58:59.488912: step 449, loss 2.30045, acc 0.09\n",
      "2017-03-29T11:58:59.632923: step 450, loss 2.30866, acc 0.09\n",
      "2017-03-29T11:58:59.772122: step 451, loss 2.31382, acc 0.09\n",
      "2017-03-29T11:58:59.917132: step 452, loss 2.29557, acc 0.15\n",
      "2017-03-29T11:59:00.057097: step 453, loss 2.31199, acc 0.1\n",
      "2017-03-29T11:59:00.200983: step 454, loss 2.30875, acc 0.05\n",
      "2017-03-29T11:59:00.343908: step 455, loss 2.31473, acc 0.08\n",
      "2017-03-29T11:59:00.489307: step 456, loss 2.31824, acc 0.1\n",
      "2017-03-29T11:59:00.633578: step 457, loss 2.30709, acc 0.1\n",
      "2017-03-29T11:59:00.778407: step 458, loss 2.31123, acc 0.09\n",
      "2017-03-29T11:59:00.919969: step 459, loss 2.31443, acc 0.08\n",
      "2017-03-29T11:59:01.064057: step 460, loss 2.31201, acc 0.07\n",
      "2017-03-29T11:59:01.203173: step 461, loss 2.30504, acc 0.15\n",
      "2017-03-29T11:59:01.347802: step 462, loss 2.2886, acc 0.15\n",
      "2017-03-29T11:59:01.487453: step 463, loss 2.31486, acc 0.09\n",
      "2017-03-29T11:59:01.632004: step 464, loss 2.30793, acc 0.13\n",
      "2017-03-29T11:59:01.772162: step 465, loss 2.30619, acc 0.1\n",
      "2017-03-29T11:59:01.916345: step 466, loss 2.3273, acc 0.1\n",
      "2017-03-29T11:59:02.056217: step 467, loss 2.31247, acc 0.08\n",
      "2017-03-29T11:59:02.200428: step 468, loss 2.31888, acc 0.09\n",
      "2017-03-29T11:59:02.340363: step 469, loss 2.31396, acc 0.05\n",
      "2017-03-29T11:59:02.485361: step 470, loss 2.30777, acc 0.1\n",
      "2017-03-29T11:59:02.625490: step 471, loss 2.30744, acc 0.08\n",
      "2017-03-29T11:59:02.769159: step 472, loss 2.30828, acc 0.1\n",
      "2017-03-29T11:59:02.908763: step 473, loss 2.31503, acc 0.06\n",
      "2017-03-29T11:59:03.053420: step 474, loss 2.3001, acc 0.16\n",
      "2017-03-29T11:59:03.194490: step 475, loss 2.31229, acc 0.11\n",
      "2017-03-29T11:59:03.338583: step 476, loss 2.3131, acc 0.1\n",
      "2017-03-29T11:59:03.485796: step 477, loss 2.30542, acc 0.1\n",
      "2017-03-29T11:59:03.629301: step 478, loss 2.30984, acc 0.09\n",
      "2017-03-29T11:59:03.771450: step 479, loss 2.29589, acc 0.09\n",
      "2017-03-29T11:59:03.915875: step 480, loss 2.30957, acc 0.09\n",
      "2017-03-29T11:59:04.058660: step 481, loss 2.31145, acc 0.08\n",
      "2017-03-29T11:59:04.204293: step 482, loss 2.29775, acc 0.05\n",
      "2017-03-29T11:59:04.348982: step 483, loss 2.3297, acc 0.03\n",
      "2017-03-29T11:59:04.494343: step 484, loss 2.30794, acc 0.03\n",
      "2017-03-29T11:59:04.636681: step 485, loss 2.30568, acc 0.12\n",
      "2017-03-29T11:59:04.782265: step 486, loss 2.3038, acc 0.1\n",
      "2017-03-29T11:59:04.922788: step 487, loss 2.32651, acc 0.06\n",
      "2017-03-29T11:59:05.067446: step 488, loss 2.30318, acc 0.1\n",
      "2017-03-29T11:59:05.207294: step 489, loss 2.29976, acc 0.11\n",
      "2017-03-29T11:59:05.352754: step 490, loss 2.31277, acc 0.07\n",
      "2017-03-29T11:59:05.492161: step 491, loss 2.30947, acc 0.06\n",
      "2017-03-29T11:59:05.636783: step 492, loss 2.30868, acc 0.08\n",
      "2017-03-29T11:59:05.780195: step 493, loss 2.32178, acc 0.08\n",
      "2017-03-29T11:59:05.925766: step 494, loss 2.30965, acc 0.09\n",
      "2017-03-29T11:59:06.070421: step 495, loss 2.30599, acc 0.11\n",
      "2017-03-29T11:59:06.213684: step 496, loss 2.31221, acc 0.11\n",
      "2017-03-29T11:59:06.355931: step 497, loss 2.30408, acc 0.11\n",
      "2017-03-29T11:59:06.500595: step 498, loss 2.29567, acc 0.11\n",
      "2017-03-29T11:59:06.639820: step 499, loss 2.3047, acc 0.11\n",
      "2017-03-29T11:59:06.784527: step 500, loss 2.31673, acc 0.1\n",
      "2017-03-29T11:59:06.924434: step 501, loss 2.29909, acc 0.12\n",
      "2017-03-29T11:59:07.069310: step 502, loss 2.3004, acc 0.12\n",
      "2017-03-29T11:59:07.208105: step 503, loss 2.33349, acc 0.05\n",
      "2017-03-29T11:59:07.353040: step 504, loss 2.30917, acc 0.07\n",
      "2017-03-29T11:59:07.492674: step 505, loss 2.31624, acc 0.05\n",
      "2017-03-29T11:59:07.637216: step 506, loss 2.31591, acc 0.08\n",
      "2017-03-29T11:59:07.780649: step 507, loss 2.30033, acc 0.12\n",
      "2017-03-29T11:59:07.924500: step 508, loss 2.32245, acc 0.1\n",
      "2017-03-29T11:59:08.068661: step 509, loss 2.31021, acc 0.07\n",
      "2017-03-29T11:59:08.212206: step 510, loss 2.31417, acc 0.07\n",
      "2017-03-29T11:59:08.355923: step 511, loss 2.30855, acc 0.09\n",
      "2017-03-29T11:59:08.500987: step 512, loss 2.32675, acc 0.1\n",
      "2017-03-29T11:59:08.640752: step 513, loss 2.30291, acc 0.11\n",
      "2017-03-29T11:59:08.786182: step 514, loss 2.30064, acc 0.1\n",
      "2017-03-29T11:59:08.925278: step 515, loss 2.3069, acc 0.11\n",
      "2017-03-29T11:59:09.070072: step 516, loss 2.31664, acc 0.09\n",
      "2017-03-29T11:59:09.208247: step 517, loss 2.30413, acc 0.12\n",
      "2017-03-29T11:59:09.353879: step 518, loss 2.29934, acc 0.1\n",
      "2017-03-29T11:59:09.493569: step 519, loss 2.29878, acc 0.18\n",
      "2017-03-29T11:59:09.638049: step 520, loss 2.31831, acc 0.09\n",
      "2017-03-29T11:59:09.777221: step 521, loss 2.2905, acc 0.13\n",
      "2017-03-29T11:59:09.922245: step 522, loss 2.3334, acc 0.1\n",
      "2017-03-29T11:59:10.062418: step 523, loss 2.29847, acc 0.1\n",
      "2017-03-29T11:59:10.206990: step 524, loss 2.30967, acc 0.13\n",
      "2017-03-29T11:59:10.346698: step 525, loss 2.30619, acc 0.1\n",
      "2017-03-29T11:59:10.491869: step 526, loss 2.30608, acc 0.08\n",
      "2017-03-29T11:59:10.632017: step 527, loss 2.29204, acc 0.11\n",
      "2017-03-29T11:59:10.776733: step 528, loss 2.3125, acc 0.1\n",
      "2017-03-29T11:59:10.916586: step 529, loss 2.3189, acc 0.11\n",
      "2017-03-29T11:59:11.061927: step 530, loss 2.2852, acc 0.11\n",
      "2017-03-29T11:59:11.202242: step 531, loss 2.29504, acc 0.12\n",
      "2017-03-29T11:59:11.346787: step 532, loss 2.32518, acc 0.06\n",
      "2017-03-29T11:59:11.487229: step 533, loss 2.30844, acc 0.09\n",
      "2017-03-29T11:59:11.631099: step 534, loss 2.30899, acc 0.09\n",
      "2017-03-29T11:59:11.771576: step 535, loss 2.30356, acc 0.11\n",
      "2017-03-29T11:59:11.916506: step 536, loss 2.29584, acc 0.12\n",
      "2017-03-29T11:59:12.056512: step 537, loss 2.30087, acc 0.11\n",
      "2017-03-29T11:59:12.201381: step 538, loss 2.30523, acc 0.11\n",
      "2017-03-29T11:59:12.340858: step 539, loss 2.30956, acc 0.1\n",
      "2017-03-29T11:59:12.485230: step 540, loss 2.3053, acc 0.12\n",
      "2017-03-29T11:59:12.625210: step 541, loss 2.30304, acc 0.11\n",
      "2017-03-29T11:59:12.769648: step 542, loss 2.30363, acc 0.12\n",
      "2017-03-29T11:59:12.909553: step 543, loss 2.30366, acc 0.09\n",
      "2017-03-29T11:59:13.054914: step 544, loss 2.30903, acc 0.1\n",
      "2017-03-29T11:59:13.194267: step 545, loss 2.30563, acc 0.09\n",
      "2017-03-29T11:59:13.338942: step 546, loss 2.3023, acc 0.09\n",
      "2017-03-29T11:59:13.478792: step 547, loss 2.30338, acc 0.1\n",
      "2017-03-29T11:59:13.624283: step 548, loss 2.30159, acc 0.11\n",
      "2017-03-29T11:59:13.764492: step 549, loss 2.30026, acc 0.11\n",
      "2017-03-29T11:59:13.908851: step 550, loss 2.30906, acc 0.1\n",
      "2017-03-29T11:59:14.131611: step 551, loss 2.30526, acc 0.09\n",
      "2017-03-29T11:59:14.272860: step 552, loss 2.30053, acc 0.11\n",
      "2017-03-29T11:59:14.412715: step 553, loss 2.31476, acc 0.08\n",
      "2017-03-29T11:59:14.557905: step 554, loss 2.29598, acc 0.13\n",
      "2017-03-29T11:59:14.700598: step 555, loss 2.28944, acc 0.12\n",
      "2017-03-29T11:59:14.847188: step 556, loss 2.31879, acc 0.1\n",
      "2017-03-29T11:59:14.990425: step 557, loss 2.32068, acc 0.1\n",
      "2017-03-29T11:59:15.135688: step 558, loss 2.33143, acc 0.07\n",
      "2017-03-29T11:59:15.275323: step 559, loss 2.30259, acc 0.09\n",
      "2017-03-29T11:59:15.420668: step 560, loss 2.29789, acc 0.17\n",
      "2017-03-29T11:59:15.560802: step 561, loss 2.33316, acc 0.06\n",
      "2017-03-29T11:59:15.706728: step 562, loss 2.30669, acc 0.1\n",
      "2017-03-29T11:59:15.847300: step 563, loss 2.31645, acc 0.06\n",
      "2017-03-29T11:59:15.992248: step 564, loss 2.30608, acc 0.12\n",
      "2017-03-29T11:59:16.133228: step 565, loss 2.3028, acc 0.1\n",
      "2017-03-29T11:59:16.278267: step 566, loss 2.30773, acc 0.08\n",
      "2017-03-29T11:59:16.417794: step 567, loss 2.27926, acc 0.13\n",
      "2017-03-29T11:59:16.562566: step 568, loss 2.31812, acc 0.12\n",
      "2017-03-29T11:59:16.702069: step 569, loss 2.30708, acc 0.08\n",
      "2017-03-29T11:59:16.845278: step 570, loss 2.3133, acc 0.12\n",
      "2017-03-29T11:59:16.994571: step 571, loss 2.2965, acc 0.07\n",
      "2017-03-29T11:59:17.140792: step 572, loss 2.30237, acc 0.13\n",
      "2017-03-29T11:59:17.286315: step 573, loss 2.31588, acc 0.04\n",
      "2017-03-29T11:59:17.431393: step 574, loss 2.31341, acc 0.12\n",
      "2017-03-29T11:59:17.575021: step 575, loss 2.30218, acc 0.1\n",
      "2017-03-29T11:59:17.720396: step 576, loss 2.30053, acc 0.09\n",
      "2017-03-29T11:59:17.859504: step 577, loss 2.27878, acc 0.12\n",
      "2017-03-29T11:59:18.004996: step 578, loss 2.33862, acc 0.09\n",
      "2017-03-29T11:59:18.145211: step 579, loss 2.29268, acc 0.13\n",
      "2017-03-29T11:59:18.290016: step 580, loss 2.32523, acc 0.11\n",
      "2017-03-29T11:59:18.429493: step 581, loss 2.30409, acc 0.14\n",
      "2017-03-29T11:59:18.574157: step 582, loss 2.31789, acc 0.06\n",
      "2017-03-29T11:59:18.713569: step 583, loss 2.28843, acc 0.09\n",
      "2017-03-29T11:59:18.858368: step 584, loss 2.30876, acc 0.08\n",
      "2017-03-29T11:59:18.997924: step 585, loss 2.29152, acc 0.14\n",
      "2017-03-29T11:59:19.142631: step 586, loss 2.29363, acc 0.12\n",
      "2017-03-29T11:59:19.282268: step 587, loss 2.28738, acc 0.12\n",
      "2017-03-29T11:59:19.426294: step 588, loss 2.32397, acc 0.09\n",
      "2017-03-29T11:59:19.566804: step 589, loss 2.29709, acc 0.16\n",
      "2017-03-29T11:59:19.711375: step 590, loss 2.32388, acc 0.06\n",
      "2017-03-29T11:59:19.850785: step 591, loss 2.30341, acc 0.11\n",
      "2017-03-29T11:59:19.995628: step 592, loss 2.30429, acc 0.11\n",
      "2017-03-29T11:59:20.136536: step 593, loss 2.31201, acc 0.08\n",
      "2017-03-29T11:59:20.281853: step 594, loss 2.31684, acc 0.08\n",
      "2017-03-29T11:59:20.423092: step 595, loss 2.34366, acc 0.11\n",
      "2017-03-29T11:59:20.567740: step 596, loss 2.31796, acc 0.08\n",
      "2017-03-29T11:59:20.707829: step 597, loss 2.32442, acc 0.05\n",
      "2017-03-29T11:59:20.853105: step 598, loss 2.30292, acc 0.1\n",
      "2017-03-29T11:59:20.993586: step 599, loss 2.30366, acc 0.11\n",
      "2017-03-29T11:59:21.138818: step 600, loss 2.29909, acc 0.12\n",
      "2017-03-29T11:59:21.282059: step 601, loss 2.31121, acc 0.16\n",
      "2017-03-29T11:59:21.427131: step 602, loss 2.30488, acc 0.12\n",
      "2017-03-29T11:59:21.572670: step 603, loss 2.32178, acc 0.08\n",
      "2017-03-29T11:59:21.717199: step 604, loss 2.2884, acc 0.14\n",
      "2017-03-29T11:59:21.861892: step 605, loss 2.32457, acc 0.13\n",
      "2017-03-29T11:59:22.006606: step 606, loss 2.31704, acc 0.1\n",
      "2017-03-29T11:59:22.151685: step 607, loss 2.30639, acc 0.14\n",
      "2017-03-29T11:59:22.297485: step 608, loss 2.31008, acc 0.06\n",
      "2017-03-29T11:59:22.441896: step 609, loss 2.30847, acc 0.09\n",
      "2017-03-29T11:59:22.585280: step 610, loss 2.31935, acc 0.06\n",
      "2017-03-29T11:59:22.727500: step 611, loss 2.30878, acc 0.07\n",
      "2017-03-29T11:59:22.872527: step 612, loss 2.30772, acc 0.13\n",
      "2017-03-29T11:59:23.012047: step 613, loss 2.30755, acc 0.12\n",
      "2017-03-29T11:59:23.156817: step 614, loss 2.29553, acc 0.09\n",
      "2017-03-29T11:59:23.296374: step 615, loss 2.31955, acc 0.07\n",
      "2017-03-29T11:59:23.440567: step 616, loss 2.30795, acc 0.12\n",
      "2017-03-29T11:59:23.579045: step 617, loss 2.30935, acc 0.11\n",
      "2017-03-29T11:59:23.723548: step 618, loss 2.33211, acc 0.07\n",
      "2017-03-29T11:59:23.863200: step 619, loss 2.29885, acc 0.1\n",
      "2017-03-29T11:59:24.008016: step 620, loss 2.30341, acc 0.09\n",
      "2017-03-29T11:59:24.147297: step 621, loss 2.31237, acc 0.05\n",
      "2017-03-29T11:59:24.292196: step 622, loss 2.3098, acc 0.1\n",
      "2017-03-29T11:59:24.431538: step 623, loss 2.30092, acc 0.12\n",
      "2017-03-29T11:59:24.576325: step 624, loss 2.32737, acc 0.1\n",
      "2017-03-29T11:59:24.716390: step 625, loss 2.30022, acc 0.14\n",
      "2017-03-29T11:59:24.860353: step 626, loss 2.32396, acc 0.04\n",
      "2017-03-29T11:59:24.999599: step 627, loss 2.3068, acc 0.13\n",
      "2017-03-29T11:59:25.145090: step 628, loss 2.31267, acc 0.1\n",
      "2017-03-29T11:59:25.290726: step 629, loss 2.30109, acc 0.1\n",
      "2017-03-29T11:59:25.436176: step 630, loss 2.32631, acc 0.05\n",
      "2017-03-29T11:59:25.582096: step 631, loss 2.31732, acc 0.06\n",
      "2017-03-29T11:59:25.726486: step 632, loss 2.30681, acc 0.03\n",
      "2017-03-29T11:59:25.871111: step 633, loss 2.32034, acc 0.09\n",
      "2017-03-29T11:59:26.016179: step 634, loss 2.30752, acc 0.14\n",
      "2017-03-29T11:59:26.161051: step 635, loss 2.3085, acc 0.08\n",
      "2017-03-29T11:59:26.306547: step 636, loss 2.30073, acc 0.09\n",
      "2017-03-29T11:59:26.450860: step 637, loss 2.30256, acc 0.13\n",
      "2017-03-29T11:59:26.594327: step 638, loss 2.31768, acc 0.08\n",
      "2017-03-29T11:59:26.738744: step 639, loss 2.30664, acc 0.09\n",
      "2017-03-29T11:59:26.881196: step 640, loss 2.31717, acc 0.05\n",
      "2017-03-29T11:59:27.025787: step 641, loss 2.31147, acc 0.07\n",
      "2017-03-29T11:59:27.171063: step 642, loss 2.31038, acc 0.12\n",
      "2017-03-29T11:59:27.312535: step 643, loss 2.30495, acc 0.08\n",
      "2017-03-29T11:59:27.456689: step 644, loss 2.30041, acc 0.13\n",
      "2017-03-29T11:59:27.595571: step 645, loss 2.31916, acc 0.09\n",
      "2017-03-29T11:59:27.740642: step 646, loss 2.3073, acc 0.1\n",
      "2017-03-29T11:59:27.879896: step 647, loss 2.31473, acc 0.08\n",
      "2017-03-29T11:59:28.025211: step 648, loss 2.29205, acc 0.18\n",
      "2017-03-29T11:59:28.164915: step 649, loss 2.29036, acc 0.07\n",
      "2017-03-29T11:59:28.308993: step 650, loss 2.33667, acc 0.1\n",
      "2017-03-29T11:59:28.448694: step 651, loss 2.29511, acc 0.12\n",
      "2017-03-29T11:59:28.593917: step 652, loss 2.32267, acc 0.11\n",
      "2017-03-29T11:59:28.733244: step 653, loss 2.29508, acc 0.11\n",
      "2017-03-29T11:59:28.877586: step 654, loss 2.31339, acc 0.12\n",
      "2017-03-29T11:59:29.016803: step 655, loss 2.29746, acc 0.12\n",
      "2017-03-29T11:59:29.161180: step 656, loss 2.30295, acc 0.1\n",
      "2017-03-29T11:59:29.300103: step 657, loss 2.2863, acc 0.13\n",
      "2017-03-29T11:59:29.445110: step 658, loss 2.31187, acc 0.11\n",
      "2017-03-29T11:59:29.584158: step 659, loss 2.32316, acc 0.07\n",
      "2017-03-29T11:59:29.729104: step 660, loss 2.30403, acc 0.11\n",
      "2017-03-29T11:59:29.868641: step 661, loss 2.31378, acc 0.09\n",
      "2017-03-29T11:59:30.013176: step 662, loss 2.30455, acc 0.12\n",
      "2017-03-29T11:59:30.152413: step 663, loss 2.30611, acc 0.1\n",
      "2017-03-29T11:59:30.297412: step 664, loss 2.31922, acc 0.08\n",
      "2017-03-29T11:59:30.436567: step 665, loss 2.30855, acc 0.04\n",
      "2017-03-29T11:59:30.581529: step 666, loss 2.3098, acc 0.09\n",
      "2017-03-29T11:59:30.720368: step 667, loss 2.31267, acc 0.14\n",
      "2017-03-29T11:59:30.864785: step 668, loss 2.29334, acc 0.14\n",
      "2017-03-29T11:59:31.003835: step 669, loss 2.29583, acc 0.09\n",
      "2017-03-29T11:59:31.148471: step 670, loss 2.32367, acc 0.08\n",
      "2017-03-29T11:59:31.287237: step 671, loss 2.29837, acc 0.07\n",
      "2017-03-29T11:59:31.431753: step 672, loss 2.28701, acc 0.18\n",
      "2017-03-29T11:59:31.570589: step 673, loss 2.31578, acc 0.09\n",
      "2017-03-29T11:59:31.715410: step 674, loss 2.30734, acc 0.12\n",
      "2017-03-29T11:59:31.854455: step 675, loss 2.28761, acc 0.16\n",
      "2017-03-29T11:59:31.999026: step 676, loss 2.32064, acc 0.09\n",
      "2017-03-29T11:59:32.138155: step 677, loss 2.2974, acc 0.1\n",
      "2017-03-29T11:59:32.282557: step 678, loss 2.3004, acc 0.08\n",
      "2017-03-29T11:59:32.421811: step 679, loss 2.31147, acc 0.1\n",
      "2017-03-29T11:59:32.567082: step 680, loss 2.30655, acc 0.07\n",
      "2017-03-29T11:59:32.707868: step 681, loss 2.33987, acc 0.08\n",
      "2017-03-29T11:59:32.852935: step 682, loss 2.2971, acc 0.1\n",
      "2017-03-29T11:59:32.991726: step 683, loss 2.27756, acc 0.14\n",
      "2017-03-29T11:59:33.136331: step 684, loss 2.29488, acc 0.14\n",
      "2017-03-29T11:59:33.275157: step 685, loss 2.2828, acc 0.14\n",
      "2017-03-29T11:59:33.420910: step 686, loss 2.35892, acc 0.05\n",
      "2017-03-29T11:59:33.560434: step 687, loss 2.31317, acc 0.09\n",
      "2017-03-29T11:59:33.705738: step 688, loss 2.30692, acc 0.11\n",
      "2017-03-29T11:59:33.848828: step 689, loss 2.29794, acc 0.12\n",
      "2017-03-29T11:59:33.992374: step 690, loss 2.29877, acc 0.1\n",
      "2017-03-29T11:59:34.137256: step 691, loss 2.30361, acc 0.14\n",
      "2017-03-29T11:59:34.280425: step 692, loss 2.30782, acc 0.11\n",
      "2017-03-29T11:59:34.423435: step 693, loss 2.32213, acc 0.05\n",
      "2017-03-29T11:59:34.567687: step 694, loss 2.3025, acc 0.11\n",
      "2017-03-29T11:59:34.706722: step 695, loss 2.32072, acc 0.1\n",
      "2017-03-29T11:59:34.852352: step 696, loss 2.3229, acc 0.08\n",
      "2017-03-29T11:59:34.991280: step 697, loss 2.30198, acc 0.08\n",
      "2017-03-29T11:59:35.136706: step 698, loss 2.30974, acc 0.05\n",
      "2017-03-29T11:59:35.275373: step 699, loss 2.28913, acc 0.08\n",
      "2017-03-29T11:59:35.419374: step 700, loss 2.31983, acc 0.1\n",
      "2017-03-29T11:59:35.558400: step 701, loss 2.30545, acc 0.13\n",
      "2017-03-29T11:59:35.703956: step 702, loss 2.30626, acc 0.08\n",
      "2017-03-29T11:59:35.845606: step 703, loss 2.31571, acc 0.08\n",
      "2017-03-29T11:59:35.991471: step 704, loss 2.31599, acc 0.12\n",
      "2017-03-29T11:59:36.133213: step 705, loss 2.29884, acc 0.14\n",
      "2017-03-29T11:59:36.278255: step 706, loss 2.31636, acc 0.12\n",
      "2017-03-29T11:59:36.417239: step 707, loss 2.30344, acc 0.12\n",
      "2017-03-29T11:59:36.562056: step 708, loss 2.30781, acc 0.09\n",
      "2017-03-29T11:59:36.701156: step 709, loss 2.31632, acc 0.1\n",
      "2017-03-29T11:59:36.845677: step 710, loss 2.29602, acc 0.1\n",
      "2017-03-29T11:59:36.985158: step 711, loss 2.31962, acc 0.1\n",
      "2017-03-29T11:59:37.129808: step 712, loss 2.31586, acc 0.05\n",
      "2017-03-29T11:59:37.269265: step 713, loss 2.31141, acc 0.08\n",
      "2017-03-29T11:59:37.413707: step 714, loss 2.3098, acc 0.05\n",
      "2017-03-29T11:59:37.553167: step 715, loss 2.28957, acc 0.14\n",
      "2017-03-29T11:59:37.698491: step 716, loss 2.30693, acc 0.13\n",
      "2017-03-29T11:59:37.843518: step 717, loss 2.3169, acc 0.1\n",
      "2017-03-29T11:59:37.988243: step 718, loss 2.3159, acc 0.09\n",
      "2017-03-29T11:59:38.132779: step 719, loss 2.29848, acc 0.16\n",
      "2017-03-29T11:59:38.277434: step 720, loss 2.29808, acc 0.16\n",
      "2017-03-29T11:59:38.422784: step 721, loss 2.31574, acc 0.1\n",
      "2017-03-29T11:59:38.564695: step 722, loss 2.31356, acc 0.07\n",
      "2017-03-29T11:59:38.709672: step 723, loss 2.31579, acc 0.09\n",
      "2017-03-29T11:59:38.854816: step 724, loss 2.29852, acc 0.13\n",
      "2017-03-29T11:59:38.999500: step 725, loss 2.32392, acc 0.09\n",
      "2017-03-29T11:59:39.143823: step 726, loss 2.29416, acc 0.13\n",
      "2017-03-29T11:59:39.288368: step 727, loss 2.29152, acc 0.09\n",
      "2017-03-29T11:59:39.429724: step 728, loss 2.30508, acc 0.08\n",
      "2017-03-29T11:59:39.573609: step 729, loss 2.31874, acc 0.1\n",
      "2017-03-29T11:59:39.717160: step 730, loss 2.32177, acc 0.07\n",
      "2017-03-29T11:59:39.859839: step 731, loss 2.29966, acc 0.09\n",
      "2017-03-29T11:59:40.005794: step 732, loss 2.29616, acc 0.1\n",
      "2017-03-29T11:59:40.150449: step 733, loss 2.31022, acc 0.1\n",
      "2017-03-29T11:59:40.295738: step 734, loss 2.33062, acc 0.09\n",
      "2017-03-29T11:59:40.440587: step 735, loss 2.30445, acc 0.07\n",
      "2017-03-29T11:59:40.585936: step 736, loss 2.31149, acc 0.09\n",
      "2017-03-29T11:59:40.730534: step 737, loss 2.31717, acc 0.1\n",
      "2017-03-29T11:59:40.874694: step 738, loss 2.29662, acc 0.14\n",
      "2017-03-29T11:59:41.019313: step 739, loss 2.28765, acc 0.09\n",
      "2017-03-29T11:59:41.163937: step 740, loss 2.30404, acc 0.15\n",
      "2017-03-29T11:59:41.302940: step 741, loss 2.31364, acc 0.11\n",
      "2017-03-29T11:59:41.447590: step 742, loss 2.30397, acc 0.12\n",
      "2017-03-29T11:59:41.591353: step 743, loss 2.28469, acc 0.12\n",
      "2017-03-29T11:59:41.736324: step 744, loss 2.32409, acc 0.13\n",
      "2017-03-29T11:59:41.878744: step 745, loss 2.30019, acc 0.11\n",
      "2017-03-29T11:59:42.024629: step 746, loss 2.31206, acc 0.09\n",
      "2017-03-29T11:59:42.169103: step 747, loss 2.29955, acc 0.12\n",
      "2017-03-29T11:59:42.312928: step 748, loss 2.30415, acc 0.1\n",
      "2017-03-29T11:59:42.458664: step 749, loss 2.30548, acc 0.1\n",
      "2017-03-29T11:59:42.602733: step 750, loss 2.32105, acc 0.08\n",
      "2017-03-29T11:59:42.747487: step 751, loss 2.32742, acc 0.02\n",
      "2017-03-29T11:59:42.892383: step 752, loss 2.30978, acc 0.1\n",
      "2017-03-29T11:59:43.037822: step 753, loss 2.29921, acc 0.13\n",
      "2017-03-29T11:59:43.182311: step 754, loss 2.27957, acc 0.15\n",
      "2017-03-29T11:59:43.321499: step 755, loss 2.31783, acc 0.15\n",
      "2017-03-29T11:59:43.460035: step 756, loss 2.31499, acc 0.07\n",
      "2017-03-29T11:59:43.598376: step 757, loss 2.308, acc 0.1\n",
      "2017-03-29T11:59:43.737585: step 758, loss 2.31464, acc 0.11\n",
      "2017-03-29T11:59:43.881956: step 759, loss 2.30214, acc 0.11\n",
      "2017-03-29T11:59:44.021599: step 760, loss 2.29468, acc 0.12\n",
      "2017-03-29T11:59:44.163527: step 761, loss 2.30526, acc 0.12\n",
      "2017-03-29T11:59:44.307605: step 762, loss 2.29327, acc 0.11\n",
      "2017-03-29T11:59:44.451081: step 763, loss 2.27316, acc 0.13\n",
      "2017-03-29T11:59:44.595056: step 764, loss 2.29289, acc 0.11\n",
      "2017-03-29T11:59:44.735923: step 765, loss 2.30357, acc 0.1\n",
      "2017-03-29T11:59:44.879813: step 766, loss 2.31406, acc 0.14\n",
      "2017-03-29T11:59:45.019013: step 767, loss 2.28675, acc 0.12\n",
      "2017-03-29T11:59:45.160164: step 768, loss 2.28786, acc 0.16\n",
      "2017-03-29T11:59:45.303949: step 769, loss 2.32187, acc 0.12\n",
      "2017-03-29T11:59:45.445701: step 770, loss 2.30729, acc 0.07\n",
      "2017-03-29T11:59:45.587243: step 771, loss 2.34003, acc 0.09\n",
      "2017-03-29T11:59:45.728951: step 772, loss 2.27852, acc 0.12\n",
      "2017-03-29T11:59:45.873085: step 773, loss 2.31459, acc 0.11\n",
      "2017-03-29T11:59:46.017202: step 774, loss 2.29378, acc 0.14\n",
      "2017-03-29T11:59:46.160524: step 775, loss 2.31287, acc 0.14\n",
      "2017-03-29T11:59:46.304763: step 776, loss 2.30328, acc 0.12\n",
      "2017-03-29T11:59:46.444893: step 777, loss 2.32297, acc 0.11\n",
      "2017-03-29T11:59:46.587073: step 778, loss 2.3046, acc 0.1\n",
      "2017-03-29T11:59:46.729362: step 779, loss 2.30689, acc 0.12\n",
      "2017-03-29T11:59:46.871065: step 780, loss 2.29975, acc 0.13\n",
      "2017-03-29T11:59:47.014285: step 781, loss 2.30522, acc 0.13\n",
      "2017-03-29T11:59:47.157422: step 782, loss 2.31506, acc 0.09\n",
      "2017-03-29T11:59:47.299230: step 783, loss 2.29871, acc 0.12\n",
      "2017-03-29T11:59:47.437717: step 784, loss 2.32241, acc 0.07\n",
      "2017-03-29T11:59:47.579966: step 785, loss 2.30872, acc 0.06\n",
      "2017-03-29T11:59:47.723771: step 786, loss 2.30565, acc 0.09\n",
      "2017-03-29T11:59:47.865709: step 787, loss 2.30516, acc 0.11\n",
      "2017-03-29T11:59:48.008199: step 788, loss 2.28989, acc 0.14\n",
      "2017-03-29T11:59:48.149558: step 789, loss 2.30189, acc 0.13\n",
      "2017-03-29T11:59:48.293332: step 790, loss 2.32696, acc 0.05\n",
      "2017-03-29T11:59:48.431517: step 791, loss 2.29567, acc 0.13\n",
      "2017-03-29T11:59:48.570082: step 792, loss 2.30659, acc 0.11\n",
      "2017-03-29T11:59:48.708839: step 793, loss 2.29554, acc 0.1\n",
      "2017-03-29T11:59:48.847511: step 794, loss 2.31824, acc 0.11\n",
      "2017-03-29T11:59:48.986592: step 795, loss 2.30131, acc 0.09\n",
      "2017-03-29T11:59:49.124955: step 796, loss 2.30856, acc 0.1\n",
      "2017-03-29T11:59:49.264033: step 797, loss 2.31401, acc 0.07\n",
      "2017-03-29T11:59:49.402432: step 798, loss 2.29826, acc 0.12\n",
      "2017-03-29T11:59:49.541256: step 799, loss 2.30332, acc 0.12\n",
      "2017-03-29T11:59:49.680062: step 800, loss 2.30327, acc 0.1\n",
      "2017-03-29T11:59:49.819105: step 801, loss 2.2968, acc 0.11\n",
      "2017-03-29T11:59:49.957910: step 802, loss 2.31938, acc 0.08\n",
      "2017-03-29T11:59:50.096662: step 803, loss 2.30868, acc 0.11\n",
      "2017-03-29T11:59:50.235104: step 804, loss 2.29112, acc 0.13\n",
      "2017-03-29T11:59:50.373290: step 805, loss 2.31359, acc 0.08\n",
      "2017-03-29T11:59:50.512025: step 806, loss 2.29704, acc 0.1\n",
      "2017-03-29T11:59:50.651118: step 807, loss 2.29332, acc 0.18\n",
      "2017-03-29T11:59:50.790017: step 808, loss 2.33667, acc 0.08\n",
      "2017-03-29T11:59:50.928220: step 809, loss 2.32958, acc 0.07\n",
      "2017-03-29T11:59:51.066861: step 810, loss 2.30071, acc 0.08\n",
      "2017-03-29T11:59:51.205661: step 811, loss 2.29613, acc 0.14\n",
      "2017-03-29T11:59:51.348205: step 812, loss 2.3098, acc 0.11\n",
      "2017-03-29T11:59:51.491003: step 813, loss 2.31379, acc 0.05\n",
      "2017-03-29T11:59:51.630521: step 814, loss 2.31423, acc 0.11\n",
      "2017-03-29T11:59:51.770515: step 815, loss 2.30654, acc 0.12\n",
      "2017-03-29T11:59:51.910527: step 816, loss 2.30447, acc 0.14\n",
      "2017-03-29T11:59:52.050528: step 817, loss 2.31619, acc 0.06\n",
      "2017-03-29T11:59:52.191081: step 818, loss 2.27866, acc 0.19\n",
      "2017-03-29T11:59:52.329881: step 819, loss 2.29606, acc 0.11\n",
      "2017-03-29T11:59:52.472067: step 820, loss 2.32498, acc 0.06\n",
      "2017-03-29T11:59:52.611291: step 821, loss 2.30424, acc 0.08\n",
      "2017-03-29T11:59:52.755835: step 822, loss 2.30911, acc 0.08\n",
      "2017-03-29T11:59:52.899440: step 823, loss 2.33155, acc 0.07\n",
      "2017-03-29T11:59:53.043823: step 824, loss 2.30448, acc 0.08\n",
      "2017-03-29T11:59:53.190719: step 825, loss 2.30933, acc 0.06\n",
      "2017-03-29T11:59:53.335828: step 826, loss 2.29044, acc 0.17\n",
      "2017-03-29T11:59:53.485176: step 827, loss 2.30252, acc 0.1\n",
      "2017-03-29T11:59:53.629473: step 828, loss 2.30991, acc 0.11\n",
      "2017-03-29T11:59:53.774041: step 829, loss 2.31261, acc 0.13\n",
      "2017-03-29T11:59:53.918599: step 830, loss 2.2991, acc 0.13\n",
      "2017-03-29T11:59:54.064136: step 831, loss 2.32, acc 0.09\n",
      "2017-03-29T11:59:54.208341: step 832, loss 2.29715, acc 0.16\n",
      "2017-03-29T11:59:54.349943: step 833, loss 2.31012, acc 0.11\n",
      "2017-03-29T11:59:54.488177: step 834, loss 2.30786, acc 0.12\n",
      "2017-03-29T11:59:54.626397: step 835, loss 2.30487, acc 0.1\n",
      "2017-03-29T11:59:54.766504: step 836, loss 2.30774, acc 0.03\n",
      "2017-03-29T11:59:54.908277: step 837, loss 2.30207, acc 0.09\n",
      "2017-03-29T11:59:55.053173: step 838, loss 2.31366, acc 0.09\n",
      "2017-03-29T11:59:55.197227: step 839, loss 2.30986, acc 0.1\n",
      "2017-03-29T11:59:55.341421: step 840, loss 2.29812, acc 0.12\n",
      "2017-03-29T11:59:55.482380: step 841, loss 2.28797, acc 0.09\n",
      "2017-03-29T11:59:55.628172: step 842, loss 2.31342, acc 0.09\n",
      "2017-03-29T11:59:55.773374: step 843, loss 2.30489, acc 0.1\n",
      "2017-03-29T11:59:55.915861: step 844, loss 2.30561, acc 0.12\n",
      "2017-03-29T11:59:56.061101: step 845, loss 2.30529, acc 0.14\n",
      "2017-03-29T11:59:56.205518: step 846, loss 2.31176, acc 0.06\n",
      "2017-03-29T11:59:56.350077: step 847, loss 2.29887, acc 0.12\n",
      "2017-03-29T11:59:56.495182: step 848, loss 2.29183, acc 0.11\n",
      "2017-03-29T11:59:56.639511: step 849, loss 2.29712, acc 0.07\n",
      "2017-03-29T11:59:56.784321: step 850, loss 2.30728, acc 0.13\n",
      "2017-03-29T11:59:56.928016: step 851, loss 2.28974, acc 0.11\n",
      "2017-03-29T11:59:57.073227: step 852, loss 2.31538, acc 0.09\n",
      "2017-03-29T11:59:57.217929: step 853, loss 2.28826, acc 0.1\n",
      "2017-03-29T11:59:57.362939: step 854, loss 2.32879, acc 0.06\n",
      "2017-03-29T11:59:57.507313: step 855, loss 2.31346, acc 0.1\n",
      "2017-03-29T11:59:57.652690: step 856, loss 2.297, acc 0.06\n",
      "2017-03-29T11:59:57.797321: step 857, loss 2.31659, acc 0.09\n",
      "2017-03-29T11:59:57.942123: step 858, loss 2.30697, acc 0.13\n",
      "2017-03-29T11:59:58.086930: step 859, loss 2.30404, acc 0.09\n",
      "2017-03-29T11:59:58.232596: step 860, loss 2.30472, acc 0.08\n",
      "2017-03-29T11:59:58.377180: step 861, loss 2.31352, acc 0.09\n",
      "2017-03-29T11:59:58.522875: step 862, loss 2.30197, acc 0.09\n",
      "2017-03-29T11:59:58.667903: step 863, loss 2.30579, acc 0.1\n",
      "2017-03-29T11:59:58.812787: step 864, loss 2.30087, acc 0.09\n",
      "2017-03-29T11:59:58.957493: step 865, loss 2.3039, acc 0.08\n",
      "2017-03-29T11:59:59.101493: step 866, loss 2.29821, acc 0.14\n",
      "2017-03-29T11:59:59.246413: step 867, loss 2.30873, acc 0.09\n",
      "2017-03-29T11:59:59.392204: step 868, loss 2.30476, acc 0.07\n",
      "2017-03-29T11:59:59.537242: step 869, loss 2.30647, acc 0.1\n",
      "2017-03-29T11:59:59.682167: step 870, loss 2.30655, acc 0.08\n",
      "2017-03-29T11:59:59.826706: step 871, loss 2.30365, acc 0.09\n",
      "2017-03-29T11:59:59.970797: step 872, loss 2.3071, acc 0.11\n",
      "2017-03-29T12:00:00.115244: step 873, loss 2.30771, acc 0.11\n",
      "2017-03-29T12:00:00.260001: step 874, loss 2.30061, acc 0.09\n",
      "2017-03-29T12:00:00.404044: step 875, loss 2.28772, acc 0.11\n",
      "2017-03-29T12:00:00.548616: step 876, loss 2.2969, acc 0.15\n",
      "2017-03-29T12:00:00.694048: step 877, loss 2.29741, acc 0.14\n",
      "2017-03-29T12:00:00.838964: step 878, loss 2.30143, acc 0.13\n",
      "2017-03-29T12:00:00.982978: step 879, loss 2.30296, acc 0.14\n",
      "2017-03-29T12:00:01.127972: step 880, loss 2.30629, acc 0.13\n",
      "2017-03-29T12:00:01.272087: step 881, loss 2.32216, acc 0.1\n",
      "2017-03-29T12:00:01.416984: step 882, loss 2.28796, acc 0.14\n",
      "2017-03-29T12:00:01.561534: step 883, loss 2.30439, acc 0.14\n",
      "2017-03-29T12:00:01.706776: step 884, loss 2.31901, acc 0.1\n",
      "2017-03-29T12:00:01.851066: step 885, loss 2.29985, acc 0.12\n",
      "2017-03-29T12:00:01.995264: step 886, loss 2.29769, acc 0.16\n",
      "2017-03-29T12:00:02.140194: step 887, loss 2.30083, acc 0.13\n",
      "2017-03-29T12:00:02.285893: step 888, loss 2.29536, acc 0.1\n",
      "2017-03-29T12:00:02.431916: step 889, loss 2.30501, acc 0.14\n",
      "2017-03-29T12:00:02.576023: step 890, loss 2.3166, acc 0.11\n",
      "2017-03-29T12:00:02.720632: step 891, loss 2.2891, acc 0.17\n",
      "2017-03-29T12:00:02.865355: step 892, loss 2.29748, acc 0.14\n",
      "2017-03-29T12:00:03.009565: step 893, loss 2.31733, acc 0.09\n",
      "2017-03-29T12:00:03.153963: step 894, loss 2.3143, acc 0.09\n",
      "2017-03-29T12:00:03.298791: step 895, loss 2.29474, acc 0.06\n",
      "2017-03-29T12:00:03.443458: step 896, loss 2.31643, acc 0.09\n",
      "2017-03-29T12:00:03.588613: step 897, loss 2.32711, acc 0.06\n",
      "2017-03-29T12:00:03.733608: step 898, loss 2.30785, acc 0.12\n",
      "2017-03-29T12:00:03.878237: step 899, loss 2.30454, acc 0.09\n",
      "2017-03-29T12:00:04.023334: step 900, loss 2.2883, acc 0.15\n",
      "2017-03-29T12:00:04.168995: step 901, loss 2.29753, acc 0.12\n",
      "2017-03-29T12:00:04.314498: step 902, loss 2.28502, acc 0.1\n",
      "2017-03-29T12:00:04.460124: step 903, loss 2.30522, acc 0.07\n",
      "2017-03-29T12:00:04.605554: step 904, loss 2.30133, acc 0.16\n",
      "2017-03-29T12:00:04.749953: step 905, loss 2.3089, acc 0.08\n",
      "2017-03-29T12:00:04.894591: step 906, loss 2.31931, acc 0.1\n",
      "2017-03-29T12:00:05.037847: step 907, loss 2.30275, acc 0.08\n",
      "2017-03-29T12:00:05.182697: step 908, loss 2.32936, acc 0.09\n",
      "2017-03-29T12:00:05.327993: step 909, loss 2.31925, acc 0.1\n",
      "2017-03-29T12:00:05.473669: step 910, loss 2.3028, acc 0.09\n",
      "2017-03-29T12:00:05.619279: step 911, loss 2.29352, acc 0.13\n",
      "2017-03-29T12:00:05.758423: step 912, loss 2.30318, acc 0.13\n",
      "2017-03-29T12:00:05.897994: step 913, loss 2.31041, acc 0.09\n",
      "2017-03-29T12:00:06.037595: step 914, loss 2.30554, acc 0.08\n",
      "2017-03-29T12:00:06.176294: step 915, loss 2.29871, acc 0.1\n",
      "2017-03-29T12:00:06.314923: step 916, loss 2.28455, acc 0.13\n",
      "2017-03-29T12:00:06.454097: step 917, loss 2.31001, acc 0.08\n",
      "2017-03-29T12:00:06.592707: step 918, loss 2.32367, acc 0.1\n",
      "2017-03-29T12:00:06.731039: step 919, loss 2.29082, acc 0.07\n",
      "2017-03-29T12:00:06.869570: step 920, loss 2.31921, acc 0.12\n",
      "2017-03-29T12:00:07.007953: step 921, loss 2.30343, acc 0.11\n",
      "2017-03-29T12:00:07.146534: step 922, loss 2.3152, acc 0.09\n",
      "2017-03-29T12:00:07.285292: step 923, loss 2.2917, acc 0.07\n",
      "2017-03-29T12:00:07.423764: step 924, loss 2.32503, acc 0.08\n",
      "2017-03-29T12:00:07.561812: step 925, loss 2.30347, acc 0.16\n",
      "2017-03-29T12:00:07.701855: step 926, loss 2.30996, acc 0.09\n",
      "2017-03-29T12:00:07.840964: step 927, loss 2.32015, acc 0.12\n",
      "2017-03-29T12:00:07.979318: step 928, loss 2.30061, acc 0.13\n",
      "2017-03-29T12:00:08.117498: step 929, loss 2.31166, acc 0.1\n",
      "2017-03-29T12:00:08.255868: step 930, loss 2.28172, acc 0.11\n",
      "2017-03-29T12:00:08.394046: step 931, loss 2.29603, acc 0.08\n",
      "2017-03-29T12:00:08.533297: step 932, loss 2.30754, acc 0.04\n",
      "2017-03-29T12:00:08.673888: step 933, loss 2.29429, acc 0.11\n",
      "2017-03-29T12:00:08.814447: step 934, loss 2.30043, acc 0.17\n",
      "2017-03-29T12:00:08.952789: step 935, loss 2.29607, acc 0.09\n",
      "2017-03-29T12:00:09.091276: step 936, loss 2.32951, acc 0.1\n",
      "2017-03-29T12:00:09.229451: step 937, loss 2.31172, acc 0.1\n",
      "2017-03-29T12:00:09.368224: step 938, loss 2.29372, acc 0.14\n",
      "2017-03-29T12:00:09.506730: step 939, loss 2.30337, acc 0.1\n",
      "2017-03-29T12:00:09.648224: step 940, loss 2.27943, acc 0.13\n",
      "2017-03-29T12:00:09.788706: step 941, loss 2.2994, acc 0.1\n",
      "2017-03-29T12:00:09.933723: step 942, loss 2.2832, acc 0.13\n",
      "2017-03-29T12:00:10.076365: step 943, loss 2.30735, acc 0.09\n",
      "2017-03-29T12:00:10.221032: step 944, loss 2.3043, acc 0.12\n",
      "2017-03-29T12:00:10.365013: step 945, loss 2.28601, acc 0.14\n",
      "2017-03-29T12:00:10.509629: step 946, loss 2.35257, acc 0.08\n",
      "2017-03-29T12:00:10.652920: step 947, loss 2.30841, acc 0.14\n",
      "2017-03-29T12:00:10.797251: step 948, loss 2.31119, acc 0.06\n",
      "2017-03-29T12:00:10.941484: step 949, loss 2.32002, acc 0.1\n",
      "2017-03-29T12:00:11.086501: step 950, loss 2.33113, acc 0.05\n",
      "2017-03-29T12:00:11.228558: step 951, loss 2.3018, acc 0.11\n",
      "2017-03-29T12:00:11.372981: step 952, loss 2.30684, acc 0.06\n",
      "2017-03-29T12:00:11.518843: step 953, loss 2.30398, acc 0.13\n",
      "2017-03-29T12:00:11.662877: step 954, loss 2.31378, acc 0.1\n",
      "2017-03-29T12:00:11.807786: step 955, loss 2.30243, acc 0.11\n",
      "2017-03-29T12:00:11.952189: step 956, loss 2.29156, acc 0.14\n",
      "2017-03-29T12:00:12.095898: step 957, loss 2.29073, acc 0.15\n",
      "2017-03-29T12:00:12.240651: step 958, loss 2.31031, acc 0.11\n",
      "2017-03-29T12:00:12.384032: step 959, loss 2.31133, acc 0.13\n",
      "2017-03-29T12:00:12.529006: step 960, loss 2.31109, acc 0.07\n",
      "2017-03-29T12:00:12.673702: step 961, loss 2.29056, acc 0.15\n",
      "2017-03-29T12:00:12.818590: step 962, loss 2.30697, acc 0.12\n",
      "2017-03-29T12:00:12.958755: step 963, loss 2.30125, acc 0.09\n",
      "2017-03-29T12:00:13.097955: step 964, loss 2.30056, acc 0.13\n",
      "2017-03-29T12:00:13.242386: step 965, loss 2.30565, acc 0.12\n",
      "2017-03-29T12:00:13.387240: step 966, loss 2.29331, acc 0.15\n",
      "2017-03-29T12:00:13.531216: step 967, loss 2.30758, acc 0.16\n",
      "2017-03-29T12:00:13.676562: step 968, loss 2.30446, acc 0.11\n",
      "2017-03-29T12:00:13.822653: step 969, loss 2.2611, acc 0.19\n",
      "2017-03-29T12:00:13.967952: step 970, loss 2.31879, acc 0.1\n",
      "2017-03-29T12:00:14.112508: step 971, loss 2.29254, acc 0.11\n",
      "2017-03-29T12:00:14.256431: step 972, loss 2.27096, acc 0.13\n",
      "2017-03-29T12:00:14.402013: step 973, loss 2.31797, acc 0.09\n",
      "2017-03-29T12:00:14.547924: step 974, loss 2.33269, acc 0.08\n",
      "2017-03-29T12:00:14.692974: step 975, loss 2.29936, acc 0.12\n",
      "2017-03-29T12:00:14.838023: step 976, loss 2.31675, acc 0.1\n",
      "2017-03-29T12:00:14.983220: step 977, loss 2.32203, acc 0.05\n",
      "2017-03-29T12:00:15.128304: step 978, loss 2.3124, acc 0.12\n",
      "2017-03-29T12:00:15.272698: step 979, loss 2.31264, acc 0.09\n",
      "2017-03-29T12:00:15.415855: step 980, loss 2.30825, acc 0.11\n",
      "2017-03-29T12:00:15.554422: step 981, loss 2.2929, acc 0.14\n",
      "2017-03-29T12:00:15.692485: step 982, loss 2.28772, acc 0.13\n",
      "2017-03-29T12:00:15.830912: step 983, loss 2.30765, acc 0.09\n",
      "2017-03-29T12:00:15.969442: step 984, loss 2.32508, acc 0.06\n",
      "2017-03-29T12:00:16.108017: step 985, loss 2.29502, acc 0.14\n",
      "2017-03-29T12:00:16.246546: step 986, loss 2.31855, acc 0.07\n",
      "2017-03-29T12:00:16.384490: step 987, loss 2.29988, acc 0.12\n",
      "2017-03-29T12:00:16.522839: step 988, loss 2.28659, acc 0.1\n",
      "2017-03-29T12:00:16.661388: step 989, loss 2.33287, acc 0.06\n",
      "2017-03-29T12:00:16.799834: step 990, loss 2.30527, acc 0.11\n",
      "2017-03-29T12:00:16.937994: step 991, loss 2.30773, acc 0.07\n",
      "2017-03-29T12:00:17.077196: step 992, loss 2.29909, acc 0.06\n",
      "2017-03-29T12:00:17.215600: step 993, loss 2.28418, acc 0.15\n",
      "2017-03-29T12:00:17.354208: step 994, loss 2.33451, acc 0.06\n",
      "2017-03-29T12:00:17.492657: step 995, loss 2.30276, acc 0.12\n",
      "2017-03-29T12:00:17.631332: step 996, loss 2.29942, acc 0.16\n",
      "2017-03-29T12:00:17.769539: step 997, loss 2.30297, acc 0.08\n",
      "2017-03-29T12:00:17.908734: step 998, loss 2.30054, acc 0.13\n",
      "2017-03-29T12:00:18.047405: step 999, loss 2.30348, acc 0.1\n",
      "2017-03-29T12:00:18.186051: step 1000, loss 2.31994, acc 0.1\n",
      "Test accuracy\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[10000,28,28,32]\n\t [[Node: Conv_1/conv2d/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape, conv2d/kernel/read)]]\n\t [[Node: accuracy/accuracy/_137 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_60_accuracy/accuracy\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'Conv_1/conv2d/convolution', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-7cf776365c5b>\", line 4, in <module>\n    mnist_dnn = MNIST_CNN(img_length=784, hidden_layer_size = 512, num_classes=10)\n  File \"<ipython-input-12-6e2984777d2c>\", line 24, in __init__\n    activation=tf.nn.relu)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/layers/convolutional.py\", line 509, in conv2d\n    return layer.apply(inputs)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 303, in apply\n    return self.__call__(inputs, **kwargs)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 273, in __call__\n    outputs = self.call(inputs, **kwargs)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/layers/convolutional.py\", line 156, in call\n    data_format=utils.convert_data_format(self.data_format, self.rank + 2))\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 639, in convolution\n    op=op)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 308, in with_space_to_batch\n    return op(input, num_spatial_dims, padding)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 631, in op\n    name=name)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 129, in _non_atrous_convolution\n    name=name)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 396, in conv2d\n    data_format=data_format, name=name)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[10000,28,28,32]\n\t [[Node: Conv_1/conv2d/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape, conv2d/kernel/read)]]\n\t [[Node: accuracy/accuracy/_137 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_60_accuracy/accuracy\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[10000,28,28,32]\n\t [[Node: Conv_1/conv2d/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape, conv2d/kernel/read)]]\n\t [[Node: accuracy/accuracy/_137 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_60_accuracy/accuracy\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a7277fac58e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m print(sess.run(mnist_dnn.accuracy, feed_dict={mnist_dnn.input_x: mnist.test.images,\n\u001b[0;32m---> 17\u001b[0;31m                                   mnist_dnn.input_y: mnist.test.labels}))\n\u001b[0m",
      "\u001b[0;32m/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[10000,28,28,32]\n\t [[Node: Conv_1/conv2d/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape, conv2d/kernel/read)]]\n\t [[Node: accuracy/accuracy/_137 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_60_accuracy/accuracy\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'Conv_1/conv2d/convolution', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-7cf776365c5b>\", line 4, in <module>\n    mnist_dnn = MNIST_CNN(img_length=784, hidden_layer_size = 512, num_classes=10)\n  File \"<ipython-input-12-6e2984777d2c>\", line 24, in __init__\n    activation=tf.nn.relu)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/layers/convolutional.py\", line 509, in conv2d\n    return layer.apply(inputs)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 303, in apply\n    return self.__call__(inputs, **kwargs)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 273, in __call__\n    outputs = self.call(inputs, **kwargs)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/layers/convolutional.py\", line 156, in call\n    data_format=utils.convert_data_format(self.data_format, self.rank + 2))\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 639, in convolution\n    op=op)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 308, in with_space_to_batch\n    return op(input, num_spatial_dims, padding)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 631, in op\n    name=name)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 129, in _non_atrous_convolution\n    name=name)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 396, in conv2d\n    data_format=data_format, name=name)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/csegura/virtualenv/TensorFlow3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[10000,28,28,32]\n\t [[Node: Conv_1/conv2d/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape, conv2d/kernel/read)]]\n\t [[Node: accuracy/accuracy/_137 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_60_accuracy/accuracy\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# Train\n",
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    feed_dict={mnist_dnn.input_x: batch_xs, mnist_dnn.input_y: batch_ys}\n",
    "    _, step, summaries, loss, accuracy = sess.run([train_op, global_step, train_summary_op, mnist_dnn.loss, mnist_dnn.accuracy],  feed_dict)\n",
    "\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "    train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "# Test trained model\n",
    "print(\"Test accuracy\")\n",
    "print(sess.run(mnist_dnn.accuracy, feed_dict={mnist_dnn.input_x: mnist.test.images,\n",
    "                                  mnist_dnn.input_y: mnist.test.labels}))\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "livereveal": {
   "height": 720,
   "scroll": true,
   "start_slideshow_at": "selected",
   "width": 1280
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
